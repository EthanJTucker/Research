---
title: "Ethan's Project for Fall 2020"
author: "Ethan Tucker"
date: "June 26th, 2020"
output: html_document
---

ATTRIBUTION:
The data I use in this analysis were collected by Andrew Eckstrom of (?). Without this data none of the work that follows would have been possible. I also want to thank Professor Nordmoe for helping me with this project, from connecting me with the data set to all the helpful waychecks!

Potential error: Dataset does not distinguish between sections, so class tracker puts all sections together regardless of whether teaching quality and grade performance varied between sections.

Potential error: I can only track students \textit{once they have taken a math or chemistry course}. This is an issue for the first course analyses, because the students may have taken courses in prior terms in other departments.




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
options(digits = 3)
```

```{r Initialize}
library(readxl)
library(tidyverse)
# library(merTools)
library(lme4)
Gen_Data <- read_excel("Stats Research XLS.xlsx")
set.seed(1000)


Gen_Data <- Gen_Data %>%
                  filter(`Student Random ID` != 649419)
```

ISSUE WITH DATA SET: THE EASY DATA TRACKER DOES NOT DIFFERENTIATE BETWEEN TYPES OF WITHDRAWS. Email Dr. Eckstrom?
ISSUE WITH DATA SET: HARD TO DIFFERENTIATE BETWEEN DIFFERENT SECTIONS Uni



GENDATA ANALYSIS (CHEMISTRY AND MATH STUDENTS)

```{r Gen_Data Semester Numbering, cache = TRUE}
##Fix Gen_Data semester listing. We want them listed numerically from 1 to N.

Number.Semesters <- function(x) {
  x <- x %>% arrange(Semester)
  
  individual.cases <- x %>% pull(Semester)
  
  individual.cases[1]
  
    for(y in seq_along(along.with = individual.cases)){
      if(individual.cases[y] == "2010/FA"){
        individual.cases[y] <- 1}
      else if(individual.cases[y] == "2011/WI"){
        individual.cases[y] <- 2}
      else if(individual.cases[y] == "2011/SU"){
        individual.cases[y] <- 3}
      else if(individual.cases[y] == "2011/FA"){
        individual.cases[y] <- 4}
      else if(individual.cases[y] == "2012/WI"){
        individual.cases[y] <- 5}
      else if(individual.cases[y] == "2012/SU"){
        individual.cases[y] <- 6}
      else if(individual.cases[y] == "2012/FA"){
        individual.cases[y] <- 7}
      else if(individual.cases[y] == "2013/WI"){
        individual.cases[y] <- 8}
      else if(individual.cases[y] == "2013/SU"){
        individual.cases[y] <- 9}
      else if(individual.cases[y] == "2013/FA"){
        individual.cases[y] <- 10}
      else if(individual.cases[y] == "2014/WI"){
        individual.cases[y] <- 11}
      else if(individual.cases[y] == "2014/SU"){
        individual.cases[y] <- 12}
      else if(individual.cases[y] == "2014/FA"){
        individual.cases[y] <- 13}
      else if(individual.cases[y] == "2015/WI"){
        individual.cases[y] <- 14}
      else if(individual.cases[y] == "2015/SU"){
        individual.cases[y] <- 15}
      else if(individual.cases[y] == "2015/FA"){
        individual.cases[y] <- 16}
      else if(individual.cases[y] == "2016/WI"){
        individual.cases[y] <- 17}
      else if(individual.cases[y] == "2016/SU"){
        individual.cases[y] <- 18}
      else if(individual.cases[y] == "2016/FA"){
        individual.cases[y] <- 19}
      else if(individual.cases[y] == "2017/WI"){
        individual.cases[y] <- 20}
      else{stop("There was an invalid Semester name passed to Number.Semesters")}
      }
    x <- mutate(x, Total.Term = individual.cases)
    x
  }
  
Gen_Data <- Number.Semesters(Gen_Data)
```


```{r N.Profs and N.Students}
##Total number of professors
Profs.List <- unique(Gen_Data$`Faculty Random ID`)
(n.p <- length(Profs.List))
##Total number of students
Students.List <- unique(Gen_Data$`Student Random ID`)
(n.s <- length(Students.List))
##Total quantity of course entries
n <- length(Gen_Data$`Course Code`)
```

```{r Gen_Data.Grade.Converter} 
##This chunk creates the GPA colunmn in Gen_Data
###Following grade converter obtained and adapted from stack overflow user A5C1D2H2I1M1N2O1R2T1 on the page https://stackoverflow.com/questions/22746508/r-simplifying-code-to-convert-letter-grades-to-numeric-grades on 8/19/2020

convert_grades <- function(x) {
    if (x == "WS") {
        x <- 0
    } else if (x == "WF"){
      x <- 0
    } else if (x == "W"){
      x <- NA
    } else if (x == "WP"){
      x <- NA
    } else if (x == "A") {
        x <- 4
    } else if (x == "A-") {
        x <- 3.7
    } else if (x == "B+") {
        x <- 3.3
    } else if (x == "B") {
        x <- 3
    } else if (x == "B-") {
        x <- 2.7
    } else if (x == "C+") {
        x <- 2.3
    } else if (x == "C") {
        x <- 2
    } else if (x == "C-") {
        x <- 1.7
    } else if (x == "D+") {
        x <- 1.3
    } else if (x == "D") {
        x <- 1
    } else if (x == "D-") {
        x <- 0.7
    } else if (x == "F") {
        x <- 0
    } else {
        x <- NA
    }
  x <- as.double(x)
    return(x)
}


## Fix broken data points
for(i in 1:n){
  if(Gen_Data$Grade[i] == "@D"){
    Gen_Data$Grade[i] <- "D"
  }
  if(Gen_Data$Grade[i] == "@F"){
    Gen_Data$Grade[i] <- "F"
  }
}

##Apply Grade Converter
Ass.GPA <- Gen_Data$Grade
Ass.GPA [] <- sapply(Ass.GPA, convert_grades)
Gen_Data <- mutate(Gen_Data, "GPA Assigned" = as.numeric(Ass.GPA))
```


```{r Initialize Department}
## Initialize Department in Gen_Data
Gen_Data <- Gen_Data %>%
                mutate(Department = str_sub(`Course Code`, 1, 3))
```

# ```{r Class Tracker Mean GPA Assigned and Delta, cache=TRUE}
# ##This chunk creates the Class Tracker, Mean GPA, and Delta columns in Gen_Data. This takes a while to run.
# 
# ##IMPORTANT NOTE: the new Gen_Data variable Class Mean GPA is calculated with na.rm = TRUE, and therefore it is the mean gpa assigned of those students who did not withdraw or take incompletes that did not get resolved.
# ##Initial Conditions
# f <- 1
# c <- 1
# cntr <- 1
# prv.cntr <- 0
# Gen_Data <- mutate(Gen_Data, "Class Tracker" = NA, "Class Mean GPA" = NA, "Delta" = NA)
# Gen_Data <- arrange(arrange(arrange(Gen_Data,`Faculty Random ID`), Semester), `Course Code`)
# 
# while(is.na(Gen_Data$`Class Tracker`[n])){
# if(cntr == 4332){
#   break
# }
#   
# Current.Course.Name <- Gen_Data$`Course Code`[f]
# Current.Semester <- Gen_Data$Semester[f]
# Current.Professor <- Gen_Data$`Faculty Random ID`[f]
# 
# while(cntr != prv.cntr ){
# 
#   if(Gen_Data$`Course Code`[c] == Current.Course.Name && Gen_Data$Semester[c] == Current.Semester && Gen_Data$`Faculty Random ID`[c] == Current.Professor){
#     Gen_Data$`Class Tracker`[c] <- cntr
#     c <- c+1
#     }
#   
#   else{
#     ##Calculate class mean GPA and individual student Delta's
#     Class.GPAS <- rep(0,c-f)
#     for(z in 2:(c-f)){
#       Class.GPAS[z] <- as.double(Gen_Data$"GPA Assigned"[z-1+f])
#     }
#      class.mean.gpa <- mean(Class.GPAS, na.rm = TRUE)
#     
#     for(k in f:(c-1)){
#       Gen_Data$`Class Mean GPA`[k] <- class.mean.gpa
#       Gen_Data$`Delta`[k] <- Gen_Data$"GPA Assigned"[k] - class.mean.gpa
#     }
#     prv.cntr <- prv.cntr + 1
#   }
# }
# 
# cntr <- cntr + 1
# 
# ## c-f store to vector one known number of classes
# f <- c
# }
# 
# ##There's some jank where this doesn't do the class gpa and delta for the final Class Tracker, so below I tell R to do it manually. This is terrible coding hide your dignity. My one thought was to add a dummy row at the end of Gen_Data and delete it after the loop, because I think R is stopping the loop as soon as the final Class Tracker is assigned, and completes neither Delta nor Class Mean GPA for the final class.
# ```
# 
# ```{r}
# ##Rewrite above code chunk with dplyr if time allows
# ```
# 
# JANK:
# 
# 
# ```{r Fix final class}
# ##Fix final class from jank method
# f <- 139171
# l <- 139228
# k <- l - (f-1)
# Gen_Data <- arrange(arrange(arrange(Gen_Data,`Faculty Random ID`),Semester),`Course Code`)
# Class.GPAS2 <- rep(0,k)
# for(i in f:l){
#   Class.GPAS2[i-(f-1)] <- as.double(Gen_Data$"GPA Assigned"[i])
#   Gen_Data$`Class Tracker`[i] <- 4332 ##CHANGE
# }
# class.mean.gpa <- mean(Class.GPAS2,na.rm = TRUE)
# for(i in f:l){
#   Gen_Data$`Class Mean GPA`[i] <- class.mean.gpa
#   Gen_Data$Delta[i] <- Gen_Data$"GPA Assigned"[i] - class.mean.gpa
# }
# 
# ```

```{r Initialize Student.Term, cache = TRUE}
##Here we convert Total.Term into numeric, and create the Student.Term variable which describes a student's term progression through OCC

##Initialize Student.Term into Gen_Data. The arrange step is critical for proper student indexing.
Gen_Data <- Gen_Data %>%
              mutate(Total.Term = as.integer(Total.Term)) %>%
                arrange(`Student Random ID`, Total.Term)

##Function that determines which indecies within Gen_Data a given student possesses
student.index <- function(cntr){
  student.id <- unique(Gen_Data$`Student Random ID`)[cntr]
  student.indecies <- which(Gen_Data$`Student Random ID` == student.id)
  return(student.indecies)
}
    
##Function that numbers a student's terms in OCC's math and chemistry departments using positive integers. 
student.terms <- function(index){
  total.terms <- Gen_Data$Total.Term[index]
  student.terms <- rep(0, length(total.terms))
  last.total.term <- total.terms[1]
  student.terms[1] <- 1
  
  if(length(total.terms) >=2){
  
    for(i in 2:length(total.terms)){
      if(last.total.term == total.terms[i]){
        student.terms[i] <- student.terms[i-1]
      } else{
        last.total.term <- total.terms[i]
        student.terms[i] <- student.terms[i-1] + 1
      }
  }
    
  }
  return(student.terms)
}


##Use functions to assign student terms. This algorithm is very slow - I couldn't think of a better way to do it. It works and only has to compile once, so it's good enough.
n.s <- length(unique(Gen_Data$`Student Random ID`))

Gen_Data2 <- Gen_Data %>%
              mutate(Student.Term = 0)
for(i in 1:n.s){
  Gen_Data2$Student.Term[student.index(i)] <- student.terms(student.index(i))
}
```

```{r Initialize Department and make N.Courses.By.Term}
##Create a new data frame that tracks individual students through their years at OCC. First we need to figure out how many classes a student takes in math/chem departments by term.

##Initialize Department in Gen_Data
Gen_Data3 <- Gen_Data2 %>%
              mutate(Department = str_sub(`Course Code`,1,3))

N.Courses.By.Term <- Gen_Data3 %>%
                      group_by(`Student Random ID`, Student.Term, Department) %>%
                        count() %>%
                          ungroup() %>%
                            rename(Num.Courses = n) %>%
                              group_by(Department, Num.Courses) %>%
                                count()


##This dataset can be used to show that students are much more likely to take n math courses simultaneously than n chemistry courses. Additionally, it tells us we need to make a robust points tracking algorithm. Student 28108 passed two math courses AND a chemistry course simultaneously (during student.term 3), which tells us that a student can take multiple courses per semester in the same department.
```

```{r Initialize Class Size}
##Create class size variable for statistical significance tests. There are 5 courses with iffy class sizes: 861, 2636, 3894, 4234, and 3148. The first four of these have n = 1 - they are independent studies. 3148 had n = 3, I'm not sure how that happened. Doing this filtration gives n > 7, which will allow for safe usage of the t test and bootstrap. Moreover, the distribution of received grades for small class size may be inherently different from the overall distribution. Fortunately we only had to remove very few cases, so overall significance is essentially undamaged.
Good.Class.Size <- Gen_Data3 %>%
                      group_by(`Course Code`,`Faculty Random ID`, Total.Term) %>%
                        count() %>%
                          mutate(class_size = n, Significant = if_else(n > 7, TRUE, FALSE)) %>%
                            dplyr::select(-n) %>%
                              filter(Significant == TRUE)

Gen_Data4 <- left_join(Good.Class.Size, Gen_Data3, by = c("Faculty Random ID", "Total.Term", "Course Code")) %>%
              dplyr::select(-Significant)
```

```{r Initialize Max Terms}
##Initialize Max Terms variable in Gen_Data
Max_terms <- Gen_Data4 %>%
              arrange(`Student Random ID`, Student.Term) %>%
                group_by(`Student Random ID`) %>%
                  summarise(max.terms = max(Student.Term))

Gen_Data5 <- left_join(Gen_Data4, Max_terms) %>% View()
```
```{r Initialize Course Credits and Course Names}
##In order to create cumulative GPA we need to weight by credit hours. This chunk creates a credit variable for each row in Gen Data

##Found CHE-0950 credits at https://www.oaklandcc.edu/finaid/docs/fa39a_satisfactoryacademic.pdf . I could not find the name, so I'm calling it preparation for chemistry because it is high school level chemistry according to https://wmich.edu/sites/default/files/attachments/u684/2016/Chemistry-%20Oakland%20CC_0.pdf  
##Found MAT-1045 credits at https://www.oaklandcc.edu/schedule/docs/OCC_Schedule_2017_WI.pdf
#Found MAT-1140 credits at http://dalnetarchive.org/bitstream/handle/11061/1402/2007%20Mathematics-%20Curriculum%20Review%20Self%20Study.pdf?sequence=1&isAllowed=y

##Makes a lookup table for Course Code, Course Names, and Course Credits
Course.Credits <- tibble(`Course Code` = unique(Gen_Data4$`Course Code`),
                  `Course Credits` =  c(4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4 , 4 , 3, 4, 4, 4, 4, 3, 4, 4, 5, 4),
                    `Course Names` = c("Preparation for Chemistry", "Introductory Chemistry", "Survey of Organic and Biochemistry", "General Chemistry I", "General Chemistry II", "Organic Chemistry I", "Organic Chemistry II", "Organic Chemistry Lab", "Fundamentals of Arithmetic", "Preparation for Algebra", "Buisness Mathematics", "Elementary Algebra", "Math Literacy", "Plane Geometry", "Intermediate Algebra", "Finite Mathematics", "Quantitative Reasoning", "College Algebra", "Trigenometry", "Statistics", "Applied Calculus", "Precalculus", "Calculus I"))

##Store Course Credits into Gen_Data
Gen_Data6 <- Gen_Data5 %>%
                left_join(Course.Credits, by = "Course Code")
```

```{r Cumulative GPA, warning = FALSE}
## This chunk creates the cumulative GPA variable in Gen_Data. This allows us to track our students through OCC without having such a stratified model - cumulative GPA is much less discrete than GPA Assigned.

##This first section creates the Cum.GPA variable with a simple weighted mean in a dummy dataset which will eventually be stored as Gen_Data
No_Withdraws <- Gen_Data6 %>%
                        arrange(`Student Random ID`, Student.Term) %>%
                                filter(!is.na(`GPA Assigned`)) %>%
                                        mutate(Cum.GPA = 0)
current.numerator <- 0
current.denominator <- 0
current.student <- 31
n.w <- length(No_Withdraws$`GPA Assigned`)
for(i in 1:n.w){
        if(No_Withdraws$`Student Random ID`[i] != current.student){
                current.numerator <- 0
                current.denominator <- 0
                current.student <- No_Withdraws$`Student Random ID`[i]
        }
        current.numerator <- current.numerator + No_Withdraws$`GPA Assigned`[i] * No_Withdraws$`Course Credits`[i]
        current.denominator <- current.denominator + No_Withdraws$`Course Credits`[i]
        
        No_Withdraws$Cum.GPA[i] <- current.numerator / current.denominator
                
}


Temp_Data <- Gen_Data6 %>%
                left_join(No_Withdraws) %>%
                        arrange(`Student Random ID`, Student.Term)
 

##This second section assigns the previous Cum.GPA to all withdraws if the student had a previous Cum.GPA. This allows for better visualization in ggplot, and better modeling with lmer.

##This chunk throws an error at the end of execution, but it's fine. Run the next chunk to store the cumulative GPA's to Gen_Data
for(i in 2:n){
        current.student <- Temp_Data$`Student Random ID`[i]
        previous.student <- Temp_Data$`Student Random ID`[i-1]
        
        if(is.na(Temp_Data$Cum.GPA[i])){
                if(current.student == previous.student){Temp_Data$Cum.GPA[i] <- Temp_Data$Cum.GPA[i-1] }
        }
}
```

```{r Initialize Cum.GPA}
##Here we store Cum.GPA into Gen_Data
Gen_Data7 <- Temp_Data
```

```{r No muli-department faculty}
## This chunk shows that there are no faculty that teach in both the math and chemistry departments.
counter <- Gen_Data7 %>%
              group_by(`Faculty Random ID`, Department) %>%
                count() 
##This is the difference between the total number of faculty - department combinations and the number of unique faculty.
length(counter$`Faculty Random ID`) - length(unique(Gen_Data7$`Faculty Random ID`))
```


```{r Initialize Course Code Mean GPA}
##Determine course difficulty by average GPA assigned to those who did not withdraw. Proportion of withdraws by course code is represented in future section.
Course.GPAs <- Gen_Data7 %>%
                 ungroup() %>%
                    group_by(`Course Code`) %>%
                      summarise(Course.Code.Mean.GPA = mean(Cum.GPA, na.rm = TRUE))

Gen_Data8 <- Gen_Data7 %>%
              left_join(Course.GPAs, by = "Course Code")
```




Preliminary disclaimers:

1. It is impossible to differentiate graduate student instructors from full professors based on the random ID's. To avoid being rude, in the following analysis I will refer to an individual faculty member interchangeably as a faculty member or as a professor to avoid repetition, and to grant the anonymous person the highest possible title they might possess.
2. Plane Geometry was only taught once and had only 15 students, so the data from that course is highly subject to variance. Whenever possible I try to keep the number of observations above 29, and so I will not include Plane Geometry in the following analysis. [ACTUALLY DO THIS]


Questions:

1) Does the professor a student encounters first in a department influence future performance? Are some faculty better at teaching some courses than others?


  As the below graph shows, it is impossible to create an inference for this question on the entire data set. Some students withdraw from their courses in the first term and thus do not receive grades - it is impossible to analyze their GPA assigned in the first term. We can however measure probability of success by filtering for only those students that withdrew from courses in the first term, and those who did not. This brings us to a second issue - the data we have are only for the courses that students took in the math and chemistry departments during their stay at OCC. We cannot with any degree of certainty claim that those first recorded terms in the dataset are indeed the first terms. Student X could have taken an English course in a term before appearing in our data set for all we know. We have implicitly filtered for: (1) students who did not for some reason attend a standard four year college, and (2) students that wished to take math and/or chemistry courses in community college. We have moreover restricted our departmental information to only chemistry and mathematics courses, which removes our ability to deal with the possibility that the math or chemistry departments are outliers in this question. Alas, let's do our best.
  
```{r Not First Withdraw Justification, warning=FALSE}
##Create plot to illustrate the inability to use all students for Q1. We must then do a filtration!
Plot.Data1 <- Gen_Data8 %>%
                group_by(`Student Random ID`) %>%
                  slice(1) %>%
                    mutate(First.Term.Withdraw = if_else(is.na(Cum.GPA), "Yes", "No")) %>%
                      dplyr::select(`Student Random ID`, First.Term.Withdraw) %>%
                        right_join(Gen_Data8, by = c("Student Random ID"))

Plot.Data1 %>% 
    filter(`Student Random ID` <= 10^3) %>%
        ggplot(aes(x = Student.Term, y = Cum.GPA, group = factor(`Student Random ID`), color = First.Term.Withdraw)) +
        geom_point(position = position_jitter(width = 0.2, height = 0.1)) + 
        geom_smooth(method = "lm", se = FALSE) + 
        labs(x = "Student's Term", y = "Cumulative GPA", title = "Cumulative GPA by Student's Term for 178 Randomly Selected Students", color = "Did student withdraw from first course?")
```

6 out of the total 178 randomly drawn students withdrew from their first course. The GPA they were assigned in their second term cannot be considered an accurate substitute, because the question specifically regards the $\textit{experience}$ a student had in their first exposure to community college mathematics or chemistry. To partially answer this question, we must then first filter for all students that took only one course in their first term, did not withdraw from that first course, and took at least two terms total. By mandating these conditions we invariably reduce our sample size, but because our dataset is so large and the conditions rather lenient at this stage we expect no issue. In particular, after this filtration we are left with $35687$ unique students distributed over $267$ unique faculty members. The mean faculty member then will contribute to $\sim 133.66$ measurable outcomes using this scheme - well over the $30 \sim 40$ that is standard guidance for utilizing the $Z$ test statistic as Normal approximation.

```{r}
##Initialize number of courses per term,  filter for those students that took only one course in their first term and took at least two graded terms in the math or chemistry departments. That is, math first term then chemistry second term is totally legal.
Course.Num <- Gen_Data8 %>%
                group_by(`Student Random ID`, Student.Term) %>%
                  count() %>%
                    right_join(Gen_Data8, by = c("Student Random ID", "Student.Term")) %>%
                      rename(Num.Courses.In.Term = n) %>%
                        filter(max.terms >= 2, Num.Courses.In.Term*Student.Term == 1)



##Initialize First Prof and First Course
Gen_Data9 <- Course.Num %>%
              mutate(First.Prof = `Faculty Random ID`, First.Course = `Course Code`) %>%
                ungroup() %>%
                  dplyr::select(First.Prof, `Student Random ID`, First.Course) %>%
                    right_join(Gen_Data8, by = c("Student Random ID"))

##Filter for students that did not withdraw from first course
Not.First.Withdraw <- Gen_Data9 %>%
                       ungroup()%>%
                          group_by(`Student Random ID`) %>%
                            slice(1) %>%
                              filter(Student.Term == 1, !is.na(`GPA Assigned`), !is.na(First.Prof)) %>%
                                dplyr::select(`Student Random ID`)

Filtered.Data1 <- Gen_Data9 %>%
                    semi_join(Not.First.Withdraw, by = "Student Random ID") %>%
                      group_by(`Student Random ID`) %>%
                        mutate(Withdraw = if_else(is.na(`GPA Assigned`), TRUE, FALSE)) %>%
                          mutate(Total.Grades = sum(Withdraw == FALSE))

##Filter for only those students that recieved grades in at least two terms. We can use the number of withdraws to determine this, because we have already guarenteed the student did not withdraw in the first term, and there was only one course in the first term.
At.Least.Two.Graded.Terms <- Filtered.Data1 %>%
                                filter(Total.Grades >= 2) %>%
                                  slice(1) %>%
                                    dplyr::select(`Student Random ID`)

Filtered.Data1 <- Filtered.Data1 %>%
                    semi_join(At.Least.Two.Graded.Terms, by = "Student Random ID")

##Number of students removed by filtration
length(unique(Gen_Data$`Student Random ID`)) - length(unique(Filtered.Data1$`Student Random ID`))
##Number of remaining students
length(unique(Filtered.Data1$`Student Random ID`))
##Number of faculty removed by filtration
length(unique(Gen_Data$`Faculty Random ID`)) - length(unique(Filtered.Data1$`Faculty Random ID`))
##Number of remaining faculty
length(unique(Filtered.Data1$`Faculty Random ID`))

##Show that there are no entries of Filtered.Data1 that have an NA value for First.Prof - that is we have successfully chosen only those students who did not withdraw from their first course
length(Filtered.Data1$First.Prof[is.na(Filtered.Data1$First.Prof)])
```

Here we have created a variable in Gen_Data that tracks the first professor a student had.   

We can represent the mean student's path through OCC using a linear mixed-effects model. At the time of my writing this, the package lme4 was the most current mixed modeling package available through CRAN. (CITE) A linear mixed-effects model represents a dependent variable - in our case the cumulative GPA of a student - as a linear function of some inputs. The benefit of using such a model is that instead of just using a fixed effect as predictor, such as the student's term number, we can manually specify certain variables to lme4 that either effect the intercept or the slope of our linear model. One possible covariant effect to future cumulative GPA achievement may be the course in which students begin their tenure at OCC's math/chemistry departments. For instance - there may be a difference in both the starting GPA and the slope for cumulative GPA in the mean student that begins their community college math courses in Preparation for Algebra (MAT - 1050) and a student that began in Calculus II (Mat - 1740). Before we can appropriately analyze the effect that individual professors of these courses have on students that begin in these courses, we need to first determine the mean case for all these courses. The below chunk employs lme4 to create a linear mixed-effects model for each course using only those students that: (1) did not withdraw from the course in their first term so that we have a measurable GPA intercept, and (2) took more than one course in the data set so that we can generate a slope between two observations.  An alternative to assumption (1) is to allow students to withdraw from their first course, but in this case it becomes impossible to gauge how their course experience was based on cumulative gpa. We will look at proportion of withdraws against the mean for each course as an additional metric of professor effect later.

```{r Make Manual DeltaC Models and DeltaC's}
## Determine the first observation of GPA for each student, then use it as the intercept for the mean linear mixed effects model. Store student's first course GPA to Filtered Data.
Filtered.Data2 <-  Filtered.Data1 %>%
                      ungroup() %>%
                        group_by(`Student Random ID`) %>%
                          slice(1) %>%
                            mutate(First.Course.GPA = Cum.GPA) %>%
                              dplyr::select(`Student Random ID`, First.Course.GPA) %>%
                                right_join(Filtered.Data1, by = "Student Random ID")

##  Store student's last term cumulative GPA to a variable in Filtered.Data for error analysis
Filtered.Data3 <- Filtered.Data2  %>%
                    ungroup()%>%
                      group_by(`Student Random ID`) %>%
                        slice(n()) %>%
                          mutate(Last.Term.Cum.GPA = Cum.GPA) %>%
                            dplyr::select(`Student Random ID`, Last.Term.Cum.GPA) %>%
                              right_join(Filtered.Data2, by = "Student Random ID")

##Initialize Course Slopes into Filtered.Data. This uses the maximum terms taken as the denominator, which approximates that student's Cum.GPA slope does not change during their stay in OCC's math and chemistry departments.
Filtered.Data4 <- Filtered.Data3 %>%
                    group_by(`Student Random ID`) %>%
                      mutate(Cum.GPA.Slope = (Last.Term.Cum.GPA - First.Course.GPA)/Total.Grades)

Course.Mean.Cum.GPA.Slopes <- Filtered.Data4 %>%
                                slice(1) %>%
                                  ungroup() %>%
                                    group_by(First.Course, max.terms) %>%
                                      summarise(Course.Mean.Cum.GPA.Slope = mean(Cum.GPA.Slope), Num.Students = n()) %>%
                                        filter(Num.Students >= 40)

Manual.DeltaC.Data <- Filtered.Data4 %>%
                        semi_join(Course.Mean.Cum.GPA.Slopes, by = c("First.Course", "max.terms")) %>%
                          left_join(Course.Mean.Cum.GPA.Slopes, by = c("First.Course", "max.terms")) %>%
                            mutate(Expected.Last.Term.Cum.GPA = Course.Mean.Cum.GPA.Slope*max.terms + First.Course.GPA) %>%
                              mutate(DeltaC = Last.Term.Cum.GPA - Expected.Last.Term.Cum.GPA) %>%
                                dplyr::select(`Student Random ID`, First.Course, First.Prof, DeltaC, everything()) %>%
                                  group_by(`Student Random ID`) %>%
                                    slice(1)

Experiment.One.Mean.DeltaCs <- Manual.DeltaC.Data %>%
                                ungroup() %>%
                                  group_by(First.Prof) %>%
                                    summarise(Mean.DeltaC2 = mean(DeltaC), Sd.DeltaC2 = sd(DeltaC), Num.Students = n()) %>%
                                      filter(Num.Students >= 8)

Experiment.Two.Mean.DeltaCs <- Manual.DeltaC.Data %>%
                                ungroup() %>%
                                  group_by(First.Prof, First.Course) %>%
                                    summarise(Mean.DeltaC2 = mean(DeltaC), Sd.DeltaC2 = sd(DeltaC), Num.Students = n()) %>%
                                      filter(Num.Students >= 8)
```



```{r Make LME4 Models}
## Find those courses that affect a students future cumulative GPA slope as a function of term.

courses <- unique(Filtered.Data1$`Course Code`)


## These models predict a student's Cumulative GPA in the math and chemistry departments during their stay at OCC depending on the course they took first. This 
make.course.model <- function(course){
  One.Course.Data <- Filtered.Data1 %>%
                      filter(First.Course == course)
    if(length(One.Course.Data$`Student Random ID`) >= 40){
      Cum.course.model <- lmer(Cum.GPA ~ Student.Term + (1|`Student Random ID`), data = One.Course.Data)
      return(summary(Cum.course.model)$coefficients)
  } else{return(str_c("Insufficient students started at OCC in ",course, " to create an accurate model (minimum 40, observed ", length(One.Course.Data$`Student Random ID`), ")",   sep = ""))}
}

course.summaries <- sapply(courses, make.course.model)


##This function pulls those courses with a significant positive impact on future cumulative gpa
sig.summaries.positive <- function(list){
  Sig.Courses.Positive <- rep("Remove", length(list))
  for(i in 1:length(list)){
    if(!is.null(dim(course.summaries[[i]]))){
    intercept.tval <- list[[i]][1,3]
    Student.Term.tval <- list[[i]][2,3]
      if(intercept.tval >= qnorm(0.95) & Student.Term.tval >= qnorm(0.95)){
        Sig.Courses.Positive[i] <- names(course.summaries[i])
    }
    }
  }
  return(Sig.Courses.Positive[Sig.Courses.Positive != "Remove"])
}

## And this is a function that extracts those courses that have a significant negative effect on future GPA
sig.summaries.negative <- function(list){
  Sig.Courses.Negative <- rep("Remove", length(list))
  for(i in 1:length(list)){
    if(!is.null(dim(course.summaries[[i]]))){
    intercept.tval <- list[[i]][1,3]
    Student.Term.tval <- list[[i]][2,3]
      if(intercept.tval >= qnorm(0.95) & Student.Term.tval <= qnorm(0.05)){
        Sig.Courses.Negative[i] <- names(course.summaries[i])
    }
    }
  }
  return(Sig.Courses.Negative[Sig.Courses.Negative != "Remove"])
}


##These vectors contain all the courses that had significant effects on future cumulative GPA.
significant.courses.positive <- sig.summaries.positive(course.summaries)
significant.courses.negative <- sig.summaries.negative(course.summaries)

##These are the courses that did not have enough students begin in them to use the approximate Z test statistic for estimation of future cumulative GPA
not.enough.data <- rep("remove", length(course.summaries))
for(i in 1:length(course.summaries)){
  if(is.null(dim(course.summaries[[i]]))){
    not.enough.data[i] <- names(course.summaries[i])
  }
}
not.enough.data <- not.enough.data[not.enough.data != "remove"]

## Finally, these are the courses that did have enough data to use the approximate Z test statistic but did not significantly influence future cumulative GPA. Students that began in these courses will be compared to the global mean.
`%notin%` <- Negate(`%in%`)
not.significant <- courses[courses %notin% significant.courses.positive & courses %notin% significant.courses.negative & courses %notin% not.enough.data]

## Tibble used to calculate mean number of courses taught by faculty for following paragraph.
Prof.Courses <-  Filtered.Data1 %>%
                    ungroup() %>%
                      group_by(`Faculty Random ID`, `Course Code`) %>%
                        count() %>%
                          dplyr::select(-n) %>%
                            ungroup() %>%
                              group_by(`Faculty Random ID`) %>%
                                summarise(Num.Courses.Taught = n())
```
For brevity I will refer to individual courses by their course code, but I have included a lookup table at the end of this document that includes course name and the number of credits for those interested. The courses that had a significant positive effect on future cumulative GPA were `r significant.courses.positive`. The courses that had a significant negative effect on future cumulative GPA were `r significant.courses.negative`. There are two categories of non-significant courses: those in which an insufficient number of students started to draw a proper inference (I used n >= 30), and those which had sufficient data but whose students' GPAS did not follow a statistically significant common pattern using $\alpha = 0.01$. The courses that did not have enough students start in them for analysis were `r not.enough.data`. The courses that had enough students take them first but did not significantly effect future cumulative GPA were `r not.significant`.

To obtain a model of professor influence on future cumulative GPA, we first filter out all those students that begain in courses that did not have enough data to provide inference. This is a source of error in the analysis: there may or may not be a significant effect on future cumulative gpa for those students that began in these courses, but we would need more data to be confident. It is therefore inappropriate to superimpose the mean student's cumulative gpa progression onto the students who took these courses. We therefore cannot measure the future effects of the professors that taught such courses - this may be a big error because on average faculty only teach `r mean(Prof.Courses$Num.Courses.Taught)` different courses. On the other hand, it $\textit{is}$ appropriate to superimpose the mean onto those students whose first math or chemistry course was in the list `r not.significant`. The students whose first class had a significant effect on their outcome will be measured against their peers.

We now begin working towards building vectors for each professor that contain the change in cumulative GPA for all students from first to final term in $\textit{both}$ the mathematics and chemistry departments. We will then take the difference between this change and the expected change as predicted by our linear mixed effects models (we have adjusted for the slope and intercepts effects caused by course). This is a measure of student outcome. We have then refined our question to: how does the professor an OCC student have first in the math or chemistry department affect that student's final performance in those departments? Outside of those departments? After creating this distribution we will test for significant effects two ways. First we will use a standard one sample t-test, obtaining the degrees of freedom from the number of students that started with that faculty member minus one. We will also bootstrap the values to create a model of the sampling distribution, then create a confidence interval to compare against the mean outcome. All professors with a statistically significant $\texit{positive}$ effect on student outcomes as compared to the mean student adjusted for course will be categorized as "good", and likewise those with significant $\textit{negative}$ effects will be categorized as "poor". Based on our choice for $\alpha$ we expect some error - there should be approximately $M = \alpha * n_{prof}$ misclassifications. If M is not so different from the sum of the numbers of "good" and "poor" professors, we will be confident that there is little real variance in the effect professors have on students (in the math and chemistry departments of OCC).


```{r Strata Justification}
##Create overall visualization of final cumulative GPA by first course
Filtered.Data1 %>%
  ungroup() %>%
    group_by(`Student Random ID`) %>%
      slice(n()) %>%
        filter(max.terms >= 2) %>%
          dplyr::select(Cum.GPA, `Student Random ID`) %>%
            rename(Last.Term.Cum.GPA = Cum.GPA) %>%
              right_join(Filtered.Data1) %>%
                  group_by(`Student Random ID`) %>%
                    slice(n()) %>%
                      ungroup() %>%
                        ggplot() +
                        geom_boxplot(aes(x = Last.Term.Cum.GPA, color = `First.Course`)) +
                        theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
                        labs(x = "Final Cumulative GPA", y = "Course Code", color = "First Course", title = "Boxplots of Final Cumulative GPA for all Students that \n Passed Filtration Conditions, Colored by First Course")
```

##PLOT STRATA JUSTIFICATION

As can be seen in the above boxplot, the students that begin in each possible course have inherently different final cumulative GPA distributions. We must then account for this discrepancy when determining the effects that the first professor has on cumulative GPA. We wish to create a linear mixed-effects model that accounts for the projection of future cumulative GPA imparted by the first course a student takes. To do so, we will split students into strata by first course. The courses that have at least 30 students start in it will be ruled sufficient to create a reasonable projection for the mean student. For each student, the increase/decrease per term predicted by the germane linear mixed-effects model will be multiplied by the number of terms the student enrolled in math/chemistry courses at OCC, and added to the initial observation of cumulative GPA - this is the expected outcome. The performance of a student will then be judged by comparing their observed final cumulative GPA to the expected outcome.  In the following chunk we will build these stratified models, and create a statistic called $\Delta C$ which is the aforementioned difference between observed and expected outcome. A positive $\Delta C$ implies that the student's final cumulative GPA exceeded the prediction by the linear mixed effects model. I would like to note that the slope predicted by a linear mixed model will be a constant over the entire stratum. As such, a student that scores a $4.0$ GPA in their first course where the course slope was positive will never be able to meet expectations by the model, and so will erroneously have a negative $\Delta C$ score even if they score straight A's in each math and chemistry course they take at OCC. Similarly, a student that fails the first course with a negative slope cannot obtain a negative $\Delta C$ score. These errors will be largely outweighed by students that score in the middle in the first course, but are important to note. As we will see briefly, the overall error for the mean student in all strata is around $0.01$ GPA points, and so the models are collectively quite good. Later, we will examine alternate statistics for ranking professor influence that are more error adverse, namely withdraw rate and the number of future courses taken (adjusted for stratum).

```{r Create LME4 DeltaC's}
## Create course - specific models of Cum.GPA progression through student's stay at OCC in Filtered.Data using Cum.GPA as a predictor. First, obtain a confidence interval for the effect of Student.Term on the mean student for superposition onto those courses that had a measurable non-significant deviation from the mean effect on student.term.

##Overall model for students 
Cum.lmer.model2 <- lmer(Cum.GPA ~ Student.Term + (1|`Student Random ID`), data = Filtered.Data1)

##First need to create tibble containing the variables (1) Course Code, (2) LMER slope for all courses that had a significant effect. If the course had no significant effect the slope will be the mean slope defined in the previous chunk.
n.courses <- length(unique(Filtered.Data3$`Course Code`))
course.slopes <- tibble(First.Course = unique(Filtered.Data3$`Course Code`), 
                        Course.LMER.slope = rep(0, n.courses))

## Grab lmer slopes for significant courses
set.sig.Course.LMER.Slopes <- function(list){
  
  temp.course.slopes <- course.slopes
  
  for(i in list){
    course.slope <- course.summaries[[i]][2,1]
    temp.course.slopes$Course.LMER.slope[temp.course.slopes$First.Course == i] <- course.slope
  }
  return(temp.course.slopes)
}

course.slopes <- set.sig.Course.LMER.Slopes(significant.courses.positive)
course.slopes <- set.sig.Course.LMER.Slopes(significant.courses.negative)

##Superimpose mean onto those courses that had at least 30 observations, but did not have significant deviation from the mean 

set.mean.Course.LMER.Slopes <- function(list){
  
  temp.course.slopes <- course.slopes
  
  for(i in list){
    course.slope <- summary(Cum.lmer.model2)$coefficients[2,1]
    temp.course.slopes$Course.LMER.slope[temp.course.slopes$First.Course == i] <- course.slope
  }
  return(temp.course.slopes)
}

course.slopes <- set.mean.Course.LMER.Slopes(not.significant)

## Remove non significant courses for upcoming semi-join, then join back to filtered data.
course.slopes <- course.slopes %>%
                    filter(Course.LMER.slope != 0)

##Join significant courses slopes back onto Filtered.Data  
LMER.Slopes.Data <- Filtered.Data3 %>%
                      semi_join(course.slopes, by = "First.Course") %>%
                        left_join(course.slopes, by = "First.Course")

## Determine difference between expected and observed final cumulative GPA in DeltaC, and rejoin into Filtered.Data
LMER.DeltaC.Data <- LMER.Slopes.Data %>% 
                      filter(Student.Term == max.terms) %>%
                        mutate(LMER.Predict = First.Course.GPA + Course.LMER.slope*max.terms) %>%
                          mutate(DeltaC = Last.Term.Cum.GPA - LMER.Predict) %>%
                            dplyr::select(`Student Random ID`, DeltaC) %>%
                              right_join(LMER.Slopes.Data, by = c("Student Random ID"))
```

There are the $19$ remaining courses after filtration, using the requirement that a valid course have at least 30 students begin in it. Here we have created a dataframe consisting of only those students whose mean course effect on future cumulative gpa was measurable - significant or not. For these students we have measured the difference between their final cumulative GPA in the math and chemistry department and the expected final cumulative GPA adjusted for starting course. We will now assign these $\Delta C$ values to the student's first professor, then create several graphs using this statistic. We will furthermore differentiate between the significant mean effects that faculty had on future cumulative GPA and those that we have insufficient data to obtain an accurate estimate for.

```{r Assign LME4 Prof DeltaCs and Make Stats}
##Assign general points

##Represent the Mean DeltaC values for all professors. We require faculty to have at least 8 students that passed filtration begin in one of their courses for measurement.
LMER.DeltaC.Data2 <- LMER.DeltaC.Data %>% 
                      ungroup() %>%
                        group_by(`Student Random ID`) %>%
                          slice(n()) %>%
                            ungroup() %>%
                              dplyr::select(`Student Random ID`, DeltaC, First.Prof) %>%
                                group_by(First.Prof) %>%
                                  filter(n() >= 8)

## Find overall mean of DeltaC. If there is no residual error in our model, this value should be precisely zero. 
overall.mean.DeltaC <- LMER.DeltaC.Data2 %>%
                        pull(DeltaC) %>%
                          mean()

## Find overall standard deviation of DeltaC among all students
overall.sd.DeltaC <- LMER.DeltaC.Data2 %>%
                        pull(DeltaC) %>%
                          sd()

##Create sample size column for t test, and standard error
LMER.DeltaC.Data3 <- LMER.DeltaC.Data2 %>%
                      group_by(First.Prof) %>%
                        summarise(Mean.DeltaC = mean(DeltaC), sample.size = n(), standard.error = sd(DeltaC)/sqrt(n()))

## Find the average of the mean DeltaC's amongst all students taught first by faculty members
prof.mean.DeltaC <- LMER.DeltaC.Data3 %>%
                      pull(Mean.DeltaC) %>%
                        mean()

## Find standard deviation of mean DeltaC among faculty
prof.sd.DeltaC <- LMER.DeltaC.Data3 %>%
                    pull(Mean.DeltaC) %>%
                      sd()

## Find mean standard error amongst faculty mean Delta C's
prof.mean.se.DeltaC <- LMER.DeltaC.Data3 %>%
                        pull(standard.error) %>%
                          mean()

## T test procedure found at https://libguides.library.kent.edu/SPSS/OneSampletTest
## Carry out t test for those faculty members that had at least 
LMER.DeltaC.Data4 <- LMER.DeltaC.Data3 %>%
                      filter(sample.size >= 8) %>%
                        mutate(requisite.tval = qt(0.95, sample.size-1), observed.tval = abs((Mean.DeltaC-overall.mean.DeltaC))/standard.error) %>%
                         mutate(significant = if_else(observed.tval > requisite.tval, TRUE, FALSE, FALSE), Prof.Quality = if_else(Mean.DeltaC > overall.mean.DeltaC, "Good", "Poor"))

head(LMER.DeltaC.Data4, 10)

##Determine Faculty quality for those professors that had significant effects on future cumulative gpa
Prof.Qualities <- LMER.DeltaC.Data4 %>%
                    filter(significant == TRUE) %>%
                        pull(Prof.Quality)

# length(Prof.Qualities) is the total number of faculty that had significant effect on future cumulative GPA
# length(Prof.Qualities[Prof.Qualities == "Good"]) is the total number of faculty that had significant positive effect on future cumulative GPA. This number can be subtracted from the line above to find total number of faculty that had significant negative effect on future cumulative GPA.
```

The significance test I have just ran employs the statistic $\Delta C$, which is calculated as a student's observed final cumulative GPA minus the expected final cumulative GPA predicted by the germane of the linear models given the student's first recorded value of GPA. That is, $\Delta C$ is the deviation of an individual student's final cumulative GPA from the mean final cumulative GPA of those students that began in the same course (e.g. Trigonometry). I will interchangeably refer to this statistic as $\Delta C$ and the longer definition.

The overall error in our course-corrected linear mixed-effects model after all filtration is `r overall.mean.DeltaC ` GPA points, with mean standard errors . That is, if the linear mixed-effects model perfectly described the data, the output $\Delta C$ would have population mean $0$. The observed population mean for $\Delta C$ is `r overall.mean.DeltaC`, which means that either: 1) there is at least one confounding variable that affects a student's cumulative GPA, or that 2) the term dependency of a student's cumulative GPA is not linear. I am personally more inclined to believe the latter, because GPA is bounded on the interval $[0,4]$, and so cannot perfectly follow an affine path over an arbitrary number of semesters unless the slope were exactly zero. That said, for the mean student taking the mean number of terms the models are pretty good. Now that we know the overall mean value for $\Delta C$, we can use the one sample t-test and bootstrap confidence intervals with $\alpha = 0.01$ to determine whether a given faculty member had a mean positive or negative on their students final cumulative GPA's. I want to be clear: the $\Delta C$ statistic I've created is not immune to harsh/lenient grading by the first professor, because it is measured assuming that the first observed GPA value was an accurate intercept for the linear mixed effects model. That said, the model attempts to correct for individual student variance by starting the affine from the first observed GPA value.

In order to apply a one sample t-test using the test statistic $t = \frac{\bar{x} - \mu}{s_e}$, we must have 3 things: 
1) A sample mean. In my code I have labeled this as a variable of DeltaC.Data called Mean.DeltaC. It is the mean deviation from the course corrected linear mixed model experienced by all the students that started with the observed faculty member. 
2) A sample standard error. This is calculated as the sample standard deviation of DeltaC values divided by the square root of the number of students that started with the observed faculty member.
3) A population mean or estimate for population mean. In our case we have obtained an estimate for population mean because of all the filtration that was necessary. 

The one sample t-test is carried out for all faculty members that had at least 3 students start with them. Significance is determined by whether the absolute value of the observed t statistic surpassed the $0.95$ t quantile with degrees of freedom equal to the number of students that started with the faculty minus one. Results of the one sample t-test are stored in the dataframe DeltaC.Data3. In total, there were `r length(Prof.Qualities)` faculty with significant effects on future cumulative GPA for the students that had them first out of the original `r length(unique(Gen_Data$`Faculty Random ID`))` faculty members. Of those `r length(Prof.Qualities)`, `r length(Prof.Qualities[Prof.Qualities == "Good"])` faculty made a positive difference in future cumulative GPA, while `r length(Prof.Qualities)-length(Prof.Qualities[Prof.Qualities == "Good"])` made a $\textit{negative}$ difference in future cumulative GPA. These numbers are very close because they are measured from whether the faculty members students that started with them did better or worse than the applicable of the 23 linear mixed-effects models suggested they would do.  

```{r}
##Course Heights for plots
Course.Heights <- tibble(`Course Code` = unique(Filtered.Data4$First.Course)) %>%
                    arrange(`Course Code`) %>%
                      cbind(tibble(Height = seq(-0.35, 0.35, length.out = length(unique(Filtered.Data4$First.Course)))))

Manual.Comparison.Data <- Manual.DeltaC.Data %>%
                            dplyr::select(`Student Random ID`, DeltaC, First.Course, First.Prof, max.terms) %>%
                              rename(Manual.DeltaC = DeltaC)

LMER.Comparison.Data <- LMER.DeltaC.Data %>%
                          group_by(`Student Random ID`) %>%
                            slice(1) %>%
                              ungroup() %>%
                                dplyr::select(`Student Random ID`, DeltaC, First.Course, First.Prof, max.terms) %>%
                                  rename(LMER.DeltaC = DeltaC)
  
full_join(Manual.Comparison.Data, LMER.Comparison.Data, by = c("Student Random ID", "First.Course", "First.Prof", "max.terms")) %>%
  mutate(Delta.DeltaC = Manual.DeltaC - LMER.DeltaC) %>%
    dplyr::select(`Student Random ID`, Delta.DeltaC, First.Prof, First.Course) %>%
      left_join(Course.Heights, by = c("First.Course" = "Course Code")) %>%
      ggplot(aes(x = Delta.DeltaC, y = Height)) + 
      geom_point(aes(color = First.Course))


```

Let's summarize the results from this analysis. Once again, in the following when I reference points, I refer to the difference between the expected final cumulative GPA and the observed final cumulative GPA as predicted by the linear mixed-effects model for students that started in the relevant course. In a nutshell, the professors for which I could distill inference had a range of effects from `r DeltaC.Data3 %>% filter(significant == TRUE) %>% pull(Mean.DeltaC) %>% min()` cumulative GPA points to `r DeltaC.Data3 %>% filter(significant == TRUE) %>% pull(Mean.DeltaC) %>% max()` with standard deviation `r DeltaC.Data3 %>% filter(significant == TRUE) %>% pull(Mean.DeltaC) %>% sd()`. The mean effect for those professors that had significant effect was `r DeltaC.Data3 %>% filter(significant == TRUE) %>% pull(Mean.DeltaC) %>% mean()` points. I have graphed the effects below.

```{r DeltaC Plots, warning=FALSE}

##Boxplots of Difference between Expected and Observed Cumulative GPA as predicted by linear mixed effects models for each course, colored by course code, for students that obeyed filtration assumptions
LMER.DeltaC.Data %>%
  ungroup() %>%
    group_by(`Student Random ID`) %>%
      slice(1) %>%
        ungroup() %>%
          ggplot(aes(color = First.Course)) +
          geom_boxplot(aes(x = DeltaC), alpha = 0.9) +
          labs(x = "\u0394C (GPA Points)", y = "First Course Code", color = "Course Code", title = "Boxplots of \u0394C for each Individual Student that Passed\nFiltration, Colored by First Course Code.") +
          theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
          scale_x_continuous(limits = c(-4,4))

##Histogram of significant mean professor effects onto final cumulative GPA for students that obeyed filtration assumptions.
LMER.DeltaC.Data4 %>%
  filter(significant == TRUE) %>%
    dplyr::select(First.Prof, Mean.DeltaC, standard.error) %>%
      mutate(Hist.Colors = factor(if_else(Mean.DeltaC < mean(pull(filter(DeltaC.Data3, significant == TRUE), Mean.DeltaC)), "green", "red"))) %>%
        ggplot() +
        geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 22) +
        labs(x = "T-Significant Mean \u0394C (GPA Points) among those students whom started with a professor and passed filtration", y = "Count", title = "Histogram of T-Significant Professor Mean \u0394C's, Filled by Effect\nPolarity from the Mean") +
        theme(legend.position = "none")

Experiment.One.Mean.DeltaCs %>%
  mutate(Hist.Colors = factor(if_else(Mean.DeltaC < mean(pull(filter(DeltaC.Data3, significant == TRUE), Mean.DeltaC)), "green", "red"))) %>%
        ggplot() +
        geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 22) +
        labs(x = "T-Significant Mean \u0394C (GPA Points) among those students whom started with a professor and passed filtration", y = "Count", title = "Histogram of T-Significant Professor Mean \u0394C's, Filled by Effect\nPolarity from the Mean") +
        theme(legend.position = "none")

##Histogram of ALL mean professor effects onto final cumulative GPA for students that obeyed filtration assumptions. This graph is just to put the previous one in context, not for inference.
DeltaC.Data3 %>%
    dplyr::select(First.Prof, Mean.DeltaC, standard.error) %>%
      mutate(Hist.Colors = factor(if_else(Mean.DeltaC < overall.mean.DeltaC, "green", "red"))) %>%
        ggplot() +
        geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 22) +
        labs(x = "Mean \u0394C (GPA Points)", y = "Count", title = "Histogram of ALL Mean Professor \u0394C Effects, Filled by Effect\nPolarity from the Mean") +
        theme(legend.position = "none")

##Normal probability plot for T-Significant DeltaC
DeltaC.Data3 %>%
  filter(significant == TRUE) %>%
    pull(Mean.DeltaC) %>%
     qqnorm(main = "Normal Q-Q Plot for Significant Mean Professor \u0394C Effects")

##Normal probability plot for All DeltaC
DeltaC.Data3 %>%
  pull(Mean.DeltaC) %>%
   qqnorm(main = "Normal Q-Q Plot for Significant Mean Professor \u0394C Effects")

##Course Mean DeltaC Values for all students that passed filtration assumptions
Course.DeltaC.Means <- Filtered.Data5 %>%
                        group_by(`Student Random ID`) %>%
                          slice(1) %>%
                            ungroup() %>%
                              group_by(First.Course) %>%
                                summarise(Course.Mean.DeltaC = mean(DeltaC), Course.Sd.DeltaC = sd(DeltaC))

##Scatterplot for only significant mean DeltaC values as measured by a one-sample t test onto each of the 19 strata
Filtered.Data5 %>%
  ungroup() %>%
    group_by(`Student Random ID`) %>%
      slice(n()) %>%
        dplyr::select(`Student Random ID`, First.Course, First.Prof, DeltaC, Course.Code.Mean.GPA) %>%
          ungroup() %>%
            group_by(First.Prof, First.Course) %>%
              summarise(Mean.DeltaC = mean(DeltaC, na.rm = TRUE), n = n()) %>%
                filter(n >= 3) %>%
                  left_join(Course.DeltaC.Means, by = "First.Course") %>%
                    mutate(requisite.tval = qt(0.95, n-1), observed.tval = abs(Mean.DeltaC - Course.Mean.DeltaC)/(Course.Sd.DeltaC/sqrt(n))) %>%
                      mutate(significant = if_else(observed.tval > requisite.tval, TRUE, FALSE)) %>%
                        filter(significant == TRUE) %>%
                          rename(`Faculty Random ID` = First.Prof, `Course Code` = First.Course) %>%
                            ungroup() %>%               
                              left_join(Course.Heights, by = "Course Code") %>%
                                ggplot() +
                                geom_point(aes(x = Mean.DeltaC, y = Height, color = `Course Code`, size = n), position = position_jitter(height = 0.005)) +
                                labs(x = "T-Significant Mean \u0394C (GPA Points)", y = "Class", n = "Number of Students", color = "Course Code", n = "Number of Students", title = "Scatterplot of Mean Faculty \u0394C Effects by Course\nfor T-Significant Courses") +
                                scale_x_continuous(limits = c(-2,2))
           
##Scatterplot of ALL mean DeltaC values for each of the 19 strata
Filtered.Data5 %>%
  ungroup() %>%
    group_by(`Student Random ID`) %>%
      slice(n()) %>%
        dplyr::select(`Student Random ID`, First.Course, First.Prof, DeltaC, Course.Code.Mean.GPA) %>%
          ungroup() %>%
            group_by(First.Prof, First.Course) %>%
              summarise(Mean.DeltaC = mean(DeltaC, na.rm = TRUE), n = n()) %>%
                filter(n >= 3) %>%
                  left_join(Course.DeltaC.Means, by = "First.Course") %>%
                    rename(`Faculty Random ID` = First.Prof, `Course Code` = First.Course) %>%
                      ungroup() %>%               
                        left_join(Course.Heights, by = "Course Code") %>%
                          ggplot() +
                          geom_point(aes(x = Mean.DeltaC, y = Height, color = `Course Code`, size = n), position = position_jitter(height = 0.005)) +
                          labs(x = "Mean \u0394C (GPA Points)", y = "Class", size = "Number of Students", color = "Course Code", title = "Scatterplot of Mean Faculty \u0394C Effects by Course\nfor ALL Courses") +
                          scale_x_continuous(limits = c(-2,2))


##Vector containing the mean number of students that a professor taught for a single course, used in below paragraphs   
Mean.Students <- Filtered.Data5 %>%
                  ungroup() %>%
                    group_by(`Course Code`, `Faculty Random ID`) %>%
                      count() %>%
                        ungroup() %>%
                          group_by(`Faculty Random ID`) %>%
                            summarise(Mean.Students = mean(n)) %>%
                              pull(Mean.Students)

DeltaC.Data %>%
  mutate(First.Course = as.factor(First.Course)) %>%
    group_by(First.Course) %>%
      summarise(Course.Mean.DeltaC = mean(DeltaC)) %>%
         ggplot(aes(x = fct_reorder(First.Course, -Course.Mean.DeltaC), y = Course.Mean.DeltaC)) +
          theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
          geom_bar(aes(fill = First.Course), stat = "identity") +
          labs(x = "Course Code", y = "Course Mean \u0394C", title = "Bar Chart of Course Mean \u0394C's")

##Statistics about Mean.Students vector
##mean(Mean.Students) is the mean number of students taught by a single faculty member for a single course code.
##sd(Mean.Students) is the standard deviation of the number of students taught by a single faculty member for a single course code.
```

```{r}
Filtered.Data

```

The graph of all mean differences (including professors without significant effect) is included to clarify why a dip exists in the histogram of only significant mean differences. By the nature of the beast non-significant mean differences occur around the mean value of differences in final cumulative GPA, which is why there seems to be a missing bin in the histogram of significant effects. The distribution of this random variable is quite Gaussian, as can be seen in the probability plot above and the overall shape of the histograms. 

  The boxplot I have included is a plot of the individual student's $\Delta C$ values in the 19 strata wherein at least 30 students started in the course. The courses with more students had a wider range of student outcomes as would be expected. There is no IQR for a single course that is drastically different from any other. This is also not surprising because the linear mixed-effects models take the outcome of the mean student to be the expected value, which invariably causes the mean difference for each course before filtration to be zero. The variation observed by our filtration specifications may simply be random noise. One notable course that may be of interest is CHE-1520, the filtered students of which had an unusually high number of positive outcomes. This may just be variance, because CHE-1520 only had `r DeltaC.Data %>% right_join(Gen_Data9, by = "Student Random ID") %>% group_by(`Student Random ID`) %>% slice(n()) %>% ungroup() %>% group_by(First.Course) %>% count() %>% filter(First.Course == "CHE-1520") %>% pull(n)` students start in the course, which is low with respect to the mean of `r DeltaC.Data %>% right_join(Gen_Data9, by = "Student Random ID") %>% group_by(`Student Random ID`) %>% slice(n()) %>% ungroup() %>% group_by(First.Course) %>% count() %>% filter(n >= 30) %>% pull(n) %>% mean()` students. 
  
  The first scatterplot contains the mean $\Delta C$ values for the professors the students had first. The courses which the professors taught are separated and then checked for significance at the $\alpha = 0.05$ level. Each professor may have taught several different types of courses, and so their mean $\textit{for each course}$ is represented as a value in the boxplot for that course. Because the average faculty member taught `r mean(Mean.Students)` with a standard deviation of `r sd(Mean.Students)`, it is expected that many faculty-courses will not have enough data to prove significant. Indeed,  
  
  By comparison with the previous boxplot, we can gain some insight into the distribution of the $\Delta C$ values. For example, looking at  CHE-1520, it seems that the students that started in CHE-1520 really did have a higher future cumulative GPA, as the mean $\Delta C$ values for each of the five faculty that taught at least one CHE-1520 course are all above 0. It is important to note that this is the only case of all-positive $\Delta C$ values on this plot. However, none of the t values for these faculty teaching this course d met the required $0.95$ quantile, and so we cannot significantly indicate this result as real.

Now we will bootstrap the individual values of difference between expected and observed final cumulative GPA for each professor as an alternative significance test, and we will note the number of professors whose significance changed between experiments. This will determine our confidence in the initial ranking.


```{r}
##Bootstrap for each faculty member that taught students that passed filtration assumptions. This chunk creates the necessary functions.
Boot.Data <- Filtered.Data5 %>%
              ungroup() %>%
                group_by(`Student Random ID`) %>%
                  slice(n()) %>%
                    dplyr::select(`Student Random ID`, DeltaC, First.Prof, First.Course)

##Initialize list of faculty that passed filtration conditions
Filtered.Profs <- unique(Boot.Data$First.Prof)

##Record mean DeltaC over all students that passed filtration conditions. Also create boot.data 
global.mean <- mean(Boot.Data$DeltaC)

##Create bootstrapper function which creates a bootstrap distribution for a given faculty member either overall or by course code
make.bootstrap.means <- function(prof.num, course.code = NA) {
  if(is.na(course.code)){
    og.sample <- Boot.Data$DeltaC[Boot.Data$First.Prof == prof.num]
  } else{
    og.sample <- Boot.Data$DeltaC[Boot.Data$First.Prof == prof.num & Boot.Data$First.Course == course.code]
  }
  samp.size <- length(og.sample)
  boot.means <- rep(0,10^4)
    for(i in seq_along(boot.means)){
      boot.sample <- sample(og.sample, replace = TRUE, size = samp.size)
      boot.means[i] <- mean(boot.sample)
    }
  boot.means <- tibble(DeltaC.Boot.Means = boot.means)
  return(boot.means)
}

##Create observed mean grabber for both overall and course-specific bootstraps
get.observed.mean <- function(course.code = NA){
  if(is.na(course.code)){
  observed.DeltaCs <- Boot.Data %>%
                          pull(DeltaC)
  } else{
    observed.DeltaCs <- Boot.Data %>%
                        filter(First.Course == course.code) %>%
                          pull(DeltaC)
  }
  return(mean(observed.DeltaCs))
}
```

```{r}
##Make significance dataframes for general bootstraps. The column Significant 2 notes that this significance test may create different results than the one sample t-test. We will use \alpha = 0.05 as we did for the t-test. This  chunk runs slowly due to the fact that it needs to run 10^4 * 10^3 = ten million simulations for accuracy. Increasing runtime will decrease error slightly, but there are exponentially diminishing returns. We need to restrict our bootstraps to at least 8 distinct students so that there is sufficiently rich data. CITATION https://stats.stackexchange.com/questions/33300/determining-sample-size-necessary-for-bootstrap-method-proposed-method

##This is for a faculty's overall mean DeltaC's
Sig.Profs <- Filtered.Data5 %>% 
                ungroup() %>%
                  group_by(`Student Random ID`) %>%
                    slice(1) %>%
                      ungroup() %>%
                        group_by(First.Prof) %>%
                          summarise(Mean.DeltaC = mean(DeltaC), Num.Students = n(), SD.DeltaC = sd(DeltaC)) %>%
                            filter(Num.Students >= 8)

Sig.Profs <- Sig.Profs %>%
                cbind(Significant2 = rep(NA, length(Sig.Profs$First.Prof)))

##This is for the course-specific bootstraps.
Sig.Courses <- Filtered.Data5 %>% 
                  ungroup() %>%
                    group_by(`Student Random ID`) %>%
                      slice(1) %>%
                        ungroup() %>%
                          group_by(First.Prof, First.Course) %>%
                            summarise(Mean.DeltaC = mean(DeltaC), Num.Students = n(), SD.DeltaC = sd(DeltaC)) %>%
                              filter(Num.Students >= 8)

Sig.Courses <- Sig.Courses %>%
                cbind(Significant3 = rep(NA, length(Sig.Courses$First.Prof)))

##Create significance evaluation function to be applied against all faculty in Sig.Profs / Sig.Courses. The output will be a significance observation for one professor.
evaluate.significance <- function(prof.num, course.code = NA){
  current.significance <- NA
  if(is.na(course.code)){
  current.bootstrap.distribution <- make.bootstrap.means(prof.num) %>% 
                                      pull(DeltaC.Boot.Means)
  } else{
    current.bootstrap.distribution <- make.bootstrap.means(prof.num, course.code) %>% 
                                        pull(DeltaC.Boot.Means)
  }

  lower.quantile <- quantile(current.bootstrap.distribution, 0.025)
  upper.quantile <- quantile(current.bootstrap.distribution, 0.975)
  benchmark <- get.observed.mean(course.code)
  
    if(upper.quantile <= benchmark){
      current.significance <- TRUE
    } else if(lower.quantile >= benchmark){
      current.significance <- TRUE
    } else{
      current.significance <- FALSE
    }
  return(current.significance)
}

##Neither sapply nor apply wanted to work so I just manually apply the functions here. This fills out the Significant2 column in Sig.Profs, and the Significant3 column in Sig.Courses.
for(i in seq_along(Sig.Profs$First.Prof)){
  Sig.Profs$Significant2[i] <- evaluate.significance(Sig.Profs$First.Prof[i])  
}

for(i in seq_along(Sig.Courses$First.Prof)){
  Sig.Courses$Significant3[i] <- evaluate.significance(Sig.Courses$First.Prof[i], Sig.Courses$First.Course[i])
}

##Make two test cases for visualization - one where the faculty has a significant mean delta and one where they do not.
sig.test.prof <- Filtered.Profs[1]
nonsig.test.prof <- Filtered.Profs[2]

##Create visualization of a faculty member with a mean DeltaC significantly different from 0. This is bootstrap of all taught courses
sig.test.data <- make.bootstrap.means(sig.test.prof)
sig.test.data %>%
  ggplot() +
  geom_histogram(aes(x = DeltaC.Boot.Means)) +
  geom_vline(xintercept = c(quantile(sig.test.data$DeltaC.Boot.Means, 0.025), quantile(sig.test.data$DeltaC.Boot.Means, 0.975), mean(DeltaC.Data3$Mean.DeltaC)), color = c("red", "red", "blue")) +
  labs(x = "Bootstrap Sample Mean \u0394C", y = "Count", title = "Example Histogram of the Bootstrap Distribution for a Faculty Member with a\nSignificant Mean \u0394C")

##Create visualization of a faculty member with a mean DeltaC NOT significantly different from 0.
nonsig.test.data <- make.bootstrap.means(nonsig.test.prof)
nonsig.test.data %>%
  ggplot() +
  geom_histogram(aes(x = DeltaC.Boot.Means)) +
  geom_vline(xintercept = c(quantile(nonsig.test.data$DeltaC.Boot.Means, 0.025), quantile(nonsig.test.data$DeltaC.Boot.Means, 0.975), mean(DeltaC.Data3$Mean.DeltaC)), color = c("red", "red", "blue")) +
  labs(x = "Bootstrap Sample Mean \u0394C", y = "Count", title = "Example Histogram of the Bootstrap Distribution for a Faculty Member with a\nNon-Significant Mean \u0394C")
```

The general bootstrap process goes as follows:

We can to simulate the sampling distribution to determine whether there is a statistically significant difference between the population Mean $\DeltaC$ for all faculty members and an individual professor's observation of Mean $\Delta C$. We can obtain a more accurate boundary by simulating the sampling distribution using bootstrapping. The general process for the bootstrapping of a population parameter $\theta$ using a given sample containing an estimate $\hat{\theta}$ follows:

1) Create an empty vector $V_{boot}$ with length equal to a large number $n_{sim}$
2) Create a sample with replacement $S_{boot}$ from the original sample whose length is equal to the length of the original sample
3) Store the observed mean of $\hat{\theta}$ from (2) into $V_{boot}$
4) Repeat (2) and (3) $n_{sim}-1$ times.

The resulting vector $V_{boot}$ contains the bootstrap sampling distribution, which is an estimate for the sampling distribution. As such, we can use the bootstrap distribution to create a confidence interval for $\theta$. We then use the built-in quantile() function to create $95\%$ confidence intervals for $\hat{\theta}$. These confidence intervals, represented by red vertical lines in the graphs I have included above, are then checked against the population parameter $\theta$, represented as a blue vertical line. For our case, if the global $\DeltaC$ mean of `r global.mean` is within the $95\%$ confidence interval, then the professor is ruled not significantly different from the mean case. That is, we do not have sufficient data to claim that the effect the faculty in question had on the final cumulative GPA of those students that started with them deviated significantly from the mean effect. That is, if the blue line is outside of the two red lines, the result is significant at the  $\alpha = 0.05$ level.  On the other hand, if the global mean $\DeltaC$ is not within the bound, then we have shown at the $\alpha = 0.05$ level that the individual faculty in question had a different $\textit{population}$ mean $\DeltaC$ than the average faculty member. I would like to emphasize that the bootstrap prediction of significance is adjusted for stratum because the individual observations of $\Delta C$ are drawn from strata-adjusted values of expected final cumulative GPA. We will now compare the bootstrap-derived significance with the one sample t-test derived significance, and use the intersection of these tests to recreate the plots seen in the t-test section.

```{r}
##Analyse the differences between the bootstraps and t-test results. 

##Experiment one
##Merge bootstrap significance to significance found by t-test for comparison.
Double.Profs <- DeltaC.Data3 %>%
                    dplyr::select(First.Prof, significant, Mean.DeltaC, sample.size) %>%
                      left_join(dplyr::select(Sig.Profs,-Mean.DeltaC), by = "First.Prof") %>%
                        mutate(Sig.Agree = if_else(significant == Significant2, TRUE, FALSE)) %>%
                          rename(T.Test.Sig = significant, Bootstrap.Sig = Significant2, num.students = sample.size)


##Determine total proportion of agreements on significance between the t-test and the bootstrap. 
total.agree1 <- Double.Profs %>% 
                filter(Sig.Agree == TRUE) %>%
                  pull(Sig.Agree) %>%
                    sum()
total.cases1 <- length(Double.Profs$Sig.Agree)

prop.agree1 <- total.agree1/total.cases1


##Determine the mean and standard deviation of DeltaC for the faculty that obtained the same significance from the t-test and the bootstrap (POSITIVE DeltaC)
positive.mean1 <- Double.Profs %>%
                  filter(Sig.Agree == TRUE,  Mean.DeltaC > global.mean) %>%
                    pull(Mean.DeltaC) %>%
                      mean()

positive.sd1 <- Double.Profs %>%
                  filter(Sig.Agree == TRUE,  Mean.DeltaC > global.mean) %>%
                    pull(Mean.DeltaC) %>%
                      sd()

##Determine the mean of DeltaC for the faculty that obtained the same significance from the t-test and the bootstrap (NEGATIVE DeltaC)
negative.mean1 <- Double.Profs %>%
                  filter(Sig.Agree == TRUE,  Mean.DeltaC < global.mean) %>%
                    pull(Mean.DeltaC) %>%
                      mean()

negative.sd1 <- Double.Profs %>%
                  filter(Sig.Agree == TRUE,  Mean.DeltaC < global.mean) %>%
                    pull(Mean.DeltaC) %>%
                      sd()

##Overall standard deviation for faculty that had doubly-significant deviation from the average mean DeltaC
significant.sd1 <- Double.Profs %>%
                    filter(Sig.Agree == TRUE, Bootstrap.Sig == TRUE) %>%
                      pull(Mean.DeltaC) %>%
                        sd()

##Experiment Two
##Compare Bootstrap to T-Test
Double.Courses <- Sig.Courses %>%
                    left_join(Course.DeltaC.Means, by = "First.Course") %>%
                      mutate(standard.error = Course.Sd.DeltaC/sqrt(Num.Students)) %>%
                        mutate(requisite.tval = qt(0.95, Num.Students - 1), observed.tval = abs(Course.Mean.DeltaC - Mean.DeltaC)/(standard.error)) %>%
                          mutate(T.Sig = if_else(observed.tval > requisite.tval, TRUE, FALSE)) %>%
                            dplyr::select(First.Prof, First.Course, Significant3, T.Sig, Num.Students, Mean.DeltaC) %>%
                              mutate(T.Test.Only.Reject = if_else(T.Sig == FALSE & Significant3 == TRUE, TRUE, FALSE), Bootstrap.Only.Reject = if_else(T.Sig == TRUE & Significant3 == FALSE, TRUE, FALSE), Sig.Agree = if_else(T.Sig == FALSE & Significant3 == FALSE | T.Sig == TRUE & Significant3 == TRUE, TRUE, FALSE)) %>%
                                ungroup() %>%
                                  left_join(Course.DeltaC.Means, by = "First.Course")

##Determine total proportion of agreements on significance between the t-test and the bootstrap. 
total.agree2 <- Double.Courses %>% 
                  filter(Sig.Agree == TRUE) %>%
                    pull(Sig.Agree) %>%
                      sum()

total.cases2 <- length(Double.Courses$Sig.Agree)

prop.agree2 <- total.agree2/total.cases2


##Determine the mean and standard deviation of DeltaC for the faculty that obtained the same significance from the t-test and the bootstrap (POSITIVE DeltaC)
positive.mean2 <- Double.Courses %>%
                  filter(Sig.Agree == TRUE,  Mean.DeltaC > Course.Mean.DeltaC) %>%
                    pull(Mean.DeltaC) %>%
                      mean()

positive.sd2 <- Double.Courses %>%
                  filter(Sig.Agree == TRUE,  Mean.DeltaC > Course.Mean.DeltaC) %>%
                    pull(Mean.DeltaC) %>%
                      sd()

##Determine the mean of DeltaC for the faculty that obtained the same significance from the t-test and the bootstrap (NEGATIVE DeltaC)
negative.mean2 <- Double.Courses %>%
                  filter(Sig.Agree == TRUE,  Mean.DeltaC < Course.Mean.DeltaC) %>%
                    pull(Mean.DeltaC) %>%
                      mean()

negative.sd2 <- Double.Courses %>%
                  filter(Sig.Agree == TRUE,  Mean.DeltaC < Course.Mean.DeltaC) %>%
                    pull(Mean.DeltaC) %>%
                      sd()

##Overall standard deviation for faculty that had doubly-significant deviation from the average mean DeltaC
significant.sd2 <- Double.Courses %>%
                    filter(Sig.Agree == TRUE, Significant3 == TRUE) %>%
                      pull(Mean.DeltaC) %>%
                        sd()
```



```{r}
##Table of differences between t-test significance and overall bootstrap significance - experiment one.
Double.Profs %>%
  mutate(T.Test.Only.Reject = if_else(T.Test.Sig == FALSE & Bootstrap.Sig == TRUE, TRUE, FALSE), Bootstrap.Only.Reject = if_else(Bootstrap.Sig == FALSE & T.Test.Sig == TRUE, TRUE, FALSE)) %>%
      group_by(T.Test.Only.Reject, Bootstrap.Only.Reject) %>%
        count()
```

The above table summarizes the differences between the overall one sample t-test and bootstrap significance measures for all faculty that had at least eight students begin in their courses and pass filtration. To be clear, the students and their respective $\Delta C$'s for any given faculty are conglomerated into a single mean $\Delta C$. I will hereafter refer to this method equivalently as experiment one. We will now discuss some statistics surrounding this comparison. In the vast majority of cases, the bootstrap significance test agrees with the t-test. The significance implied by the t-test and the bootstrap agreed for `r total.agree1` out of `r total.cases1` faculty members, which is approximately  `r prop.agree1 *100`$\%$ of all professors.  $7$ discrepancies occurred where the t-test implied that the observation of mean $\Delta C$ for a faculty member was significantly different from the overall average mean, while the bootstrap implied that it was not. There were no observed cases where the t-test rejected significance while the bootstrap failed to reject. We would expect more bootstrap rejections than t-test rejections if the sample data were not sufficiently normal due to the sample sizes being too small. We required at least eight students to start with a faculty member, which helps with the minimum sample size issue, but there is still high variance in $\Delta C$ between students.

Here are some relevant statistics regarding experiment one.  Among the faculty whose significance agreed and had positive mean $\Delta C$ values, the average mean value for $\Delta C$ was `r positive.mean1` and the standard deviation was `r positive.sd1`. As for the faculty whose significance agreed but had negative mean $\Delta C$ values, the average mean $\Delta C$ was `r negative.mean1` with a standard deviation of `r negative.sd1`. The average standard deviation of $\Delta C$ for a faculty member that had a significantly different $\Delta C$ from the overall mean was `r significant.sd1`. The sample size (total number of students that passed filtration and started with a professor) had mean `r Double.Profs %>% ungroup() %>% pull(num.students) %>% mean()` and standard deviation `r Double.Profs %>% ungroup() %>% pull(num.students) %>% sd()`. The distribution of the sample size is therefore clearly not Gaussian. We will see the distribution of sample size along with a few recreations of graphs in the t-test section in some graphs below. The experimental error rate for experiment one - where such an error is measured by the bootstrap and t-test disagreeing on the statistical significance of a faculty - was $\frac{7}{238}*100\% = 2.94\%$. This is a bit high in terms of data loss as we lose ratings on seven faculty members, though it's certainly better safe than sorry. The overall accuracy of the experiment increased because we eliminated seven potentially invalid results. 

In an effort to be as conservative as possible with inference, we will now recreate the previous plots [CITE WHICH] using only those faculty members endorsed by both confidence intervals in experiment one. These plots will not change much as compared to the t-test only plots, because only seven additional professors are to be omitted. Additionally, I will include a plot of sample size and a plot showing the relationship between the number of students that started with a faculty and the mean $\Delta C$. 

```{r}
##Recreate graphs for doubly-significant faculty: Experiment One

##Determine which faculty members had doubly significant effect onto their students.
Doubly.Sig.Profs <- Double.Profs %>%
                      filter(Sig.Agree == TRUE, Bootstrap.Sig == TRUE) %>%
                        dplyr::select(First.Prof)

##Rejoin doubly significant faculty. It is sufficient to just filter the above three lines without the select command, but I wanted to call DeltaC.Data3 for continuity.
DeltaC.Data4 <- DeltaC.Data3 %>%
                    ungroup() %>%
                      semi_join(Doubly.Sig.Profs, by = "First.Prof")

##Histogram of doubly-significant mean professor effects onto final cumulative GPA for students that obeyed filtration assumptions.
DeltaC.Data4 %>%
  mutate(Hist.Colors = factor(if_else(Mean.DeltaC < mean(pull(DeltaC.Data4, Mean.DeltaC)), "green", "red"))) %>%
        ggplot() +
        geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 22) +
        labs(x = "Doubly-Significant Mean \u0394C (GPA Points)", y = "Count", title = "Histogram of Doubly-Significant Mean \u0394C's for Experiment One,\nFilled by Effect Polarity from the Mean") +
        theme(legend.position = "none")


##Normal probability plot for Experiment One's Doubly-Significant Mean DeltaC's
DeltaC.Data4 %>%
  pull(Mean.DeltaC) %>%
    qqnorm(main = "Normal Q-Q Plot for Experiment One Mean Professor \u0394C's")

##Number of Faculty represented in below histogram compared to total
Filtered.Data5 %>%
  ungroup() %>%
    group_by(`Student Random ID`) %>%
      slice(1) %>%
        ungroup() %>%
          group_by(First.Prof) %>%
            summarise(sample.size = n()) %>%
              mutate(Sufficient.Sample.Size = if_else(sample.size >= 8, TRUE, FALSE)) %>%
                ungroup() %>%
                  group_by(Sufficient.Sample.Size) %>%
                    count()

##Histogram of sample sizes - greater than or equal to eight - for experiment one.
Double.Profs %>%
  ggplot(aes(x = num.students)) +
  geom_histogram(bins = 25) +
  geom_boxplot(color = "red", fill = "green", width = 2, lwd = 1.5) +
  scale_x_continuous(limits = c(0,150), breaks = seq(from = 0, to = 150, by = 25)) +
  labs(x = "Number of Students", y = "Count", title = "Histogram and Boxplot of Sample Sizes for Experiment One.")

##Experimental Plot: Mean.DeltaC Vs. Number of Students. Do good/bad teachers teach more students?
##Determine the total number of students taught by each faculty
Overall.Num.Students <- Gen_Data9 %>%
                          ungroup() %>%
                            group_by(`Faculty Random ID`) %>%
                              count() %>%
                                rename(Overall.Students = n)
##Join total number of students taught to data frame containing only those faculty that had significant Mean DeltaC
Double.Profs %>%
  filter(Sig.Agree == TRUE, T.Test.Sig == TRUE ) %>%
    mutate(Point.Colors = if_else(Mean.DeltaC < mean(Double.Profs$Mean.DeltaC), "green", "red")) %>%
      dplyr::select(First.Prof, Mean.DeltaC, Num.Students, Point.Colors) %>%
        left_join(Overall.Num.Students, by = c("First.Prof" = "Faculty Random ID")) %>%
          ggplot(aes(x = Overall.Students, y = Mean.DeltaC)) +
          geom_point(aes(color = Point.Colors)) +
          theme(legend.position = "none") +
          labs(x = "Total Number of Students Taught", y = "Doubly-Significant Mean \u0394C (GPA Points)", title = "Scatterplot of Faculty Mean \u0394C Vs. Number of Students that Passed\nFiltration and Started with the Faculty")


```



```{r}
##Table of differences between t-test and bootstrap for course-specific experiment two.
Double.Courses %>%
  group_by(T.Test.Only.Reject, Bootstrap.Only.Reject) %>%
    count()
```

```{r}
##Recreate graphs for doubly-significant faculty: Experiment Two
##Histogram of Distribution of Doubly-Significant Mean Delta C's
Double.Courses %>%
  filter(Sig.Agree == TRUE, T.Sig == TRUE) %>%
    mutate(Hist.Colors = factor(if_else(Mean.DeltaC < Course.Mean.DeltaC, "green", "red"))) %>%
      ggplot() +
      geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 21) +
      labs(x = "Course's Doubly-Significant Mean \u0394C (GPA Points)", y = "Count", title = "Histogram of Doubly-Significant Mean \u0394C for Experiment Two,\nFilled by Effect Polarity from the Mean") +
      theme(legend.position = "none")

##Normal probability plot for Experiment Two's Doubly-Significant Mean DeltaC's
Double.Courses %>%
  filter(Sig.Agree == TRUE, T.Sig == TRUE) %>%
    pull(Mean.DeltaC) %>%
      qqnorm(main = "Normal Q-Q Plot for Course's Doubly-Significant Mean \u0394C's")


##Scatterplot of doubly-significant course specific mean DeltaC's
Double.Courses %>%
  filter(T.Sig == TRUE, Significant3 == TRUE) %>%
    left_join(Course.Heights, by = c("First.Course" = "Course Code")) %>%
      ggplot(aes(x = Mean.DeltaC, y = Height)) +
      geom_point(aes(color = First.Course, size = Num.Students), position = position_jitter(height = 0.01)) +
      scale_x_continuous(limits = c(-1, 1), breaks = seq(from = -1, to = 1, by = .5)) +
      labs(x = "Course's Doubly-Significant Mean \u0394C (GPA Points)", y = "Course", title = "Scatterplot of All Doubly-Significant Mean \u0394Cs for\nExperiment Two", color = "Course Code", size = "Number of Students")

##Number of Faculty-Courses represented in below histogram
Filtered.Data5 %>%
  ungroup() %>%
    group_by(`Student Random ID`) %>%
      slice(1) %>%
        ungroup() %>%
          group_by(First.Course, First.Prof) %>%
            summarise(sample.size = n()) %>%
              mutate(Sufficient.Sample.Size = if_else(sample.size >= 8, TRUE, FALSE)) %>%
                ungroup() %>%
                  group_by(Sufficient.Sample.Size) %>%
                    count()

## Histogram of Sample size for faculty in individual courses
Double.Courses %>%
  ungroup() %>%
    ggplot(aes(x = Num.Students)) +
    geom_histogram() +
    geom_boxplot(color = "red", fill = "green", width = 20, lwd = 1.25) +
    scale_x_continuous(limits = c(0,150), breaks = seq(from = 0, to = 150, by = 25)) +
    labs(x = "Number of Students", y = "Count", title = "Histogram and Boxplot of Sample Size for Experiment Two")

##Experimental Plot: Mean.DeltaC Vs. Number of Students. Do good/bad teachers teach more students?
##Determine the total number of students taught by each faculty
Course.Num.Students <- Gen_Data9 %>%
                          ungroup() %>%
                            group_by(`Faculty Random ID`, `Course Code`) %>%
                              count() %>%
                                rename(Overall.Students = n)
##Join total number of students taught to data frame containing only those faculty that had significant Mean DeltaC
Double.Courses %>%
  filter(Sig.Agree == TRUE, T.Sig == TRUE ) %>%
    mutate(Point.Colors = if_else(Mean.DeltaC < mean(Double.Profs$Mean.DeltaC), "green", "red")) %>%
      dplyr::select(First.Prof, First.Course, Mean.DeltaC, Num.Students, Point.Colors) %>%
        left_join(Course.Num.Students, by = c("First.Prof" = "Faculty Random ID", "First.Course" = "Course Code")) %>%
          ggplot(aes(x = Overall.Students, y = Mean.DeltaC)) +
          geom_point(aes(color = Point.Colors)) +
          theme(legend.position = "none") +
          scale_x_continuous(limits = c(0,1100), breaks = seq(from = 0, to = 1100, by = 200)) +
          labs(x = "Total Number of Students Taught in Course", y = "Course's Doubly-Significant Mean \u0394C (GPA Points)", title = "Scatterplot of Experiment Two Doubly-Significant Mean \u0394C's Vs. Number of\nStudents that Started with the Faculty")
```

In experiment two, we increase precision to the point of individual courses. Whereas in experiment one $\Delta C$ was averaged across all the students that passed filtration and started with a specific teacher (as long as at least eight students did), in experiment two the average was restricted across students that started with a faculty $\textit{in a course}$ (as long as at least eight students did). The second table shows a larger significance correction than in experiment one. The experimental error rate for experiment two was $\frac{19+29}{647}*100\% = 7.42\%$. This higher rejection rate is predictable because the sample size must necessarily go down when we choose not to average over all a faculty's courses - as long as the faculty taught at least two courses in which students started their path through OCC's math and chemistry departments. The mean sample size in experiment two was only `r  Double.Courses %>% ungroup() %>% pull(Num.Students) %>% mean()` with standard deviation `r Double.Courses %>% ungroup() %>% pull(Num.Students) %>% sd()`. Because the ratio of mean to standard deviation decreased from experiments one to two, we predict the distribution of experiment two's sample sizes to be more skewed than that of experiment one.



```{r}
## This table enumerates the possible values of cumulative GPA for a given student's term assuming that all courses have the same weight. This does not include NA's. This is an estimate, and is not mathematically rigorous. It becomes difficult to determine when means using different numbers will yield the same result - the set {1, 2, 3, 4} gives the same mean as {1, 1, 4, 4} and {2, 2, 3, 3}. As such we use the unique() command to filter out these multiplicities. The high simulation size gives a decent confidence bound for small student terms.
Term.Grades <- Gen_Data %>%
                filter(!is.na(`GPA Assigned`)) %>%
                  arrange(`GPA Assigned`) %>%
                    pull(`GPA Assigned`) %>%
                      unique()

##This function determines the number of unique cumulative GPA's possible for a given term number
Possible.Cum.GPAs <- function(term){
                        n.sims <- 10^6
                        random.cum.gpas <- rep(-1,n.sims)
                        
                        for(i in 1:n.sims){
                              random.cum.gpas[i] <- mean(sample(Term.Grades, size = term, replace = TRUE))
                        }
                        return(unique(random.cum.gpas))
}

##This function determines the overall proportion of students that took n terms at OCC's math and chemistry departments
Term.Prop.Students <- function(term){
                        n.students <- Gen_Data9 %>%
                                        ungroup() %>%
                                          group_by(`Student Random ID`) %>%
                                            slice(1) %>%
                                              ungroup() %>%
                                                filter(max.terms == term) %>%
                                                  pull(max.terms) %>%
                                                    length()
                        prop.students <- n.students/(n.s-3)
                        return(prop.students)
}

##Print out the possible number of GPA values per term for first eight terms. 
Cum.GPA.Data <- tibble(Student.Term = seq(from = 1, to = 8, by = 1), Prop.Students = -1, Possible.Cum.GPAs = -1, )
for(i in 1:8){
  Cum.GPA.Data$Possible.Cum.GPAs[i] <- length(Possible.Cum.GPAs(i)) + 1
  Cum.GPA.Data$Prop.Students[i] <- Term.Prop.Students(i)
}

Cum.GPA.Data
```

##RESULTS AND ERROR DISCUSSION

The overall shape of the plots for doubly-significant effects are not very different from the plots of t-significant effects. There is a slightly larger gap about Mean $\Delta C = 0 $, but beyond that the distributions is essentially identical to the t-significant version. I must now include a note about why the bootstrap and t-test yield different results. The measures have error in different ways. The one sample t-test assumes that:
1. Data come from multiple measurements of a continuously distributed random variable,
2. Any subset of data (an event) is a simple random sample from the overall distribution,
3. The random variable which is being tested has a Gaussian distribution,
4. The sample size is sufficiently large (generally n >= 3), and
5. The random variable has a finite constant standard deviation.
##Citation https://www.investopedia.com/ask/answers/073115/what-assumptions-are-made-when-conducting-ttest.asp
For both the overall and course-specific experiments, our data does not satisfy condition one. Cumulative GPA is a fundamentally discrete beast. For a student with a cumulative GPA $c$, the permitted values for cumulative GPA after one more term depend on c, the allowed course credits, and the allowed GPA's obtainable in a course. For example, for a student that took a single course in their first term (which is the filtration assumption) there are only twelve possible values for cumulative GPA: `r Term.Grades` and the NA which corresponds to a withdraw. I have included a very approximate table containing the number of possible Cumulative GPA's under these assumptions, including NA's. The second term already gives $33$ possible values for cumulative GPA

```{r Initialize Results}
##Creates Results Table called Points Tracker
      
##Points Tracker contains Mean.DeltaC which is the mean effect that the faculty had on the final cumulative GPA of their students, Total.Cum.Effect which is the total effect that the faculty had on final cumulative GPA, Mean.DeltaW which is the difference in withdraw rate between this professor and the average, and Total.W.Effect: the total number of withdraws `caused` by the choice of faculty, and Mean.Delta.N: the mean faculty effect on number of future courses taken in the department.
Points.Tracker <- tibble(`Faculty Random ID` = Sig.Profs$First.Prof, Num.Students = 0, Total.Cum.GPA.Impact = -100000, Overall.DeltaC.Quantile = -1)

## Determine the total effect faculty had in the math / chemistry departments of OCC between 2010 to 2017. Overall.Points ranks faculty based on total effect as a function of Mean DeltaC and number of students that passed filtration and started with the faculty
Overall.Points1 <- Double.Profs %>%
                    filter(Sig.Agree == TRUE, Bootstrap.Sig == TRUE) %>%
                      dplyr::select(First.Prof, Mean.DeltaC, num.students) %>%
                        mutate(Overall.Cum.GPA.Impact = Mean.DeltaC * num.students, Overall.Impact.Rank = -1, Overall.Mean.DeltaC.Rank = -1) %>%
                          arrange(Overall.Cum.GPA.Impact)

##Create single use ranker function for faculty that had significant effect on Final Cumulative GPA for those students that passed filtration and started in one of their courses.
Rank.Overall.Stats <- function(data){
  Temp.Data <- data %>%
                arrange(desc(Overall.Cum.GPA.Impact))
  n.sig.profs <- length(Temp.Data$First.Prof)
  for(i in 1:n.sig.profs){
    Temp.Data$Overall.Impact.Rank[i] <- i
  }
  Temp.Data <- Temp.Data %>%
                arrange(desc(Mean.DeltaC))
  for(i in 1:n.sig.profs){
    Temp.Data$Overall.Mean.DeltaC.Rank[i] <- i
  }
  return(Temp.Data)
}

Overall.Points2 <- Overall.Points1 %>%
                    Rank.Overall.Stats()
head(Overall.Points2)

##Store Faculty's Overall Impact on Final Cumulative GPA to Points.Tracker


## 
Course.Points1 <- Double.Courses %>%
                    filter(Sig.Agree == TRUE, T.Sig == TRUE) %>%
                        dplyr::select(First.Prof, First.Course, Mean.DeltaC, Num.Students) %>%
                          mutate(Course.Cum.GPA.Impact = Mean.DeltaC * Num.Students, Course.Impact.Rank = -1, Course.Mean.DeltaC.Rank = -1)
Sig.Courses <- Course.Points1 %>%
                pull(First.Course) %>%
                  unique()

Rank.Course.Stats <- function(data){
  Temp.Data <- Course.Points1 %>%
                arrange(First.Course, desc(Course.Cum.GPA.Impact))
  
  get.course.indecies <- function(course){
    return(which(Temp.Data$First.Course == course))
  }
  
  for(i in 1:length(Sig.Courses)){
    course.indecies <- get.course.indecies(Sig.Courses[i])
    cntr <- 1
      for(k in course.indecies){
        Temp.Data$Course.Impact.Rank[k] <- cntr 
        cntr <- cntr + 1
      }
  }
  
  Temp.Data <- Temp.Data %>%
                  arrange(First.Course, desc(Mean.DeltaC))
  for(i in 1:length(Sig.Courses)){
    course.indecies <- get.course.indecies(Sig.Courses[i])
    cntr <- 1
      for(k in course.indecies){
        Temp.Data$Course.Mean.DeltaC.Rank[k] <- cntr 
        cntr <- cntr + 1
      }
  }
  
  return(Temp.Data)
}


Course.Points2 <- Course.Points1 %>%
                    Rank.Course.Stats()

head(Course.Points2)

```


```{r}
##Plots of Total Impact as a function of number of students and Mean.DeltaC
Overall.Points2 %>%
  ggplot(aes(x = Mean.DeltaC, y = Overall.Cum.GPA.Impact)) +
  geom_point(aes(color = num.students)) +
  scale_x_continuous(limits = c(-1.1,1.1), breaks = seq(from = -1, to = 1, by = 1))

Course.Points2 %>%
  ggplot(aes(x = Mean.DeltaC, y = Course.Cum.GPA.Impact)) +
  geom_point(aes(color = Num.Students)) +
  scale_x_continuous(limits = c(-1.1,1.1), breaks = seq(from = -1, to = 1, by = 1))



Overall.Points2 %>%
  ggplot(aes(x = num.students, y = Overall.Cum.GPA.Impact)) +
  geom_point(aes(color = Mean.DeltaC))# +
  # scale_x_continuous(limits = c(-1.1,1.1), breaks = seq(from = -1, to = 1, by = 1))

Course.Points2 %>%
  ggplot(aes(x = Mean.DeltaC, y = Num.Students)) +
  geom_point(aes(color = First.Course)) #+
  # scale_x_continuous(limits = c(-1.1,1.1), breaks = seq(from = -1, to = 1, by = 1))
```

##REMOVE DELTA AND SUCH FROM CODE, REWRITE SO ITS NOT NECESSARY. WILL SPEED UP AND PRETTY UP PROGRAM.


3) Do grades generally improve, worsen, or remain constant?

3.5) Is there a trend over the year the class was taught?

Answered in Part (1). Depends on number of future terms in OCC.

Assumption: we take only those students that recieved a GPA assigned for at least one course in the math and chemistry departments. For max.terms > 1 a valid observation can withdraw from the first course as long as they don't from at least one future course. This is a fundamentally different filtration assumption that we did for the first faculty effect on final cumulative GPA. 
```{r}
##Plot effect of max.terms onto final cumulative GPA for terms that had at least 30 students.
Term.Base <- Gen_Data9 %>%
              filter(max.terms == Student.Term, !is.na(Cum.GPA)) %>%
                ungroup()

Term.Means <- Term.Base %>%
                group_by(max.terms) %>%
                  summarise(Term.Mean.Cum.GPA = mean(Cum.GPA, na.rm = TRUE), Num.Students = n()) %>%
                    filter(Num.Students >= 30) %>%
                      mutate(Conf.Int.Low = 0, Conf.Int.High = 0)
                      
Term.confint1 <- function(term, conf.side){
  
  Term.GPAs <- Term.Base %>%
                  filter(max.terms == term) %>%
                    pull(Cum.GPA)
  n.sims <- 10^4
  boot.means <- rep(0, n.sims)
  for(i in 1:n.sims){
    boot.sample <- sample(Term.GPAs, size = length(Term.GPAs), replace = TRUE)
    boot.means[i] <- mean(boot.sample)
  }
  if(conf.side == "Low"){
    return(quantile(boot.means, 0.025))
  }
  if(conf.side == "High"){
    return(quantile(boot.means, 0.975))
  }
}

for(i in 1:length(Term.Means$max.terms)){
  Term.Means$Conf.Int.Low[i] <- Term.confint1(i, "Low")
  Term.Means$Conf.Int.High[i] <- Term.confint1(i, "High")
}

Term.Means

p <- Term.Means %>%
      filter(Num.Students >= 30) %>%
        ggplot(aes(x = max.terms, y = Term.Mean.Cum.GPA)) +
        geom_point(aes(size = Num.Students)) +
        geom_smooth() +
        scale_x_continuous(limits = c(1, 10), breaks = seq(from = 1, to = 10, by = 1)) +
        labs(x = "Number of Terms in Math and Chemistry Departments", y = "Mean Final Cumulative GPA (0-4 Scale)", title = "Mean Final Cumulative GPA Vs. Number of Terms in the\nMath and Chemistry Departments, with 95% Confidence Interval", size = "Students in Category")

for(i in 1:length(Term.Means$max.terms)){
  Conf.Low <- Term.Means$Conf.Int.Low[i]
  Conf.High <- Term.Means$Conf.Int.High[i]
  p <- p + geom_segment(x = i, xend = i, y = Conf.Low, yend = Conf.High, color = "red")
}

p
```

Students take courses in the math and chemistry departments for different number of terms. Oakland Community College only grants 
Associate's degrees in math and chemistry - the highest math course is Linear Algebra, and the highest chemistry course is Organic Chemistry II. While it may be a false equivalency to compare students that enroll in community college to those that enroll in standard four year schools, the highest courses offered at OCC should be completed at the latest by the end of a four-year student's second year assuming the student is majoring in that field. By this rough heuristic we estimate that a student beginning in Calculus 1 should complete OCC's math regime in about four terms at one course per term. A student might not take a math course every term, and community college students that are seeking mathematics degrees may require additional training before calculus. As can be seen in the graph above, the mean final cumulative GPA for those students that take between three and seven courses are essentially identical. Students that complete their stay in OCC's math and chemistry departments within this five course spread have the empirically highest mean cumulative GPA's by the time they finish. Past each extreme of this range mean final cumulative GPA begins to fall. There are two orientations, to each of which I will assign a speculative reason for the lower cumulative observed GPA. Please note the following hypotheses are merely my guesses at the mechanisms, and there are likely additional factors that I have not thought of that affect mean final cumulative GPA by max term.

1. $\textbf{Less than three terms:}$ A student that takes less than three terms in the math and chemistry departments may be testing out community college itself, or perhaps trialing the courses offered in the respective departments. Such students may not be as committed to their learning as those pursuing an associate's degree in math or chemistry. By choosing the students that only take one two terms of math and chemistry, we have retroactively largely filtered out those students that ARE pursuing such a degree, thus lowering the mean final cumulative GPA. An alternate explanation is that if a student does poorly in their first one or two courses in a department, they become less likely to continue study in that field. There would then be a higher concentration of lower grades in the first few terms, as we observe. 

2. $\textbf{Greater than seven terms:}$ Students that are hellbent on obtaining a degree in math or chemistry will take courses in those departments until they either finish the degree or change their conviction. If a student fails multiple courses within the department, they will necessarily need to take additional terms to retake the classes. They will then have both a higher number of terms taken, and a lower cumulative GPA. 

The mean final cumulative GPA for all terms lie within the set (`r min(Term.Means$Term.Mean.Cum.GPA)`, `r max(Term.Means$Term.Mean.Cum.GPA)`) GPA points. The minimum occurs in those students that only take one term in the math and chemistry departments, and the maximum occurs at five terms. The range is only about 0.3 GPA points, so the effect that the total terms taken is rather small, approximately the difference between a C average and a C+ average.

As the number of terms increases, the number of students that were enrolled in the math and chemistry department for that number of terms decreases exponentially and moreover monotonically. I was forced to filter for terms that had at least thirty students finish within them, thus losing a small amount of data (on the order of 50 students). The confidence intervals for the later terms become increasingly wide, to the point where the negative trend observed in terms 8 through 10 is barely significant at the $\alpha = 0.05$ level. Another interesting statistic is the mean change in cumulative GPA per term, which we will look at presently.

```{r}
## The following amounts to the first derivative of the previous graph. 

Max.Terms.Base <- Gen_Data9 %>%
                    ungroup() %>%
                      filter(!is.na(Cum.GPA)) %>%
                        group_by(`Student Random ID`) %>%
                          filter(n() > 1) %>%
                            slice(n()) %>%
                              mutate(Final.Cum.GPA = Cum.GPA) %>%
                                dplyr::select(`Student Random ID`, Final.Cum.GPA) %>%
                                  right_join(Gen_Data9, by = "Student Random ID") %>%
                                    filter(!is.na(Cum.GPA), !is.na(Final.Cum.GPA)) %>%
                                      mutate(Graded.Terms = n()) %>%
                                        ungroup() %>%
                                          group_by(`Student Random ID`) %>%
                                            slice(1) %>%
                                              mutate(GPA.Slope = (Final.Cum.GPA - Cum.GPA)/Graded.Terms) %>%
                                                ungroup()

Max.Term.Mean.GPA.Slopes <-  Max.Terms.Base %>%
                              group_by(max.terms) %>%
                                summarise(Mean.GPA.Slope = mean(GPA.Slope), Num.Students = n()) %>%
                                  filter(Num.Students > 30) %>%
                                    mutate(Conf.Int.Low = NA, Conf.Int.High = NA)

##This function makes a 95% confidence interval for MAx.Term.Mean.GPA.Slopes, the minimum of which will be stored as Cont.Int.Low, and the maximum of which will be stored as Conf.Int.High
Term.confint2 <- function(term, conf.side){
  Term.Slopes <- Max.Terms.Base %>%
                  filter(max.terms == term) %>%
                    pull(GPA.Slope)
  
  n.sims <- 10^4
  boot.means <- rep(0, n.sims)
  for(i in 1:n.sims){
    boot.sample <- sample(Term.Slopes, size = length(Term.Slopes), replace = TRUE)
    boot.means[i] <- mean(boot.sample)
  }
  if(conf.side == "Low"){
    return(quantile(boot.means, 0.025))
  }
  if(conf.side == "High"){
    return(quantile(boot.means, 0.975))
  }
}

for(i in 1:length(Max.Term.Mean.GPA.Slopes$max.terms)){
  Max.Term.Mean.GPA.Slopes$Conf.Int.Low[i] <- Term.confint2(i, "Low")
  Max.Term.Mean.GPA.Slopes$Conf.Int.High[i] <- Term.confint2(i, "High")
}

Max.Term.Mean.GPA.Slopes

p <- Max.Term.Mean.GPA.Slopes %>%
      ggplot(aes(x = max.terms, y = Mean.GPA.Slope)) +
      geom_point(aes(size = Num.Students)) + 
      geom_smooth() +
      scale_x_continuous(limits = c(1,10), breaks = seq(from = 1,to = 10, by = 1)) +
      labs(x = "Number of Terms in Math and Chemistry Departments", y = "Mean Change in Cumulative GPA per Term", title = "Mean Change in Cumulative GPA per Term Vs. Number of\nTerms, with 95% Confidence Intervals", size = "Number of Students")

for(i in 1:length(Max.Term.Mean.GPA.Slopes$max.terms)){
  Conf.Low <- Max.Term.Mean.GPA.Slopes$Conf.Int.Low[i]
  Conf.High <- Max.Term.Mean.GPA.Slopes$Conf.Int.High[i]
  p <- p + geom_segment(x = i, xend = i, y = Conf.Low, yend = Conf.High, color = "red")
}
  
p +
  geom_hline(yintercept = 0, color = "green")
```

This plot compares the mean change in a student's GPA against the number of terms that they took courses in the math and chemistry departments. This statistic is measured as the difference between final cumulative GPA and the first recorded observation of GPA, divided by the number of graded terms.  As in the previous figure, [GIVE NUMBER] 10 is the highest term number for which there is sufficient data for inference. I did not have any guesses regarding whether certain terms would yield positive or negative effects, so we have used a two-sided confidence interval. These will be compared about the expected effect of zero. If we let the variable measured on the vertical axis be $\Delta_t$ he hypotheses we will test for significance for each term are:
$$H_0: \Delta_t = 0$$
$$H_A: \Delta_t \neq 0$$
For all terms except 10, at the $\alpha = 0.05$ level we reject the null hypothesis in favor of the alternate. Because we were using a two sided confidence interval, we have shown that for terms one through nine that $\Delta_t < 0$. The $95\%$ confidence interval for term 10 intersects with the green line demarking the null hypothesis, so for term 10 we fail to reject $H_0$. We are running ten tests at the $\alpha = 0.05$ level, and so we would expect roughly one in twenty experiments to have a population mean outside of the confidence interval. For most of these CI's, the boundary is rather tight and so even in the case of such errors, the population mean is unlikely to usurp the validity of our results. The one exception may be term one, where ggplot2's standard error shading $\textit{just barely}$ intersect the green line demarking $\Delta_t$ = 0.

Using these plots we have shown that students with the highest mean final cumulative GPA tend to take between three and seven terms in the math and chemistry departments, and that those students also have a downwards trend in their cumulative GPA from their entry into the departments to the last course they take. I have included printouts of the datasets used for these analyses for consideration.




4) Do some teachers have significantly higher Withdraw rates than other professors? (p = .01)
  - Which courses have significantly higher withdraw rates?
  - Do the professors with significantly higher/lower withdraw rates match up with the professors rated "good"/"poor" by our Cum.GPA and Delta analyses?

NO POSSIBLE T TEST BECAUSE DATA IS BINOMIAL - MUST USE BOOTSTRAP

```{r Make DeltaW Bootstrapper Functions}
##Check whether each student withdrew from their course in Gen_Data
Gen_Data10 <- Gen_Data9 %>%
                mutate(Withdraw = if_else(is.na(`GPA Assigned`), TRUE, FALSE))

##Determine overall withdraw rate for usage in experiment one
Overall.Withdraw.Rate <- Gen_Data10 %>%
                          filter(is.na(`GPA Assigned`)) %>%
                            mutate(Withdraw = TRUE) %>%
                              pull(Withdraw) %>%
                                sum()/length(Gen_Data9$`GPA Assigned`)

##Determine each Course's Withdraw Rate for usage in get.observed.prop() for experiment two
Course.Withdraw.Rates <- Gen_Data10 %>%
                          ungroup() %>%
                            group_by(`Course Code`) %>%
                              mutate(Course.Students = n()) %>%     
                                filter(Course.Students >= 100) %>%
                                  filter(is.na(`GPA Assigned`)) %>%
                                    mutate(Course.Withdraws = n()) %>%
                                      mutate(Course.Withdraw.Rate = Course.Withdraws/Course.Students) %>%
                                        slice(1) %>%
                                          dplyr::select(`Course Code`, Course.Withdraw.Rate)

##Make Bootstrapper functions to evaluate significance of each faculties' observed withdraw proportion

##This grabs the overall course withdraw rate from Course.Withdraw.Rates
get.observed.prop <- function(course.code = NA){
  if(is.na(course.code)){
    observed.prop <- Overall.Withdraw.Rate
  } else{
      observed.prop <- Course.Withdraw.Rates %>%
                        filter(`Course Code` == course.code) %>%
                          pull(Course.Withdraw.Rate)
  }
  return(observed.prop)
}
##This makes a bootstrap distribution using those students that took a course with a faculty member.
make.withdraw.bootstrap.props <- function(prof.num, course.code = NA){
  if(is.na(course.code)){
    og.sample <- Gen_Data10$Withdraw[Gen_Data10$`Faculty Random ID` == prof.num]
  } else{
    og.sample <- Gen_Data10$Withdraw[Gen_Data10$`Faculty Random ID` == prof.num & Gen_Data10$`Course Code` == course.code]
  }
  samp.size <- length(og.sample)
  boot.props <- rep(0,10^4)
  for(i in seq_along(boot.means)){
    boot.sample <- sample(og.sample, replace = TRUE, size = samp.size)
    boot.props[i] <- mean(boot.sample)
  }
  boot.props <- tibble(Bootstrap.Withdraw.Prop = boot.props)
  return(boot.props)
}
##This determines whether the faculty's withdraw rate is significantly different from the course rate.
evaluate.withdraw.significance <- function(prof.num, course.code = NA){
  current.significance <- NA
  if(is.na(course.code)){
  current.bootstrap.distribution <- make.withdraw.bootstrap.props(prof.num) %>% 
                                      pull(Bootstrap.Withdraw.Prop)
  } else{
    current.bootstrap.distribution <- make.withdraw.bootstrap.props(prof.num, course.code) %>% 
                                        pull(Bootstrap.Withdraw.Prop)
  }

  lower.quantile <- quantile(current.bootstrap.distribution, 0.025)
  upper.quantile <- quantile(current.bootstrap.distribution, 0.975)
  benchmark <- get.observed.prop(course.code)
  
    if(upper.quantile <= benchmark){
      current.significance <- TRUE
    } else if(lower.quantile >= benchmark){
      current.significance <- TRUE
    } else{
      current.significance <- FALSE
    }
  return(current.significance)

}
```

```{r Create and Test DeltaW Data Sets}
##Initialize Experiment One Data Set
Overall.DeltaW.Data <- Gen_Data10 %>%
                        semi_join(Course.Withdraw.Rates, by = "Course Code") %>%
                          ungroup() %>%
                            group_by(`Faculty Random ID`) %>%
                              summarise(Observed.Withdraw.Prop = sum(Withdraw)/n(), Num.Students = n()) %>%
                                mutate(Overall.DeltaW = Overall.Withdraw.Rate - Observed.Withdraw.Prop, Significant = as.logical(NA))

##Test Significance for Experiment One
for(k in seq_along(Overall.DeltaW.Data$`Faculty Random ID`)){
  current.prof <- Overall.DeltaW.Data$`Faculty Random ID`[k]
  Overall.DeltaW.Data$Significant[k] <- evaluate.withdraw.significance(current.prof)
}

##Initialize Experiment Two Data set
Course.DeltaW.Data <- Gen_Data9 %>%
                        ungroup() %>%
                          group_by(`Faculty Random ID`, `Course Code`) %>%
                            summarise(Observed.Withdraw.Rate = sum(is.na(`GPA Assigned`)/n()), Num.Students = n()) %>%
                              left_join(Course.Withdraw.Rates, by = "Course Code") %>%
                                mutate(Course.DeltaW = Course.Withdraw.Rate - Observed.Withdraw.Rate) %>%
                                  filter(Num.Students >= 8) %>%
                                    mutate(Significant = as.logical(NA)) %>%
                                      semi_join(Course.Withdraw.Rates, by = "Course Code")

##We now apply evaluate.significance across Course DeltaW Data to obtain significance results for each combination of course and professor with at least 8 students. This is experiment two.
n.evaluations <- length(Course.DeltaW.Data$`Faculty Random ID`) 
for(k in 1:n.evaluations){
  current.prof <- Course.DeltaW.Data$`Faculty Random ID`[k]
  current.course <- Course.DeltaW.Data$`Course Code`[k]
  Course.DeltaW.Data$Significant[k] <- evaluate.withdraw.significance(current.prof, current.course)
}
##Print out the first 10 lines of Overall.Withdraw.Data and Course.Withdraw.Data
head(Overall.DeltaW.Data)
head(Course.DeltaW.Data)
```

The courses Quantitative Reasoning was taught 5 times total between 3 faculty members, and Plane Geometry was taught twice by two faculty members - once each.. These were likely trial courses that did not prove popular/useful enough to merit becoming a mainstay at OCC. Because the courses were only taught by a few faculty members, it becomes meaningless to compare the effect the professor had on withdraw rate by comparing to the course average. I have therefore removed them from the following analysis.

Where $W_E$ is the expected withdraw rate and $W_O$ is a faculty's observed withdraw rate, $\Delta W$ is calulated as:
$$\Delta W = W_E - W_O$$
The expected withdraw rate in experiment one is the constant rate `r Overall.Withdraw.Rate`, calculated as the total number of withdraws in the data set divided by the total number of observations. That is, it is the mean withdraw rate between $2010$ and $2017$ in OCC's math and chemistry departments. In experiment two, $W_E$ is set to the mean withdraw rate for students in the germane course. As such, a positive value of $\Delta W$ corresponds to a professor having a lower withdraw rate than the mean, and a negative value corresponds to a higher withdraw rate than the mean.

```{r DeltaW Plots}
##Experiment One Graphs
##Histogram of DeltaW over all faculty for experiment one
Overall.DeltaW.Data %>%
  ggplot(aes(x = Overall.DeltaW)) +
  geom_histogram(binwidth = 0.05) + 
  labs(x = "DeltaW", y = "Count", title = "Histogram of Differences between Expected and Observed Faculty Withdraw Rate")

##Normal probability plot of DeltaW over all faculty for experiment one
Overall.DeltaW.Data %>%
  pull(Overall.DeltaW) %>%
    qqnorm(main = "Normal Probability Plot of DeltaW Over All Faculty for Experiment One")

##Histogram of DeltaW over significant faculty for experiment one
Overall.DeltaW.Data %>%
  filter(Significant == TRUE) %>%
    ggplot(aes(x = Overall.DeltaW)) +
    geom_histogram(binwidth = 0.05) + 
    labs(x = "DeltaW", y = "Count", title = "Histogram of Differences Between Expected and Observed Faculty Withdraw Rate")

##Normal probability plot of DeltaW over significant faculty for experiment one
Overall.DeltaW.Data %>%
  filter(Significant == TRUE) %>%
    pull(Overall.DeltaW) %>%
      qqnorm(main = "Normal Probability Plot of DeltaW Over Significant for Experiment One")

##Experiment Two graphs
##Histogram of course withdraw rates. There seems to be a slight left skew, but this is likely due to variance as there only being 21 remaining courses. Citation https://stackoverflow.com/questions/35090883/remove-all-of-x-axis-labels-in-ggplot for prettying up the x axis.
Course.Withdraw.Rates %>%
  mutate(`Course Code` = factor(`Course Code`)) %>%
    ggplot(aes(x = fct_reorder(`Course Code`, -Course.Withdraw.Rate), y = Course.Withdraw.Rate, fill = `Course Code`)) +
    geom_bar(stat = "identity") +
    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
    labs(x = "Course Code", y = "Withdraw Rate", title = "Bar Chart of All Course Withdraw Rates")

##Histogram of DeltaW over all faculty for experiment two  
Course.DeltaW.Data %>%
  ggplot(aes(x = Course.DeltaW)) +
  geom_histogram(binwidth = 0.05) + 
  labs(x = "DeltaW", y = "Count", title = "Histogram of Differences between Expected and Observed Faculty Withdraw Rate")

##Normal Probability Plot for experiment two
Course.DeltaW.Data %>%
  pull(Course.DeltaW) %>%
    qqnorm(main = "Normal Probability Plot of DeltaW for Experiment Two")

##Scatterplot of DeltaW by course
Course.DeltaW.Data %>%
  left_join(Course.Heights, by = "Course Code") %>%
    ggplot(aes(x = Course.DeltaW, color = `Course Code`, y = Height)) +
    geom_point(position = position_jitter(height = 0.01))
```


```{r}
# Create models and plots of withdraws based on a couple different inputs.
Course.Code.withdraws.model <- lmer(n_withdraws ~ total_students + (1|`Course Code`), data = Withdraws.Data)
summary(Course.Code.withdraws.model)



##Visualization of effect of `Course Code` on n_withdraws using 10^3 bootstrap resamples
plotREsim(REsim(withdraws.model, n.sims = 10^3))

##plot(withdraws.model)


##Do I need the confint? Is that even useful?
withdraw.conf <- confint(withdraws.model, oldNames = FALSE, level = 0.95)


Plot.Data1 <- Withdraws.Data %>%
                group_by(`Faculty Random ID`) %>%
                  summarise(total_withdraws = sum(n_withdraws), total_students = sum(total_students)) %>%
                    arrange(`Faculty Random ID`)
##I'll go ahead and assign total withdraw points to profs right now before proceeding with graphing



Plot.Data1 %>%
  ggplot(aes(x = total_withdraws, y = total_students)) +
  geom_point(aes(color = factor(`Faculty Random ID`))) + 
  geom_smooth(color = "purple") +
  geom_smooth(method = "lm", color = "red") +
  theme(legend.position = "none") +
  labs(x = "Total withdraws", y = "Total students", title = "Total withdraws vs. total students, ")  
  
 ## geom_smooth(method = "lm", color = "red") +

##The standard errors of geom_smooth() and geom_smooth(method = "lm) overlap at almost all points, so it seems reasonable to assume that total withdraws vs total students can be validly represented by a linear model. I will do that presently.


```

```{r}
##total withdraws prop, number of students per prof
Points.Tracker <- arrange(Points.Tracker, Professors)
for(i in 1:length(Points.Tracker$Professors)){
  Points.Tracker$Total.Withdraw.Prop[i] <- Plot.Data1$total_withdraws[i]/Plot.Data1$total_students[i]
  Points.Tracker$Total.Students[i] <- Plot.Data1$total_students[i]
}
```


5) What is the mean number of different courses a prof teaches? What is the mean number of majors a prof teaches? Other interesting statistics?

How do individual professors vary in their ability teach various courses?

```{r}
Gen_Data <- Gen_Data %>%
              group_by(`Course Code`) %>%
                summarise(Course.Mean.GPA = mean(`GPA Assigned`, na.rm = TRUE)) %>%
                  right_join(Gen_Data, by = "Course Code")                    
```





```{r Course Lookup Table}
## This is a course lookup table
Course.Credits
```







