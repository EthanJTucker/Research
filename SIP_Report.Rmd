---
title: "Statistical Data Analysis of Math and Chemistry Community College Students"
author: "Ethan Tucker"
date: "4/7/2021"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: ["float"]
header-includes:
- \usepackage{setspace}
- \doublespacing
geometry: margin=1.5in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=77), tidy=TRUE, fig.pos = "!T",  fig.height = 4, fig.width = 8)
options(digits = 3)
```

```{r Initialize, include = FALSE}
library(readxl)
library(tidyverse)
library(knitr)
# library(merTools)
library(lme4)
Gen_Data <- read_excel("Stats Research XLS.xlsx")
set.seed(1000)


Gen_Data <- Gen_Data %>%
                  filter(`Student Random ID` != 649419)
```

DATA SET ISSUES

1) GPA's not assigned consistently across all courses / all departments at OCC for the same letter grade. As such we need to build a grade converter that normalizes grades to GPA's. This requires some coin flipping.

2)

\section{Attribution}

\section{Abstract}

\section{Table of Contents}


\section{Introduction}
        
|  I started working as a math tutor for a local company called Thompson Tutoring back in the fall of 2019. At first it was just a way to pay rent and keep myself fed, but I came to enjoy it and care about the success of my students. I have about five students that have stuck with me since the beginning, each of whom I see once a week for an hour. Oftentimes I find myself wondering how much of an effect I really have on their mathematics education. As a tutor I am generally employed by a family in two settings:

1. A student is struggling in their current math course. In this scenario my role is to be the remedial instructor, trying to fill in the gaps in my student's knowledge while simultaneously preparing them for the next test. 
2. A wealthy family is looking to get their students ahead in mathematics.
        
|  It is very difficult to measure the effect I have had on my students for a variety of reasons, primary among them small sample size and no access to quantitative data. For my SIP I still wanted to combine my newfound addiction to data analysis with education, so Dr. Nordmoe sent me the link to a datset that Professor Andrew Eckstrom of Oakland Community College (hereafter OCC) had posted to a messageboard. The source data contain `r nrow(Gen_Data)` observations in `r ncol(Gen_Data)` variables - "Student Random ID", "Course Code", "Grade", "Faculty Random ID", and "Semester". Each row corresponds to one course taken by one student in one semester between the fall of $2010$ and the winter of $2017$ in the mathematics and chemistry departments of Oakland Community College. "Student Random ID" and "Faculty Random ID" are as expected randomly generated integers than track a given student or faculty member through the file. In total, the file tracks $66164$ students and $268$ faculty. My SIP is in half a statistical analysis on new variables I create from Dr. Eckstrom's dataset, and in half learning the requisite technology to perform such an analysis.

|  To explain the goal of my project, we will need to briefly diverge into a personal tangent. I came into Kalamazoo College thinking I would follow the pre-med track and thus need to major in chemistry or biology. Four and a half years later, I am graduating with two majors, among them neither chemistry nor biology. I only took one course each in those department because in one course I found the professor unenthusiastic, and in the other borderline hostile. In the math and physics departments I found faculty that were both more personable and frankly better at teaching their subject. My majors in math and physics developed as I continued to take courses with these faculty. The central question I pose in my SIP is this: how does the first faculty member a student encounters in a department influence a student's outcome? 
        
|  The data tracks students through time, so in conjunction with grading policies and course credits obtainable through OCC's website we can track each student's cumulative GPA through their stay in the math and chemistry departments. We will build linear mixed-effects models stratified to avoid some confounding variables to determine an expected final cumulative GPA for each student that passes a few conditions, then determine the difference between observed and expected final cumulative GPA. I call this statistic $\Delta C$, short for difference in cumulative GPA. Where $C_O$ is a student's observed final cumulative GPA and $C_E$ is the final cumulative GPA predicted by the linear mixed-effects model:
  
\begin{equation}
\Delta C = C_{O} - C_{E}
\label{DeltaC Definition}
\end{equation}

|     We will then take the mean $\Delta C$ among all students that started with a certain faculty, which can be thought of as the average benefit to final cumulative GPA a student that started with the professor had with respect to the mean student. It is possible to assign each faculty “points” for only the change in cumulative GPA that comes after, but the weights are unintuitive due to future courses having less effect on change in cumulative GPA as opposed to the extant value (beyond the second course). After running significance tests we will create faculty rankings based on their (1) the overall mean $\Delta C$, and (2) mean $\Delta C$ per course they taught. There will be pretty pictures along the way.
        
|     One of the conditions required to create the $\Delta C$ statistic is that students take multiple terms in the math and/or chemistry departments. This happens to be a pretty large requirement, especially for a community college such as OCC. I have pulled public data from DataUSA regarding OCC's enrollment statistics for all years available, which ended up being the years $2012$ through $2019$. The following tables and graph summarise this enrollment data.
        
```{r Load OCC Enrollment, include = FALSE}
##Load in OCC_Enrollment from 2012 to 2017. I am missing years 2010 - 2012. Data from https://datausa.io/profile/university/oakland-community-college
OCC_Enrollment <- read_csv("OCC_Enrollment.csv")
```

```{r Create Enrollment Tables and Graph, echo = FALSE}
##Create Enrollment table, obtained by manual data entry from 
OCC_Enrollment_Table <- OCC_Enrollment %>%
                          mutate(Total = `Full Time` + `Part Time`)

kable(OCC_Enrollment_Table, caption = "OCC Fall Enrollment by Year, 2012-2019", padding = 3)

##Create mean enrollment statistics
Mean.OCC_Enrollment <- OCC_Enrollment %>%
                        mutate(Total = `Full Time` + `Part Time`) %>%
                          summarise(`Mean Full Time` = mean(`Full Time`), `Mean Part Time` = mean(`Part Time`), `Mean Total` = mean(Total))

kable(Mean.OCC_Enrollment, caption = "OCC Mean Enrollment Statistics", padding = 3)

##Legend creation method from https://community.rstudio.com/t/adding-manual-legend-to-ggplot2/41651
colors <- c("Full Time" = "blue", "Part Time" = "red", "Total" = "green")

OCC_Enrollment_Plot <- OCC_Enrollment %>%
                        mutate(Total = `Full Time` + `Part Time`) %>%
                            ggplot(aes(x = Year)) +
                            geom_line(aes(y = `Full Time`, color = "Full Time")) +
                            geom_line(aes(y = `Part Time`, color = "Part Time")) +
                            geom_line(aes(y = Total, color = "Total")) +
                            labs(x = "Year", y = "Enrollment (Number of Students)", color = "Legend") +
                            scale_y_continuous(limits = c(0,30000), breaks = seq(0,30000, 5000)) +
                            scale_color_manual(values = colors)
```

```{r OCC Enrollments Plot, fig.cap = "OCC Enrollments By Year for Full Time, Part Time, and Total Student Body"}
OCC_Enrollment_Plot
```
    
|     Both full and part time enrollment monotonically declining over the reported years. Unfortunately, professor Eckstrom's dataset begins in the year 2010, so there are two relevant years we do not have data regarding enrollment. Regardless, between $2012$ and $2019$ on average only `r Mean.OCC_Enrollment[[1]]/Mean.OCC_Enrollment[[3]]*100`$\%$ of students were enrolled full time. According to the National Student Clearinghouse, community college dropout rate is a function of time enrolled - therefore part time students are less likely to take multiple courses in the same department due to a lower course load per term. (https://www.studentclearinghouse.org/nscblog/research-center-snapshot-report-showcases-yearly-success-and-progress-rates-for-fall-2012-freshman-class/) As we will see, requiring students to have two *graded* terms drastically reduces the sample size, but still allows for many faculty to be evaluated by the $\Delta C$ metric. 
  
|     Another difficulty arises in light of a second filtration condition - students must not withdraw from the first course they take in the math or chemistry departments. It would be possible to start the models at the first graded term, but if a student withdraws from a course then takes it again, their grade may be artificially inflated and thus not serve as an honest intercept. As a result, the expectation on $\Delta C$ becomes negative rather than zero. Requiring students to receive a standard letter grade ensures that the linear mixed effects models have a consistent starting point, and that individual teachers are evaluated fairly. What if a certain faculty member had a high withdraw rate due to their course being hard, but prepared their students well for future courses and thus also had a high mean $\Delta C$? Simply put, is there a correlation between a faculty's mean $\Delta C$ and their withdraw rate? $\Delta C$ is blind to any effect outside the realm of cumulative GPA, so we must introduce a new statistic to capture variance in withdraw rates. Fortunately, the calculation of withdraw rate for individual faculty does not require filtration so the sample sizes will be much larger and thus the results much more accurate. As with the $\Delta C$ rankings, we will construct rankings using the difference between the overall withdraw rate and a faculty's observed rate for: (1) the overall case, and (2) by course taught. To be clear, where $W_O$ is a faculty's mean withdraw rate and $W_E$ is the germane expected rate, this statistic will be calculated as:

\begin{equation}
\Delta W = W_E - W_O
\label{DeltaW Definition}
\end{equation}

|     Note that we have reversed the direction of difference with respect to $\Delta C$. We have chosen orientations such that positive $\Delta$ values correspond to "good" effects for ease of human interpretation. Additionally, note that while an observation on $\Delta C$ corresponds to one student, an observation on $\Delta W$ is specific to a faculty member. To reiterate, we will average over the $\Delta C$ values among those students that started with a professor to obtain a value unique to that faculty.
        
        b.	The Importance of Data-Driven decision making
        
|     My years of instruction in physics courses has drilled into my mind that any conclusions we make about the world around us need to be supported by data. Theory is nice, but even if the math is pretty and complete there is no guarantee the assumptions we made to create the model were correct. I will therefore be conservative in speculation and only make conjecture regarding trends demonstrably present in the data. This is in accordance to the dogma of facts-based decision making. In the context of corporate decision making, David Fradin says "The reasons why facts drive better decisions include – the ability of the computer to find non-obvious answers, ability to crunch large datasets very fast, can take into account hundreds of market variables and ability to adopt a methodical repeatable process for analysis." (“Organizing and Managing Insanely Great Products” by David Fradin with RN Prasad) Reproducibility and objectivity are key to any good data analysis, and are necessary when seeking optimal answers to any scientific problems. (https://plato.stanford.edu/entries/scientific-reproducibility/) 
  
|     Any empirically derived model will have error. Statistical methods such as hypothesis testing employ significance tests to determine whether a measured effect is "real". Such tests remove the human element by determining the likelihood a measured result could be generated by random chance under the assumption that the effect is "fake". (Laura M. Chihara and Tim C. Hesterberg, Mathematical Statistics with Resampling and R).
  
|     Even if the data collected are accurate and complete, confounding variables may still be present in any models created based on choice of groups and variables. According to Glenn Hymel, "If an extraneous variable is not appropriately controlled, it may be unequally present in the comparison groups. As a result, the variable becomes a confounding variable." (Research Essentials for Massage in the Healthcare Setting, Glenn M. Hymel, in Clinical Massage in the Healthcare Setting, 2008) 
        
                i.	Importance of Data Science
|     As of 4/6/2021, Glassdoor ranks Data Scientist as the second best job in America, with a median starting salary of $\$113,736$. (https://www.glassdoor.com/List/Best-Jobs-in-America-LST_KQ0,20.htm) Data Scientists use techniques from mathematics, statistics, and computer science to create and use tools for the purpose of collecting data and analysing potentially hidden trends stored within. (Grolemund and Wickham, 2017) The world around us is being deluged with a treasure trove of data, waiting to be analysed. In machine learning, programmers feed trial data to their AI, which then creates a model. This model is then used to predict the distribution of future data. According to 
                        
        c.	R For Data Science (Textbook)
        
|     When I began this project, I had no idea what I was doing. For the first month or so I was using some retrospectively atrocious C++ inspired hard-coding methods, one remnant of which I have included commented out in the code that follows this paper. When I showed Dr. Nordmoe one copious script I had created to determine the total number of students/faculty in the data, he told me to install the tidyverse and call the unique() function. In approximately twenty seconds the process I had spent several hours on had been recreated. Moreover, the runtime was spectacularly improved. I was then told to read chapter five of \textit{R For Data Science} by Hadley Wickham and Garrett Grolemund. Chapter five is a guide to the R package dplyr, one of the many packages from the tidyverse I used in the construction of this project. As it happens, I didn't need to reinvent the wheel.  A lot of smart people have built a lot of nice functions to make R a phenomenal work environment for data analysis. \textit{R For Data Science} was immensely helpful to my project, and was my primary guide to working with tidyverse functions beyone Dr. Nordmoe's data science course. The full book can be found online for free at https://r4ds.had.co.nz/. I would like to relay the author's wishes for support of the Kakapo parrot, a critically endangered New Zealand native, at https://www.doc.govt.nz/kakapo-donate.
        
        d.	Data Science Course
        
|     In order to help learn skills for this project, I enrolled in MAT-295 (Introduction to Data Science) at K. The course used the same free textbook that I read for this project, and so is an excellent offering for all students regardless of means. MAT-295 covered tidying and presenting data, text manipulation by means of regular expressions, manipulation of categorical variables, basic date/time conversion methods, creation of simple linear models, and a brief introduction to programming in R among other subjects. Course material does not include any mathematical theory such as probability or mathematical statistics, though the math department offers courses in those subjects in the Winter/Spring each year.  Introduction to Data Science does not require probability or math stats for enrollment, but statistics in particular is a powerful weapon in data analysis that we will be making heavy use of in this paper. MAT-295 was enormously helpful in giving me practice manipulating and presenting data using the tidyverse approach, which comprises at least half of the following project.
        
        
\subsection{Statistcal Background}  

\subsection{Probability Theory}
|     Probability theory is the backbone of any statistical method. In particular, the central limit theorem is an amazing piece of technology that guarantees when sufficiently many nicely behaved i.i.d. random variables are added, their sum will have a Gaussian distribution. (https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html) Where the i.i.d. random variables $X_i$ with population mean $\mu$ and standard deviation $\sigma$:

\begin{equation}
\lim_{k \to \infty} \sum_{i=1}^k X_i ~  N(k\mu, \frac{\sigma^2}{k})
\label{Large N CLT}
\end{equation}
        
(CITE PROBABILITY BOOK)        
        
|     Clearly a perfectly normal distribution would require the sum of infinitely many random variables. In reality it is quite tedious to attempt to collect an infinite number of values to add together, so it is common practice to set a lower bound of observations before data is considered approximately normal. This bound is often considered to be between $30$ and $40$ depending on the author, but this value can decrease arbitrarily if the random variables themselves have normal distribution. (https://www.investopedia.com/terms/c/central_limit_theorem.asp#:~:text=Sample%20sizes%20equal%20to%20or,characteristics%20of%20a%20population%20accurately., https://guides.fscj.edu/Statistics/centrallimit) Unfortunately, final cumulative GPA is inherently discrete, and withdraw rate is binomial. As such we must use an approximation to the central limit theorem. In my analysis, I use the more conservative minimum of 40 cases per stratum. Where the random variables $X_1$ through $X_n$ are i.i.d. each with sample mean $\bar{x}$ and standard error $s = \frac{\sigma_x}{\sqrt{n}}$,

\begin{equation}
X = \sum_{i=1}^n X_i \sim N(n\bar{x}, s^2)
\label{Small N CLT}
\end{equation}
(CITE MATHSTATS BOOK)

|  Like the Gaussian distribution, Student's t distribution is heavily used in significance testing. For a sample of $N$ i.i.d. random variables with sample mean $\bar{x}$, a-priori population mean $\mu$, and sample standard deviation $\sigma_x$, the t statistic is defined to be:

\begin{equation}
t = \frac{\bar{x} - \mu}{\sigma_x / \sqrt{N}}
\label{T  Statistic}
\end{equation}
  
  
|     The t distribution is designed to enable the analysis of small sample size, in accordance to a few conditions. According to CITE STATISTICS TEXTBOOK, these are:
  
1. Data must be collected from random variables that come from a continuous scale. This assumption is relaxed as the number of possible values increases. As I will show later, both $\Delta C$ and $\Delta W$ have sufficient possible values per case to pass this condition.  
2. Data must be collected by way of a simple random sample. From what I can tell, Professor Eckstrom's file contains a census, and so is immune to this requirement.
3. Random variables must resemble the normal random variable under combination. This assumption protects against pathological cases such as the Cauchy distribution that has undefined variance. The possible range for $\Delta C$ is given by the set $(-4,4)$ in units GPA points, and the possible range for $\Delta W$ is given by $(-1,1)$. The empirical mean $\Delta C$ distribution will ultimately have range $()$, as $\Delta W$ will have empirical range $()$ **[FILL THIS IN]**
4. The sample must have a reasonable sample size. The central t-statistic is undefined for $N = 1$, and gradually converges to the standard normal z-statistic as $N$ increases. The smaller the original sample size, the higher the requisite t-value for significance. The only requirement on sample size is that there are enough data points to calculate the sample standard deviation. For safety, in my analysis I will require a minimum sample size of 8 students for calculation of both $\Delta C$ and $\Delta W$. The reason for the number eight will be discussed after introducing the bootstrap.

\subsection{Significance Testing}
|     We will use the t distribution to perform significance tests at the $\alpha = 0.05$ level, where $\alpha$ is the probability of rejecting the null hypothesis when it is true. (CITE STATISTICS TEXTBOOK) $\alpha$ is also known as the type I error. (CITE STATISTICS TEXTBOOK) For any significance test, we first define the null and alternate hypotheses. Faculty effects on future cumulative GPA can be either positive, zero, or negative, as can their effect on withdraw rate. There is no information outside the data that lead us to believe an individual faculty member will have a positive or negative $\Delta W$ or mean $\Delta C$, so we will use a two sided hypothesis. Where $\theta$ is the true value of a statistic (i.e. the $\Delta W$ or mean $\Delta C$ which would be calculated after an infinite number of students for one faculty):

\begin{align*}
H_0: \theta = 0 \\
H_A: \theta \neq 0
\label{Hypotheses}
\end{align*}

|     Just as it would be impossible to collect infinite variables to ensure perfect normality, we cannot mathematically prove from a sample which provides an estimate $\hat{\theta}$ that the null hypothesis is false. However, we can create a range of values where $\theta$ is expected to lie based on the observation of $\hat{\theta}$. This concept is called a confidence interval. In my analysis, I will construct such confidence intervals using two techniques from mathematical statistics.

|     As the name suggests, the one sample t-test utilizes the t distribution. Where $\alpha$ is the probability of type I error and $t_{\alpha/2, N-1}$ is the t statistic's $\frac{\alpha}{2}$ quantile for N degrees of freedom, the derivation of the $100(1 - \alpha)\%$ confidence interval follows: ##CITATION http://www.ams.sunysb.edu/~zhu/ams571/Lecture11_571.pdf

\begin{align*}
P(-t_{\alpha/2, N-1} \leq t \leq t_{\alpha/2, N-1}) = 1 - \alpha \\
P(-t_{\alpha/2, N-1} \leq \frac{\bar{x} - \mu}{\sigma_x / \sqrt{N}} \leq \leq t_{\alpha/2, N-1}) = 1 - \alpha \\
P(\bar{x} - t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}} \leq \mu \leq \bar{x} + t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}) = 1 - \alpha
\end{align*}

\begin{equation}
100(1-\alpha)\% \quad CI = [\bar{x} - t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}, \bar{x} + t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}]
\label{T Confidence Interval}
\end{equation}

Using this $95\%$ confidence interval, our hypothesis tests on $\theta$ using the t-statistic are:

\begin{align}
H_0: \theta \in [\bar{x} - t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}, \bar{x} + t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}]\\
H_A: \theta \notin [\bar{x} - t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}, \bar{x} + t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}]
\label{T Hypotheses}
\end{align}

|     The bootstrap principle is a powerful tool that can allow fo significance testing with fewer assumptions than the one sample t-test. The general process for the bootstrapping of a population parameter $\theta$ using a given sample containing an estimate $\hat{\theta}$ follows:

1. Create an empty vector $V_{boot}$ with length equal to a large number $n_{sim}$
2. Create a sample with replacement $S_{boot}$ from the original sample whose length is equal to the length of the original sample. In this example this length is N.
3. Store the observed mean of $\hat{\theta}$ from (2) into $V_{boot}$
4. Repeat steps (2) through (3) $n_{sim}-1$ times.

|     The resulting vector $V_{boot}$ contains the bootstrap sampling distribution, which is an estimate for the sampling distribution. Due to $\ref{Small N CLT}$ As such, we can use the bootstrap distribution to create a confidence interval for $\theta$. R's built-in quantile() function is used to create $95\%$ confidence intervals for $\hat{\theta}$ from this bootstrap sampling distribution. The $100(1-\alpha)\%$ bootstrap CI is given by the set:
\begin{align}
100(1-\alpha)\%CI = [\text{quantile}(V_{boot}, \frac{\alpha}{2}),\quad \text{quantile}(V_{boot}, 1- \frac{\alpha}{2})]. 
\label{Bootstrap Confidence Interval}
\end{align}

The corresponding hypothesis tests on $\theta$ are given by:

\begin{align}
H_0: \theta \in [\text{quantile}(V_{boot}, \frac{\alpha}{2}),\quad \text{quantile}(V_{boot}, 1-\frac{\alpha}{2})]\\
H_A: \theta \notin [\text{quantile}(V_{boot}, \frac{\alpha}{2}),\quad \text{quantile}(V_{boot}, 1-\frac{\alpha}{2})]
\label{Bootstrap Hypotheses}
\end{align}


|     In my analysis, the $\Delta C$ statistic is calculable for all students that take only one course in the first term, do not withdraw from that course, and receive at least one more grade. It is possible to group students by the course code of their first course, then immediately create a bootstrap sampling distribution for $\Delta C$ using only those students. This is pointless; recall we define each stratified linear models such that the mean $\Delta C$ over all students that take the same first course code is precisely zero. Furthermore, the average $\Delta C$ for a course code is not what we wish to test. Our goal in creating the $\Delta C$ statistic is to determine if individual faculty have a statistically significant effect on their student's final cumulative GPA. We will seperate this investigation into two experiments. Experiment one will be to test for significance the mean $\Delta C$ among all students that passed filtration and started with a given instructor. Experiment two will restrict the students the $\Delta C$'s for averaging to only those that started in the same course code with the same professor. In summary, we wish to construct a $95\%$ range for the population (read: "true") mean $\Delta C$ among students that started with a professor, both overall and by individual course code. Faculty in OCC's math and chemistry departments vary in the number of students taught, which effects the coding methods necessary for establishment of bootstrap confidence intervals. Specifically, we must on a case-by-case basis adjust the number of observations per bootstrap sample to mirror the number observed. To reiterate, in experiment one the number of observations per bootstrap sample will be equal to the total number of students that started with the faculty. In experiment two we set the number of observations per bootstrap sample equal to the number of students that started with the faculty *in a particular course code*. There are two ways to accomplish this:

1. In experiment one, we can create a bootstrap sampling distribution by drawing samples *from the overall list of* $\Delta C$*'s* for all students. In experiment two the $\Delta C$'s available for sampling are restricted to those of students that started in the desired course code. If the number of simulations and the number of values are each sufficiently large, one can use R's quantile() method to determine the $95\%$ CI as outlined above. The observed mean for $\Delta C$ is then checked against the confidence interval. If the observed mean is within the confidence interval we fail to reject the null hypothesis that the faculty did not have a significantly different effect on a student's final cumulative GPA than the mean difference. If the observed mean is outside the boundary, we have sufficiently shown at the $\alpha = 0.05$ level that the faculty's mean effect was different than the overall mean effect. Moreover, for such significantly different faculty we have shown at the $\alpha = 0.025$ level that the mean effect was either positive or negative depending on the polarity of the observed mean. This method amounts to determining the probability of randomly generating a bootstrap sample with a mean at least as extreme as the observed mean.

2. To prevent needing to simulate the sampling distribution from the total sample for each sample size possessed by a faculty member, we can simply use the $\Delta C$'s attained by those students that passed filtration and started with the faculty member as the population from which bootstrap samples are created (in a particular course in the case of experiment two). The confidence interval generated by sampling from the $\Delta C$'s attained by a faculty's students is range for the true mean $\Delta C$ of that instructor. Therefore, if the stratum's mean $\Delta C$ is within the generated CI, the faculty's effect cannot be shown to be different than the mean. We then fail to reject the null hypothesis. On the other hand if the $95\%$ confidence interval does not include the overall mean, then we reject the null hypothesis in favor of the alternate. Again, for faculty with significantly different effects we show at the $\alpha = 0.025$ level that said effect is either positive or negative, depending on the polarity of the observed mean. This method has the added benefit of permitting individual faculty to originate from different population mean $\Delta C$ distributions than other faculty, say, if chemistry instructors are fundamentally different from math instructors. In my analysis I chose to use this method to slightly save on processing time, but to my knowledge both are equally valid. 

|     For a faculty's $\Delta W$ the bootstrapping process and experiments follow exactly as for their mean $\Delta C$. As a final note, the bootstrap confidence interval may not accurately represent the sampling distribution if there are not enough unique observations. This is a particular issue with a fundamentally discrete statistic like difference between observed and expected final cumulative GPA. On the topic of a minimum original sample size for using the bootstrap method, the author of two books on the bootstrap Dr. Michael R Chernick states that:  (https://stats.stackexchange.com/questions/33300/determining-sample-size-necessary-for-bootstrap-method-proposed-method)
 
 > Now if the sample size is very small---say 4---the bootstrap may not work just because the set of possible bootstrap samples is not rich enough. In my book or Peter Hall's book this issue of too small a sample size is discussed. But this number of distinct bootstrap samples gets large very quickly. So this is not an issue even for sample sizes as small as 8.
 
|     From this guideline I chose to discard all faculty effects that were measured from fewer than eight students. I also chose to use eight students as minimum for the t-tests to create a closer approximation to normality.

\section{R Programming}

|     As previously stated, when I first began this project I had never taken a programming course. A large chunk of the project boiled down to creation and manipulation of new variables through the tidyverse method, but there was still many pieces of knowledge I lacked to make the analysis run. Coursera offers many excellent online courses on a wide variety of subjects, among them computer science and data science. To learn the programming skills I required to analyse Professor Eckstrom's data, I enrolled in and completed the first two modules of the *Data Science: Foundations Using R Specialization* offered by Johns Hopkins University on the Coursera platform: (1) The Data Scientist's Toolbox, and (2) R Programming. My certificates and grades I received are available in the appendix along with synopses and work product from the two courses. This section is a summary of my learning from these courses and my previous exposure to R, intended as a introduction to all necessary concepts used in my program. For readers new to the R language, I recommend keeping the R Documentation open for reference when learning the syntax and usage of new functions.

CITATION FOR ENTIRE SECTION IS R Programming for Data Science by Roger D. Peng [version 9/3/2020]            
       
\subsection{The Atomic Data Types}

|     In the R language, there are five atomic data types, also called class invariants: 

1. Numeric data, often also referred to as double-precision data, are real numbers. They can have up to fifteen significant digits, but generally are limited by observation to many fewer. In RMarkdown it is common practice to include the option "digits = k", which restricts the printing of doubles to k decimal points. Numeric data have the additional option of having the value NAN (for not a number), which most often corresponds to undefined in mathematical language. For instance, $\log(-1)$ yields the output `r log(-1)`. 

2. Integer data can only be real integers, and are useful in programming aspects such as indexing and preventing rounding errors. Integer math is a common focus of number theory.

3. Character data, or strings, are the broadest class. Any set of alphanumeric+ characters can be represented in a *string* as a character object. Packages such as stringr are available to manipulate character variables with plentiful functions equipped for regular expressions.  

4. One of if not the most important data type for programming in R is the logical, or Boolean. A logical datum either has the value `r TRUE` or `r FALSE`. When two logicals $b_1$ and $b_2$ are compared over equality - i.e. $b_1 = b_2$ - R will output `r TRUE` if both data have the same value. We will momentarily investigate the behavior of logical data. As will be apparent when discussing subsetting, the ability to programmatically compare large quantities of logical data allows for fast and convenient indexing, sorting, and filtering.

5.The final class in the R language is complex, an element of which has the form $a + bi$, where $a$ and $b$ are numeric data. I personally never used complex data beyond the single Coursera quiz that asked for the identification of a complex datum among a list of options.

|     One last important note - data can be missing in any class. A missing value is denoted with NA (for not available). The NA can therefore be represented in each class, and converted using the as.xxx() family of functions. R treats NA values as if they did have a value, but that the value is not known. For instance - the mean of the numeric vector c(.5, 2, NA) is `r mean(c(.5, 2, NA))` with class `r class(mean(c(.5, 2, NA)))`, because R cannot determine the third element's effect on the average but assumes it was a number due to how vectors are defined.

\subsection{Assignment}  

|     The cornerstone of any function-oriented programming project is the ability to save a created object to memory. A user can accomplish this by using one of R's assignment methods. There are five different assignment operators: (R programming textbook, https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/assignOps)

1. The equals operator "=" which cannot be used within a function call,

2. The left assignment operator "<-" and the right assignment operator "->" which *can* be used within a function call, and

3. The left and right super-assignment operators "<<-" and "->>", which store the created object to the parent frame. Super-assignment is most commonly found in functions where a useful object is created, such as in the process of caching to prevent the duplication of effort.

\subsection{Vectors}

|     On that note, let's discuss one of the primary weapons of the R language - vectors. R can store any sequence of data from a single class as a vector and do operations on it. Such operations are performed element-wise unless else specified. The vector's class will be the same as that of its constituent data. There are several ways to create a vector including the concatenation operator c(), the sequence generator seq(), the replicate function rep(), the colon operaton :,  and extracting a column out of a dataframe (a rectangular matrix with column names) either by using pull() or by subsetting.  Individual elements can have names, for instance:
  
```{r Demo Named Vector, echo = TRUE}
vector.names <- c("Jimmy", "Sammy", "Katrina")
my.vector <- c(17, 18, 17)
names(my.vector) <- vector.names

my.vector
```

|     Vectors can sometimes be changed from one class to another through a process called coercion. The family of functions as.class() performs this conversion. Below is some sample output demonstrating the coercion capabilities of different classes using three demo vectors: num for numeric, chr for character, and log for logical:

```{r demo vectors, echo = TRUE}
##Define demo vectors. num is a numeric vector, chr is a character vector, and log is a logical vector.
num <- c(-1, 0, 1, 2)
chr <- c("a", "b", "0", "FALSE")
log <- c(TRUE, FALSE)

##Coercion of numeric vector to character
as.character(num)
##Coercion of numeric vector to logical
as.logical(num)
##Coercion of character vector to numeric
as.numeric(chr)
##Coercion of character vector to logical
as.logical(chr)
##Coercion of logical vector to numeric
as.numeric(log)
##Coercion of logical vector to character
as.character(log)
```

|     When coercing a numeric object into the character class, each element becomes represented as a string and loses its ability to partake in arithmetic operations. Conversion from double to logical is possible, but one must be careful - only zeroes get mapped to `r FALSE`, while all other numbers get mapped to `r TRUE` unlike in some other languages. Character vectors are the least coercible. While each unicode character is stored in memory as a very large integer, R doesn't want to return these for user safety. As a result, coercion of a vector from the character class to the numeric class will result in a missing value unless R thinks the string already represents a number. Strangely, `r TRUE` / `r FALSE` values stored in a character vector will be mapped to missing values in this coercion, even though they can be coerced into numerics. Calling as.logical() will similarly fail to notice $0$ as `r FALSE`.  If a vector's class is not clear, the class() method is an excellent tool, and can also be used programmatically to obtain the class(es) of an object. In my project I used the as.class() functions rather rarely, but they serve to elucidate the inner workings of the classes themselves. 



\subsection{Logic}

|     Before moving on to subsetting vectors and lists, we first take a foray into R's evaluation of logical statements. The most straightforward way to do this is to look at some examples.

```{r Logical Practice 1, echo = TRUE}
##Practice with logical data
TRUE == TRUE
TRUE != TRUE
FALSE < TRUE
FALSE > TRUE
```

|     The double equals is a binary operator that outputs `r TRUE` i.f.f. both sides of the operator have the same evaluation. The less than and greater than operators work in a similarly intuitive manner. $!$ is the negation operator, which inverts the meaning of the logical operator it is paired with. Additionally a logical datum itself can be negated, such that !`r TRUE` evaluates to `r !TRUE`. Note that the expression `r FALSE` $<$ `r TRUE` evaluates to `r TRUE` and `r FALSE` $>$ `r TRUE` evaluates to `r FALSE` because R considers `r FALSE` to be the number $0$ and `r TRUE` to be the number $1$. This feels less than intuitive because as seen above any nonzero number coerces to `r TRUE` under the method as.logical(). Just as $0 < 1$ gets a `r TRUE`, when character data are compared over the binary operators, R considers the lexicographic ordering. This is worth exploring, but not relevant to my project. More pertinent is the evaluation of vector comparisons. Let's do another example.

```{r Logical Practice 2, echo = TRUE, warning = FALSE}
##Define some vectors to compare with different lengths.
##Vector a contains the sequence 1, 2.
a <- c(1, 2)
##Vector b contains the sequence 1, 2, 3, 4, 0.
b <- c(1:4,0)

##Compare a and b over equality
a == b
##Compare a and b over a formula
a + 2 == b
##Use the which() function
which(b >= 3)
```

|     R evaluates vector comparisons element-wise and outputs a vector with length equal to the longest input vector. If the compared vectors are not the same length, R repeatedly replicates the shorter vector end-to-end until the lengths are equal. In the above example, R changed the expression $\{1,2\} == \{1,2,3,4,0\}$ into $\{1,2,1,2,1\} == \{1,2,3,4,0\}$ before evaluating. The shorter length of $2$ does not evenly divide the longer length $5$ so R throws a warning, but continues with evaluation regardless. The output of any logical comparison is a logical vector; the previous comparisons were simply a special case where the input vector lengths were each one. $a == b$ returns a five element output vector, because the longer of the two input vectors had five elements.
  
|     The final complexity to logical evaluation in R are conditional expressions. R is equipped with three basic binary conditions: the and statement "&", the or statement "|", and the xor function xor(). A more complicated binary operator I use for the data analysis is the in operator "%in%", which checks whether the left object is within a range of values specified by the right object. These conditions work just as they do in set theory. Or has low priority in R's evaluation queue while and has high priority so that conditions evaluate in accordance to the generally desired order of operations. Just like in mathematics parenthetical statements are evaluated before their exterior for additional control over the evaluation queue.   The which() function is the odd-code-out in this example, because it does not return a logical vector. Instead it takes a logical vector as *input* and returns the indecies of the `r TRUE` elements. This is particularly useful in subsetting.  One final note, R allows for the user to define their own logical operators, a privilege I used only one time in conjunction with the Negate() function to create a negated version of the "%in%" operator called "%notin%".  

\subsection{Subsetting}

|     CRAN outlines three operators for subsetting sequences in R: (https://cran.r-project.org/doc/manuals/R-lang.html#Indexing)

1. The single bracket operator $X[i]$, where $X$ is some array and $i$ is a set of indecies. Single bracket subsetting is generally used with vectors to extract the $i^{th}$ elements of X. When used on a list, the single bracket will return the $i^{th}$ elements as a list. The user can supply indecies either as an integer vector or a logical vector. To see how this works, inspect the sample output below.

```{r Single Bracket Practice, echo = TRUE}
##We will be using my.vector for subsetting practice. Here I will print it out once again as a reminder
my.vector

##Define some integer indecies to subset
int.index <- c(2,3,4)

##Define some logical indecies to subset
lgl.index1 <- c(TRUE, FALSE)

##More practical application of logical indexing
lgl.index2 <- names(my.vector) == "Jimmy" | names(my.vector) == "Katrina"

##Define some desired names for subsetting
names.index <- c("Jimmy", "Sam")

##Use the single bracket operator to subset using integer indecies
my.vector[int.index]

##Use the single bracket operator to subset using logical indecies
my.vector[lgl.index1]

##Practical example of subsetting with logic. This returns the same output as previous example, but has a more legible index.
my.vector[lgl.index2]

##Use the single bracket operator to subset by name
my.vector[names.index]
```
|     R uses $1$-based array indexing, so when fed integer indecies it will attempt to extract elements from the object based on the position considering the index $1$ to be first position.  When we told R to take $4^{th}$ element of my.vector it determined that there *should* be a $4^{th}$ element, but that there was insufficient data to determine its value. R returns `r NA` for the fourth index of my.vector. If R instead is given a logical vector to use as indecies, it decides it needs to do a logical comparison. R will first (if necessary) replicate the indecies until the length of the index vector has at least the same length as the object to be subset; my.vector has three elements but lgl.index only has two, so R will turn $c(T, F)$ into $c(T, F, T, F)$. The elements corresponding to `r TRUE` indecies are returned. The true power of this concept arises from the combination of R's robust logical framework and element-wise operations, as can be seen in the second logical example output.
  
|     The single bracket operator can also be used to subset by name. When the single bracket is fed a character vector, R will look for an element whose name is a literal match. One of the later methods allows for partial matching, but not the single bracket. If there is no element with a name matching a supplied argument R will return an `r NA` of the appropriate class as stand-in. Note that R does not name the `r NA` value with the assumed name from the index.
  
2. The double bracket operator $X[[i]]$. Just as the single bracket variant is generally used on vectors, the double bracket is most often used on lists. Before discussing the double bracket method, first we must first ask what is an R list? Lists are another common form of array that can be thought of as a hierarchical pyramid that can hold any type of object, including other lists. Just like vectors, list elements can have names. Lists are excellent organizational tools because they are not beholden to the stipulation that each element have the same class invariant; a two element list may for example contain one function and one dataframe. Because lists may contain a variety of information, a couple good ways to obtain a sense of the contents are the str() and  glimpse() commands. I am personally partial towards glimpse due to its good behavior with other dplyr verbs. 

|     The double bracket drops all formatting on elements - for a list this includes removal of the top level list specification as well as any names associated to the level. In the sample output below, my.list contains two branches. The first branch is another list of two elements, while the second is a numeric vector of length $1$. By applying [[1]] once, R extracts the branch title Element_One. The second application of [[1]] extracts the contents of the first object stored in Element_One, which happens to be a function that returns the mean of an input. By supplying a numeric vector to this subset R will output the mean of the numeric vector. In my project I use lists to store multiple stratified mixed-effects models, and subsetting those lists allows tidy storage of slopes and t-values.
  
```{r List Example, echo = TRUE}
##Create dummy functions for list
test.mean <- function(x){mean(x)}
test.median <- function(x){median(x)}
##Create list with multiple layers
my.list <- list(list(test.mean, test.median), 100)
##Name some elements of my.list
names(my.list) <- c("Element_One", "Element_Two")
names(my.list[[1]]) <- c("Mean", "Median")
##Print the list
my.list
##The subset my.list[[1]][[1]] returns a function that takes the mean of an input vector.
my.list[[1]][[1]](c(1,2,3))
```

3. The dollar sign operator $X\$i$ is the most straightforward subsetting operator. `$` is most often used to extract variables (columns) from a dataframe by name. It can do the same for any non-atomic array, so we will do an example with the object my.list. 

```{r}
my.list$Element_One

my.list$Element_One$Median(c(1,2,3))
```

|     Like the double bracket operator, the dollar sign drops formatting on the extracted object. As such, named functions stored within lists can be immediately called, and atomic data can be immediately operated on. While I never perform a double dollar sign extraction in my program, I do perform extractions from lists using a combination of subsetting methods.
  
\subsection{Control Structures}

|     Complicated functions can be built by chaining and/or nesting conditional execution statements. In R, conditional execution arises from the a standard if-else statement. The most widely used control structures available in R are the if-else chain, the for while and repeat loops, and the reserved words break and next.  According to the R documentation for these constructs, "They function in much the same way as control statements in any Algol-like language." (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/Control) For loops run over a predetermined set of indecies supplied to the loop, and are therefore a fundamentally finite process. In my project for loops are most widely implemented for program stability. For loops are also easiest to troubleshoot, because R records the loop index in a dummy variable. While loops are more dangerous because a set condition is checked before each cycle and there is no guarantee that such a condition will ever flag. That is not to say these are not useful - the ability to iterate an arbitrary number of times is very powerful. The last type of loop is the repeat loop, which loops continuously until an internal break is called. As with the while loop, a user must be cautious to avoid an infinite loop. The advantage of a repeat loop is manual control over when the break condition gets evaluated. To highlight the differences between while and repeat, let's look at an example:

```{r Demo While Loop, echo = TRUE}
x1 <- 0
while(x1 < 0){
  x1 <- x1 + 1
}
x1
```
|     While loops check the evaluation condition at the beginning of the loop. Because zero is not less than zero, the loop immediately flags and therefore the value of $x$ remains zero.

```{r Demo Repeat Loops 1, echo = TRUE}
x2 <- 0
repeat{
  if(x2 >= 0){
    break
  }
  x2 <- x2 + 1
}
x2
```
|     Repeat loops require their user to place the break condition manually. By placing the condition at the beginning of the loop, we have recreated a while loop. Note that the condition used is inverted with respect to the while loop for usage of the if() construct.

```{r Demo Repeat Loop 1, echo = TRUE}
x3 <- 0
repeat{
  x3 <- x3 + 1
  if(x3 >= 0){
    break
  }
}
x3
```

|     Even though the condition used in this loop is identical to the previous loops, the output is different. Placing the loop break at the end of a loop causes R to run one iteration before quitting. Often this difference can be overcome by a change of condition to a while loop, which is often preferable for legibility. Changing the condition $x < 0$ to $x < 1$ in the while loop makes it equivalent to the second repeat loop. One final note - the next statement ends the current iteration and advances to the next. One application lies in the handling of missing values. In the following example, the is.na() function returns `r TRUE` if the supplied argument is a missing value and `r FALSE` otherwise. The seq_along() method creates a set of integer indecies from $1$ to the length of the supplied vector. We now seek to build a loop that counts how many `r TRUE` statements exist in a row while ignoring missing values:

```{r Example loop, echo = TRUE}
##Create vector to iterate over
x4 <- c(TRUE, TRUE, NA, FALSE, TRUE)
cntr <- 0
for(i in seq_along(x4)){ ## seq_along(x4) generates the vector 1:5
  if(is.na(x4[i])){ ##If the ith element of x4 is missing, skip the iteration
    next
  }
  if(x4[i] == TRUE){ ##If the ith element of x4 is TRUE increment the counter by one
    cntr <- cntr + 1
  }
  else{ ##By the process of elimination the current element must be FALSE, so we end the loop.
    break
  }
}

cntr
```

\subsection{Functions}
In his 2008 book *Software for Data Analysis: Programming with R*, John Chambers said:
 > Nearly everything that happens in R results from a function call. Therefore, basic programming centers on creating and refining functions.
(Chambers, John M. (2008). Software for data analysis programming with R. Berlin: Springer. ISBN 978-0-387-75935-7.)

|     It often happens that a programmer wishes to change a couple parameters before reusing some existing code. Functions accomplishes this task efficiently. Such objects can take inputs, no inputs, or have a pre-set but changeable parameter. These arguments are called the "formals" of the function. (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/formals, https://www.rdocumentation.org/packages/Rllvm/versions/0.7-0/topics/Function). The "body" consists of the code executed upon a function call using the supplied formal arguments. Functions must exist either in the current working directory or a higher directory in order to be called under R's scoping rules.

|     Let's say we had 500 logical vectors of length five and wished to find the mean number of first `r TRUE`'s before the first `r FALSE` over all such vectors. The code in the above example loop serves this purpose quite well, but copy/pasting the entire block would be neither tidy nor runtime efficient. We will store the chunk as a function, then apply it across all vectors to find the mean.

```{r Example Function, echo = TRUE}
##We need to create 500 logical vectors of length 5. The rbinom() function is excellent for this task as it simulates binomial random variables. Documentation can be found at https://stat.ethz.ch/R-manual/R-patched/library/stats/html/Binomial.html . We will create these logical vectors one-by-one, and use a loop to iterate 500 times.

results <- rep(0, 500) ##Create a vector to store the number of TRUE's before the first FALSE for each vector of length 5.

##We define the function Trues.Before.False to count the number of TRUE's before FALSE in the supplied vector, while skipping any elements that are missing. In our randomly generated vectors no data will be missing, but real data might so it is a solid safety measure.
Trues.Before.False <- function(vector){
cntr <- 0
for(i in seq_along(vector)){ ## seq_along(vector) creates indecies to iterate over with length equal to the length of the vector
  if(is.na(vector[i])){ ##If the ith element of the vector is missing, skip the iteration
    next
  }
  if(vector[i] == TRUE){ ##If the ith element of the vector is TRUE increment the counter by one 
    cntr <- cntr + 1
  }
  else{ ##By the process of elimination the current element must be FALSE, so we end the loop.
    break
  }
}
return(cntr)
}

##Apply Trues.Before.False manually across 500 randomly generated vectors of length 5.
for(j in seq_along(results)){ ## We need to be careful to use a new index j because Trues.Before.False uses the index i. This doesn't actually affect the functionality of the program, but is good practice for legibility.
  current.vector <- as.logical(rbinom(n = 5, size = 1, prob = 0.8)) ##Create a logical vector of length 5 with TRUE probability 0.8.
  results[j] <- Trues.Before.False(current.vector) ##Store the number of TRUE's before the first FALSE to the jth element of results.
}

mean(results) #Output the mean number of TRUE's before the first FALSE.
```

|     Instead of Trues.Before.False onto each vector individually, we have the option to use one of R's built in apply functions. These take an input object, an input function, and sometimes additional specifications to achieve the same result in a more efficient manner. Apply functions can make the code mode legible as well, which helps for reproducibility. The method lapply() takes a list and a function as input then returns a list wherein one item is the output of one function application. vapply specifically takes a list or a vector as input, and allows the user to specify what type of object should be returned using the argument FUN.ARGS. sapply() automatically determines what output is best, preferentially choosing vectors and matrices if possible. In my analysis I will use sapply() a couple times, but more often use manual application for conciseness. As an example, we will use sapply() to recreate the mean calculation. We will also put the entire command within another function environment for ease of duplication with a modifiable simulation number set to a default of five hundred.

```{r sapply Example, echo = TRUE}
##We need to supply sapply with a list of logical vectors with length 5. We will use the same for() loop as before to accomplish this.

Make.Trues.Before.False.Mean <- function(n.sims = 500){

five.numbers <- rep(as.numeric(NA), 5) ##Create a dummy variable to ensure list class is numeric. We initialize this vector using the coercion method as.numeric() on missing values as a safety measure to ensure the initialization does not affect the outcome.
sapply.list <- rep(list(five.numbers), n.sims) ##Create a list with n branches to store individual simulations. The individual branches contain one numeric vector of length five. For this output we set n.sims = 500
for(j in seq_along(sapply.list)){ ## We need to be careful to use a new index j because Trues.Before.False uses the index i. This doesn't actually affect the functionality of the program, but is good practice for legibility.
  sapply.list[[j]] <- as.logical(rbinom(n = 5, size = 1, prob = 0.8)) ##Create a logical vector of length 5 with TRUE probability 0.8 and save it to the jth branch of sapply.list. 
}

return(mean(sapply(sapply.list, Trues.Before.False))) ##Use sapply to determine the number of TRUE's before the first FALSE for each simulation, then return the mean of all n.sims simulations.
}

Make.Trues.Before.False.Mean()
```

|     Even though the procedure is identical, the output is different. This is because the five hundred vectors of length five get resimulated, and the number of `r TRUE`'s before the first `r FALSE` among those five numbers has some variance in accordance with the standard deviation of the sampling distribution. Recall that this is the standard error $se = \frac{\sigma_x}{\sqrt{N}}$. The deviation between instances of Make.Trues.Before.False.Mean() will then decrease monotonically as the number of simulations increases ($N \in \mathbb{N}$). Two histograms using Make.Trues.Before.False.Mean() with different number of simulations are presented below for visualization of this idea as figures $2$ and $3$.

```{r Make Hist.Data1, include=FALSE}

Hist.Data1 <- data.frame(Sample.Means = rep(as.numeric(NA), 10^4)) ##Initialize a data frame with variable Sample.Means. I chose 10^4 samples to get a good approximation of the sampling distribution for n.sims = 60. 

for(k in seq_along(Hist.Data1$Sample.Means)){
  Hist.Data1$Sample.Means[k] <- Make.Trues.Before.False.Mean(60)
}  
```

```{r, fig.cap = "Sampling Distribution of the function Make.Trues.Before.False.Mean for Sixty Vectors of Length Five, with 95% Confidence Interval."}
Hist.Data1 %>%
  ggplot(aes(x = Sample.Means)) +
  geom_histogram() +
  scale_x_continuous(limits = c(1.5, 3.75), breaks = seq(from = 1.5, to = 3.75, by = 0.25)) +
  geom_vline(xintercept = c(quantile(Hist.Data1$Sample.Means, 0.025), quantile(Hist.Data1$Sample.Means, 0.975)), color = "blue") +
  labs(x = "Sample Mean", y = "Count")
```

```{r Make Hist.Data2, include=FALSE}

Hist.Data2 <- data.frame(Sample.Means = rep(as.numeric(NA), 10^4)) ##Initialize a data frame with variable Sample.Means. I chose 10^4 samples to get a good approximation of the sampling distribution for n.sims = 1000. 

for(k in seq_along(Hist.Data2$Sample.Means)){
  Hist.Data2$Sample.Means[k] <- Make.Trues.Before.False.Mean(1000)
}  
```

```{r, fig.cap = "Sampling Distribution of Make.Trues.Before.False.Mean for One Thousand Vectors of Length Five, with 95% Confidence Interval."}
Hist.Data2 %>%
  ggplot(aes(x = Sample.Means)) +
  geom_histogram() +
  scale_x_continuous(limits = c(1.5, 3.75), breaks = seq(from = 1.5, to = 3.75, by = 0.25)) +
  geom_vline(xintercept = c(quantile(Hist.Data2$Sample.Means, 0.025), quantile(Hist.Data2$Sample.Means, 0.975)), color = "blue") +
  labs(x = "Sample Mean", y = "Count")
```


\subsection{Scoping Rules}
|     The function Trues.Before.False() is not defined locally within Make.Trues.Before.False.Mean(). R's scoping rules allow for a function call to access not only the local environment but also any higher environment such as a package all the way up to the global environment. That is, a user can assign a value to an object in a function call that will not influence the representation of a preexisting object with the same name. When R encounters a reference to an object, it begins by looking among those locally defined before proceeding upwards. As a consequence, the following R code returns $100$ instead of $10$:

```{r Scoping Example, echo = TRUE}
cntr <- 0 ## Create a counter
for(i in 1:10){ ##Define the index i to run over the vector 1:10
  for(i in 1:10){ ##Define ANOTHER index ALSO CALLED i that runs over another vector 1:10. R's scoping rules check the local value i's local value before incrementing at the beginning of the for() loop.  
    cntr <- cntr + 1
  }
}
cntr ##Output
```

In RMarkdown objects that are created in code chunks are saved to the global environment. We can therefore call functions that are not locally defined, which is immensely useful in functional programming.

\subsection{Simulation}

|     The modus operandi of simulation in R is to model how multiple random samples interact over a large number of iterations. R can generate random values from a slew of of common probability distributions including the normal, Poisson, uniform, gamma, and as has already been demonstrated the binomial. Random values can either serve many purposes, such as serving as an a-priori probability distribution from which to create a model, or as a theoretical sample against which to check an empirical distribution. A user can also calculate the PDF, CDF, and quantiles of these various distributions after supplying the necessary formal arguments. I will use the t distribution's quantile function qt() when creating confidence intervals, and the normal probability plotter qqnorm() when checking my empirical quantiles for normality. For probability plotting of other distributions see the R documentation for the function qqPlot() in EnvStats.

|     R uses the psuedo-random number generator (PNRG) algorithm called "The Mersenne Twister" created by Drs. Matsumoto and Nishimura in 1998. (https://cran.r-project.org/web/views/Distributions.html)  (https://dl.acm.org/doi/10.1145/272991.272995) The Mersenne Twister begins generating numbers from a given starting point. Furthermore, the sequence of numbers generated is entirely deterministic. Actions taken in an R script advances the "seed" of the Twister by one. By controlling the initial seed of the Mersenne Twister, the random numbers generated in any set of commands can be exactly recreated. The set.seed() method lets the user specify which seed R should use next, and therefore is an excellent tool for reproducibility of experiments. In my analysis I set the initial seed to $1000$, and then let the Mersenne Twister twist.

|     As the name suggests, R's sample() function allows its user to create random samples from a supplied atomic vector. The four main arguments of sample() that must be considered are the initial population, the sample size, the replacement condition, and whether certain elements should have weighted probabilities of being drawn. Once parameters are selected, we are ready to simulate.  In the data analysis we will use simulation in two contexts - the first is the creation of bootstrap sampling distributions, and the second is to model the approximate number of possible unique values of cumulative GPA as a function of graded term (under some assumptions). I leave these as examples of the functionality of sample(). There are two bootstrapper functions in the analysis - one for testing the significance level of a faculty's mean $\Delta C$ and one for $\Delta W$ - and one cumulative GPA simulator. By name they are make.cum.bootstrap.means(), make.withdraw.bootstrap.props(), and Possible.Cum.GPAs().

\section{The Tidyverse Arsenal}

|     While the R Programming tools I learned from Coursera, probability, and mathematical statistics are sufficient for the creation of the necessary functions to do analyse Professor Eckstrom's dataset, many smart people (Hadley Wickham and Yihui Xie primary among them) have built specialized tools in R for the tidying, manipulation, and presentation of data. The tidyverse is a collection of R packages including ggplot2, dplyr, readr, tibble, and forcats among others. Dr. Nordmoe's data science course MAT-295 is an introduction to the tidyverse approach of data analysis in R, so I will only give a brief summary of the methods I call. I highly recommend reading the following sections in conjunction with the linked tidyverse cheatsheet, as these guides contain information about the syntax and usage of methods found in one package.

\subsection{Piping} (https://www.rdocumentation.org/packages/magrittr/versions/2.0.1)
|     Having said that, there is unfortunately no cheatsheet for the tidyverse package magritrr. In liue I will try my best to clearly indicate the pipe operators syntax and usage. Complicated functions become very illegible very quickly. For example, the proper base R syntax for the composition of a function A $f_A(x,A_1,\ldots)$ with a function B $f_B(x,B_1,\ldots)$ called on an object x is $f_A(f_B(B(x,B_1,\ldots)), A_1,\ldots)$. When composing even as few as four or five functions it quickly becomes difficult to discern which formals are being supplied to which functions. When the tidyverse is loaded using the library() function, the pipe operator `%>%` is exported from the R package magritrr. Using the pipe changes the function composition into a function ordering; $x \quad \%>\%  \quad f_A(A_1, \ldots) \quad \%>\% \quad f_B(B_1,\ldots)$ clearly indicates that the object $x$ is first operated on by the function $f_A$ with formal arguments $\{A_1, \ldots\}$ before being sent into $f_B$ with formals $\{B_1,\ldots\}$.  Many tidyverse methods are built for usage with the pipe operator, allowing for legible data manipulations and transforming R from a functional language into a  an object-oriented language.

\subsection{dplyr} (https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
|    The tidyverse package dplyr provides many streamlined, composable functions for the manipulation of data sets. The filter() function removes all rows of a data set that do not obey a supplied logical condition. Similarly the slice() method can be used to choose rows by index. mutate() and transmute() create new columns or alter existing ones, and rename() is a streamlined wrapper for mutate() that changes only changes the variable name. Conversely, the select() method can either remove variables entirely or just reorder columns. A user can use the arrange() function to reorder the rows of a data set from highest to lowest or vice versa based on a supplied set of variables. Summarise creates a new data frame with columns generated procedurally from its formal arguments. Each of these methods is referred to as a dplyr verb, and works in conjunction with the group_by() function.

|     In my data analysis I used group_by() and its reciprocal ungroup() cumulatively more than any other single dplyr verb. The group_by() method converts a dataframe into a grouped dataframe. By itself this does nothing, but when combined with the other dplyr verbs a user gains enormous analytical power. Using Professor Eckstrom's dataset which I have named Gen_Data as an example;
```{r group_by example, echo = FALSE}
Non.Grouped.Example <- Gen_Data %>%
                        slice(1) %>%
                          head()
kable(Non.Grouped.Example, caption = "All rows returned by slice(1) acting on Gen_Data", padding = 3)

Grouped.Example <- Gen_Data %>%
                    group_by(`Student Random ID`) %>%
                      slice(1) %>%
                        head()

kable(Grouped.Example, caption = "First six rows returned by slice(1) acting on Gen_Data grouped by Student Random ID", padding = 3)
```

|     If possible, dplyr verbs are applied over each group. Table 3 and Table 4 show how grouping allows for greater control of data wrangling. When slice(1) is called on Gen_Data without any grouping, only the first row of the dataset is returned. When slice(1) is called on Gen_Data after the data have been grouped by Student Random ID, the first observation for each student is returned. The row(s) returned therefore depends on the current ordering of Gen_Data, which is controlled in dplyr through the arrange() method. group_by() is also commonly used with the summarise() verb to create custom variables that vary based on group. X %>% group_by() %>% count() is a convenient wrapper for summarise that is generally equivalent to X %>% group_by() %>% summarise(n = n()).

|     There are two general types of join commands available in dplyr. The mutating joins allow for two datasets with at least one common variable called a key to be combined combinatorically into one new dataset. The filtering joins remove rows from a dataset based on the presence or absence of rows in a second by key. These join commands are crucial to the execution of complex functions onto datasets where mutate is insufficient or inefficient.

\subsection{ggplot2}

|     With the notable exception of normal probability plots, every graphic I create in this report is created by the tidyverse package ggplot2. As in dplyr, the methods of ggplot2 are built to work together. Each graphic made in ggplot requires a dataframe (or tibble) and a geom. Geoms are the various graphic methods available; geom_histogram() creates a histogram and geom_point() creates a scatterplot. The nomenclature for most geoms are self-identifying. Ggplot2 uses "aesthetics" to define which variables of the supplied data get mapped to which quantities graphically. Some examples of aesthetics are color, shape, point size, and the x and y coordinates. The + operator is a general symbol which tells R to look for more commands before finishing evaluating. ggplot graphics are constructed using multiple commands connected by the + operator. 

```{r, fig.cap= "An example ggplot2 scatterplot: V1 against V2 with smoothing curve.", echo = TRUE}
Example.Scatterplot.Data <- data.frame(V1 = 1:10, V2 = seq(from = 10, to = 100, by = 10))

Example.Scatterplot.Data %>%
  ggplot(aes(x = V1, y = V2)) +
  geom_point(aes(size = 3)) +
  geom_smooth() +
  labs(size = "Point Size")
```

\subsection{Other packages}

|    Other tidyverse functions I used include tibble() from Tibble as an alternative to data.frame(), str_sub() from stringr to create department names, and factor() in conjunction with fct_reorder() from Forcats for the rearranging of categorical variables in bar charts. Finally, the impact of Yihui Xie et al.'s RMarkdown and knitr cannot be overstated. This SIP would not have compiled - and therefore would not have been possible - without them.


\section{Case Study}

|    Without further adieu, let's do some data analysis. I forego including my program itself in this section in favor of placing a copy of the code in the appendix for the sake of tidiness. To familiarize ourselves with the basic dataset posted by Professor Eckstrom we will take a glimpse of Gen_Data:

```{r First Glimpse of Gen_Data, echo = FALSE}
Gen_Data %>%
  glimpse
```

\subsection{Delta C}

```{r Gen_Data Semester Numbering, include = FALSE}
##Fix Gen_Data semester listing. We want them listed numerically from 1 to N.

Number.Semesters <- function(x) {
  x <- x %>% arrange(Semester)
  
  individual.cases <- x %>% pull(Semester)
  
  individual.cases[1]
  
    for(y in seq_along(along.with = individual.cases)){
      if(individual.cases[y] == "2010/FA"){
        individual.cases[y] <- 1}
      else if(individual.cases[y] == "2011/WI"){
        individual.cases[y] <- 2}
      else if(individual.cases[y] == "2011/SU"){
        individual.cases[y] <- 3}
      else if(individual.cases[y] == "2011/FA"){
        individual.cases[y] <- 4}
      else if(individual.cases[y] == "2012/WI"){
        individual.cases[y] <- 5}
      else if(individual.cases[y] == "2012/SU"){
        individual.cases[y] <- 6}
      else if(individual.cases[y] == "2012/FA"){
        individual.cases[y] <- 7}
      else if(individual.cases[y] == "2013/WI"){
        individual.cases[y] <- 8}
      else if(individual.cases[y] == "2013/SU"){
        individual.cases[y] <- 9}
      else if(individual.cases[y] == "2013/FA"){
        individual.cases[y] <- 10}
      else if(individual.cases[y] == "2014/WI"){
        individual.cases[y] <- 11}
      else if(individual.cases[y] == "2014/SU"){
        individual.cases[y] <- 12}
      else if(individual.cases[y] == "2014/FA"){
        individual.cases[y] <- 13}
      else if(individual.cases[y] == "2015/WI"){
        individual.cases[y] <- 14}
      else if(individual.cases[y] == "2015/SU"){
        individual.cases[y] <- 15}
      else if(individual.cases[y] == "2015/FA"){
        individual.cases[y] <- 16}
      else if(individual.cases[y] == "2016/WI"){
        individual.cases[y] <- 17}
      else if(individual.cases[y] == "2016/SU"){
        individual.cases[y] <- 18}
      else if(individual.cases[y] == "2016/FA"){
        individual.cases[y] <- 19}
      else if(individual.cases[y] == "2017/WI"){
        individual.cases[y] <- 20}
      else{stop("There was an invalid Semester name passed to Number.Semesters")}
      }
    x <- mutate(x, Total.Term = individual.cases)
    x
  }
  
Gen_Data <- Number.Semesters(Gen_Data)
```


```{r N.Profs and N.Students, include = FALSE}
##Total number of professors
Profs.List <- unique(Gen_Data$`Faculty Random ID`)
(n.p <- length(Profs.List))
##Total number of students
Students.List <- unique(Gen_Data$`Student Random ID`)
(n.s <- length(Students.List))
##Total quantity of course entries
n <- length(Gen_Data$`Course Code`)
```

```{r Gen_Data.Grade.Converter, include = FALSE} 
##This chunk creates the GPA colunmn in Gen_Data
###Following grade converter obtained and adapted from stack overflow user A5C1D2H2I1M1N2O1R2T1 on the page https://stackoverflow.com/questions/22746508/r-simplifying-code-to-convert-letter-grades-to-numeric-grades on 8/19/2020

convert_grades <- function(x) {
    if (x == "WS") {
        x <- 0
    } else if (x == "WF"){
      x <- 0
    } else if (x == "W"){
      x <- NA
    } else if (x == "WP"){
      x <- NA
    } else if (x == "A") {
        x <- 4
    } else if (x == "A-") {
        x <- 3.7
    } else if (x == "B+") {
        x <- 3.3
    } else if (x == "B") {
        x <- 3
    } else if (x == "B-") {
        x <- 2.7
    } else if (x == "C+") {
        x <- 2.3
    } else if (x == "C") {
        x <- 2
    } else if (x == "C-") {
        x <- 1.7
    } else if (x == "D+") {
        x <- 1.3
    } else if (x == "D") {
        x <- 1
    } else if (x == "D-") {
        x <- 0.7
    } else if (x == "F") {
        x <- 0
    } else {
        x <- NA
    }
  x <- as.double(x)
    return(x)
}


## Fix broken data points
for(i in 1:n){
  if(Gen_Data$Grade[i] == "@D"){
    Gen_Data$Grade[i] <- "D"
  }
  if(Gen_Data$Grade[i] == "@F"){
    Gen_Data$Grade[i] <- "F"
  }
}

##Apply Grade Converter
Ass.GPA <- Gen_Data$Grade
Ass.GPA[] <- sapply(Ass.GPA, convert_grades)
Gen_Data <- mutate(Gen_Data, "GPA Assigned" = as.numeric(Ass.GPA))
```

|     One observation pertains to one course taken in one semester by one student taught by one faculty. The letter grade recieved for this course is recorded in the variable Grade. Our eventual goal is to create a statistic based on cumulative GPA and graded term, so a good place to start the data wrangling process is converting letter grades to the standard four point GPA scale. The function used for creating GPA scores is called convert_grades(), and was adapted from stackoverflow user A5C1D2H2I1M1N2O1R2T1.  We also create integer representations of the semesters under the variable name Total.Term, where Total.Term takes a minimum at $1$ in the fall semester of 2010 and a maximum at $20$ in the winter semester of 2017. The function which accomplishes this task is Number.Semesters(). 

|     There are some missing values in the variable "GPA Assigned" which correspond to withdraws grades. Students may withdraw from courses for any number of reasons, and so no inference can be properly made about the GPA they would have received. Withdraws will be discarded in the analysis of faculty's mean $\Delta C$'s, but will be integral to the investigation into a faculty's individual $\Delta W$. To analyze trends in cumulative GPA, there must be a cumulative GPA variable. Cumulative GPA is a recursive calculation that depends on the number of credits granted by a course and the previous value of cumulative GPA.  A table of course credits can be found in the appendix. Before we can create cumulative GPA we must first create another variable that will be used in its calculation.

|     Although the Total.Term variable provides a numerical representation of the semester variable, Total.Term does not give information as to which semesters an individual student was enrolled in OCC's math and chemistry departments. In order to create a linear model for cumulative GPA progression as a function of term enrolled, it is easiest to standardize the time scale for each student. The function I created called student.terms() maps any new unique term to the first available natural number given a set of indecies in Gen_Data that define a student.  A helper function student.index() identifies the rows in Gen_Data possessed by a student. For example: the student with random id number $50$ took four courses in the math and chemistry departments in the total term set $\{2,4,8,10\}$. Student number 50 is the third student from the top when arranging by students, so student.index(3) returns the indecies in Gen_Data possessed by the desired student. Passing the output of student.index(3) to student.terms() returns the vector $\{1,2,3,4\}$. The Student.Term variable is initialized in Gen_Data by looping this process over all students. The max.terms variable simply records the largest number in the set of student terms, and will be useful later.

|     Cumulative GPA is calculated by tallying up taking the ratio of the sum of the products of course credits and GPA received with the sum of the course credits. Where C is cumulative GPA, R is course credits, A is the GPA assigned for the course, and the indecies $i \in [1,N]$ refer to all the graded terms that contribute to cumulative GPA:

\begin{equation}
C = \frac{\sum_{i=1}^{N} R_i*A_i}{\sum_{i=1}^{N} R_i}
\label{Cum.GPA Calculation}
\end{equation}

|    Although unnecessary, to observations that resulted in withdraws I assign the previous recorded cumulative GPA if extant. This is just for bookkeeping, and does not affect any statistical analysis or graphic produced. If a student has withdraws from a course without having a previous graded course in the math and chemistry departments, the observation of cumulative GPA retains its missing value.

```{r Initialize Student.Term, cache = TRUE, include = FALSE}
##Here we convert Total.Term into numeric, and create the Student.Term variable which describes a student's term progression through OCC

##Initialize Student.Term into Gen_Data. The arrange step is critical for proper student indexing.
Gen_Data <- Gen_Data %>%
              mutate(Total.Term = as.integer(Total.Term)) %>%
                arrange(`Student Random ID`, Total.Term)

##Function that determines which indecies within Gen_Data a given student possesses
student.index <- function(cntr){
  student.id <- unique(Gen_Data$`Student Random ID`)[cntr]
  student.indecies <- which(Gen_Data$`Student Random ID` == student.id)
  return(student.indecies)
}
    
##Function that numbers a student's terms in OCC's math and chemistry departments using positive integers. 
student.terms <- function(index){
  total.terms <- Gen_Data$Total.Term[index]
  student.terms <- rep(0, length(total.terms))
  last.total.term <- total.terms[1]
  student.terms[1] <- 1
  
  if(length(total.terms) >= 2){
  
    for(i in 2:length(total.terms)){
      if(last.total.term == total.terms[i]){
        student.terms[i] <- student.terms[i-1]
      } else{
        last.total.term <- total.terms[i]
        student.terms[i] <- student.terms[i-1] + 1
      }
  }
    
  }
  return(student.terms)
}


##Use functions to assign student terms. This algorithm is very slow - I couldn't think of a better way to do it. It works and only has to compile once, so it's good enough. sapply did not want to work so I use a manual application instead.
n.s <- length(unique(Gen_Data$`Student Random ID`))

Gen_Data2 <- Gen_Data %>%
              mutate(Student.Term = 0)
for(i in 1:n.s){
  Gen_Data2$Student.Term[student.index(i)] <- student.terms(student.index(i))
}
```

```{r Initialize Department and make N.Courses.By.Term, echo = FALSE}
##Create a new data frame that tracks individual students through their years at OCC. First we need to figure out how many classes a student takes in math/chem departments by term.

##Initialize Department in Gen_Data
Gen_Data3 <- Gen_Data2 %>%
              mutate(Department = str_sub(`Course Code`,1,3))

N.Courses.By.Term <- Gen_Data3 %>%
                      group_by(`Student Random ID`, Student.Term, Department) %>%
                        count() %>%
                          ungroup() %>%
                            rename(Num.Courses = n) %>%
                              group_by(Department, Num.Courses) %>%
                                count()

kable(N.Courses.By.Term, caption = "Frequency table of the number of different course codes taken in one term by department", padding = 3)
##This dataset can be used to show that students are much more likely to take n math courses simultaneously than n chemistry courses. Additionally, it tells us we need to make a robust points tracking algorithm. Student 28108 passed two math courses AND a chemistry course simultaneously (during student.term 3), which tells us that a student can take multiple courses per semester in the same department.
```

```{r Initialize Class Size, include = FALSE}
##Create class size variable for statistical significance tests. There are 5 courses with iffy class sizes: 861, 2636, 3894, 4234, and 3148. The first four of these have n = 1 - they are independent studies. 3148 had n = 3, I'm not sure how that happened. Doing this filtration gives n > 7, which will allow for safe usage of the t test and bootstrap. Moreover, the distribution of received grades for small class size may be inherently different from the overall distribution. Fortunately we only had to remove very few cases, so overall significance is essentially undamaged.
Good.Class.Size <- Gen_Data3 %>%
                      group_by(`Course Code`,`Faculty Random ID`, Total.Term) %>%
                        count() %>%
                          mutate(class_size = n, Significant = if_else(n > 7, TRUE, FALSE)) %>%
                            dplyr::select(-n) %>%
                              filter(Significant == TRUE)

Gen_Data4 <- left_join(Good.Class.Size, Gen_Data3, by = c("Faculty Random ID", "Total.Term", "Course Code")) %>%
              dplyr::select(-Significant)
```

```{r Initialize Max Terms, include = FALSE}
##Initialize Max Terms variable in Gen_Data
Max_terms <- Gen_Data4 %>%
              arrange(`Student Random ID`, Student.Term) %>%
                group_by(`Student Random ID`) %>%
                  summarise(max.terms = max(Student.Term))

Gen_Data5 <- left_join(Gen_Data4, Max_terms)
```

```{r Initialize Course Credits and Course Names, include = FALSE}
##In order to create cumulative GPA we need to weight by credit hours. This chunk creates a credit variable for each row in Gen Data

##Found CHE-0950 credits at https://www.oaklandcc.edu/finaid/docs/fa39a_satisfactoryacademic.pdf . I could not find the name, so I'm calling it preparation for chemistry because it is high school level chemistry according to https://wmich.edu/sites/default/files/attachments/u684/2016/Chemistry-%20Oakland%20CC_0.pdf  
##Found MAT-1045 credits at https://www.oaklandcc.edu/schedule/docs/OCC_Schedule_2017_WI.pdf
#Found MAT-1140 credits at http://dalnetarchive.org/bitstream/handle/11061/1402/2007%20Mathematics-%20Curriculum%20Review%20Self%20Study.pdf?sequence=1&isAllowed=y

##Makes a lookup table for Course Code, Course Names, and Course Credits
Course.Credits <- tibble(`Course Code` = unique(Gen_Data4$`Course Code`),
                  `Course Credits` =  c(4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4 , 4 , 3, 4, 4, 4, 4, 3, 4, 4, 5, 4),
                    `Course Names` = c("Preparation for Chemistry", "Introductory Chemistry", "Survey of Organic and Biochemistry", "General Chemistry I", "General Chemistry II", "Organic Chemistry I", "Organic Chemistry II", "Organic Chemistry Lab", "Fundamentals of Arithmetic", "Preparation for Algebra", "Buisness Mathematics", "Elementary Algebra", "Math Literacy", "Plane Geometry", "Intermediate Algebra", "Finite Mathematics", "Quantitative Reasoning", "College Algebra", "Trigenometry", "Statistics", "Applied Calculus", "Precalculus", "Calculus I"))

##Store Course Credits into Gen_Data
Gen_Data6 <- Gen_Data5 %>%
                left_join(Course.Credits, by = "Course Code")
```

```{r Cumulative GPA, warning = FALSE, include = FALSE}
## This chunk creates the cumulative GPA variable in Gen_Data. This allows us to track our students through OCC without having such a stratified model - cumulative GPA is much less discrete than GPA Assigned.

##This first section creates the Cum.GPA variable with a simple weighted mean in a dummy dataset which will eventually be stored as Gen_Data
No_Withdraws <- Gen_Data6 %>%
                        arrange(`Student Random ID`, Student.Term) %>%
                                filter(!is.na(`GPA Assigned`)) %>%
                                        mutate(Cum.GPA = 0)
##Set initial conditions
current.numerator <- 0
current.denominator <- 0
current.student <- 31
n.w <- length(No_Withdraws$`GPA Assigned`)
for(i in 1:n.w){ ##Loop over all terms that did not result in withdraws
        if(No_Withdraws$`Student Random ID`[i] != current.student){ ##If the ith row of No_Withdraws is a new student, record that student's id and reset numerator and denominator
                current.numerator <- 0
                current.denominator <- 0
                current.student <- No_Withdraws$`Student Random ID`[i]
        }
        ## Increment numerator and denominator using the ith row of GPA assigned and the course credits
        current.numerator <- current.numerator + No_Withdraws$`GPA Assigned`[i] * No_Withdraws$`Course Credits`[i]
        current.denominator <- current.denominator + No_Withdraws$`Course Credits`[i]
        ##Store the current observation of cumulative GPA to No_Withdraws
        No_Withdraws$Cum.GPA[i] <- current.numerator / current.denominator
                
}

##Create a temporary dataframe from Gen_Data with the current information on cumulative GPA
Temp_Data <- Gen_Data6 %>%
                left_join(No_Withdraws) %>%
                        arrange(`Student Random ID`, Student.Term)
 

##This second section assigns the previous Cum.GPA to all withdraws if the student had a previous Cum.GPA. This allows for better visualization in ggplot, and better modeling with lmer.

##Index starts at 2 so that we can fetch the student that came before. This is ad hoc programming, there is likely a better way to do it.
for(i in 2:length(Temp_Data$`Student Random ID`)){
  ##Check the current student using the ith row of Temp_Data and the previous student using the i-1th row.
        current.student <- Temp_Data$`Student Random ID`[i]
        previous.student <- Temp_Data$`Student Random ID`[i-1]
        
        ##Cumulative GPA will be missing for every withdrawn course, so if the current and previous students are identical we simply copy the previous observation of cumulative GPA from the i-1th row to the ith row. If they are not, the the current student withdrew in their first n courses, and these should not obtain a cumulative GPA. 
        if(is.na(Temp_Data$Cum.GPA[i])){
          if(is.na(current.student)){ ##When we run out of students, end the loop. This is a safety measure.
            break
          }
          if(current.student == previous.student){
            Temp_Data$Cum.GPA[i] <- Temp_Data$Cum.GPA[i-1] 
          }
        }
}

##Here we store the Cum.GPA variable into Gen_Data.
Gen_Data7 <- Temp_Data
```


```{r No muli-department faculty, include = FALSE}
## This chunk shows that there are no faculty that teach in both the math and chemistry departments.
counter <- Gen_Data7 %>%
              group_by(`Faculty Random ID`, Department) %>%
                count() 
##This is the difference between the total number of faculty - department combinations and the number of unique faculty.
length(counter$`Faculty Random ID`) - length(unique(Gen_Data7$`Faculty Random ID`))
```

\subsubsection{First Faculty Effect on Final Cumulative GPA}

|     Ideally our measure of faculty effect on final cumulative GPA would not require the faculty to be the first encountered in a department, because some professors may only teach upper level courses and therefore have many fewer students start with them. This decrease in sample size increases standard error and thus makes significance difficult to prove. Unfortunately, not all faculty can contribute equally to final cumulative GPA. For a student that takes four courses in the math and chemistry departments the first encountered faculty might be said to have a four term effect whereas the last faculty only contributes to the final term. It is unfair to give a "good" last faculty a negative score because of the contributions of previous "bad" faculty. We therefore restrict our inquiry to the first faculty encountered. Tables $6$ and $7$ describe how the overall sample size is affected by each filtration condition. 

```{r Not First Withdraw Justification, warning=FALSE}
##Create plot to illustrate the inability to use all students for Q1. We must then do a filtration!
Plot.Data1 <- Gen_Data7 %>%
                group_by(`Student Random ID`) %>%
                  slice(1) %>%
                    mutate(First.Term.Withdraw = if_else(is.na(Cum.GPA), "Yes", "No")) %>%
                      dplyr::select(`Student Random ID`, First.Term.Withdraw) %>%
                        right_join(Gen_Data7, by = "Student Random ID") %>%
                          filter(!is.na(`GPA Assigned`)) %>%
                            mutate(At.Least.Two.Grades = if_else(n() >= 2, "Yes", "No")) %>%
                              dplyr::select(`Student Random ID`, At.Least.Two.Grades, First.Term.Withdraw) %>%
                                distinct() %>%
                                  right_join(Gen_Data7, by = "Student Random ID")

First.Term.Withdraw.Props <- Plot.Data1 %>% 
                              slice(1) %>%
                                mutate(First.Term.Withdraw = if_else(!is.na(First.Term.Withdraw), First.Term.Withdraw, "Yes")) %>%
                                  ungroup() %>%
                                    group_by(First.Term.Withdraw) %>%
                                      count() %>%
                                        ungroup() %>%
                                          summarise(First.Term.Withdraw = First.Term.Withdraw, prop = n / sum(n))

kable(First.Term.Withdraw.Props, caption = "Proportion of students in OCC's math and chemistry departments that withdrew from their first course.", padding = 3)

At.Least.Two.Grades.Props <- Plot.Data1 %>% 
                              slice(1) %>%
                                mutate(At.Least.Two.Grades = if_else(!is.na(At.Least.Two.Grades), At.Least.Two.Grades, "No")) %>%
                                  ungroup() %>%
                                    group_by(At.Least.Two.Grades) %>%
                                      count() %>%
                                        ungroup() %>%
                                          summarise(At.Least.Two.Grades = At.Least.Two.Grades, prop = n / sum(n))

kable(At.Least.Two.Grades.Props, caption = "Proportion of students in OCC's math and chemistry departments that recieved grades for at least two courses", padding = 3)
```

|     Before we proceed, let's examine some potential source of error. The data we have are only for the courses that students took in the math and chemistry departments during their stay at OCC. We cannot with any degree of certainty claim that those first recorded terms in the dataset are indeed the first terms at Oakland Community College. Student $X$ could have taken an English course in a term before appearing in Professor Eckstrom's data. We have implicitly filtered for: (1) students who did not for some reason attend a standard four year college, and (2) students that wished to take math and/or chemistry courses in community college. We have moreover restricted our departmental information to only chemistry and mathematics courses, which removes our ability to discern the possibility that the math or chemistry departments are outliers in terms of cumulative GPA progression and withdraw rate. I will now indulge in conjecture.  Students pursuing degrees in other departments may be less likely to complete numerous courses in the math and chemistry departments. Students only filling prerequisites such as quantitative reasoning or calculus one would be less likely to pass the filtration condition of taking at least two graded terms. The error is therefore mitigated because problematic observations are more likely to get picked off. Such an effect is not falsifiable without a larger dataset containing more departments for comparison, and is therefore an important follow-up question. 

```{r Execute DeltaC Filtration Conditions, include=FALSE}
##Initialize number of courses per term,  filter for those students that took only one course in their first term and took at least two graded terms in the math or chemistry departments. That is, math first term then chemistry second term is totally legal.
Course.Num <- Gen_Data7 %>%
                group_by(`Student Random ID`, Student.Term) %>%
                  count() %>%
                    right_join(Gen_Data7, by = c("Student Random ID", "Student.Term")) %>%
                      rename(Num.Courses.In.Term = n) %>%
                        filter(max.terms >= 2, Num.Courses.In.Term*Student.Term == 1)



##Initialize First Prof and First Course
Gen_Data8 <- Course.Num %>%
              mutate(First.Prof = `Faculty Random ID`, First.Course = `Course Code`) %>%
                ungroup() %>%
                  dplyr::select(First.Prof, `Student Random ID`, First.Course) %>%
                    right_join(Gen_Data7, by = c("Student Random ID"))

##Filter for students that did not withdraw from first course
Not.First.Withdraw <- Gen_Data8 %>%
                       ungroup()%>%
                          group_by(`Student Random ID`) %>%
                            slice(1) %>%
                              filter(Student.Term == 1, !is.na(`GPA Assigned`), !is.na(First.Prof)) %>%
                                dplyr::select(`Student Random ID`)

Filtered.Data1 <- Gen_Data8 %>%
                    semi_join(Not.First.Withdraw, by = "Student Random ID") %>%
                      group_by(`Student Random ID`) %>%
                        mutate(Withdraw = if_else(is.na(`GPA Assigned`), TRUE, FALSE)) %>%
                          mutate(Total.Grades = sum(Withdraw == FALSE))

##Filter for only those students that recieved grades in at least two terms. We can use the number of withdraws to determine this, because we have already guarenteed the student did not withdraw in the first term, and there was only one course in the first term.
At.Least.Two.Graded.Terms <- Filtered.Data1 %>%
                                filter(Total.Grades >= 2) %>%
                                  slice(1) %>%
                                    dplyr::select(`Student Random ID`)

Filtered.Data1 <- Filtered.Data1 %>%
                    semi_join(At.Least.Two.Graded.Terms, by = "Student Random ID")
                      

##Number of students removed by filtration
N.S.Removed <- length(unique(Gen_Data$`Student Random ID`)) - length(unique(Filtered.Data1$`Student Random ID`))
##Number of remaining students
N.S.Remaining <- length(unique(Filtered.Data1$`Student Random ID`))
##Proportion of students remaining
Prop.S.Remaining <- N.S.Remaining/(N.S.Remaining + N.S.Removed)
##Number of faculty removed by filtration
N.F.Removed <- length(unique(Gen_Data$`Faculty Random ID`)) - length(unique(Filtered.Data1$`Faculty Random ID`))
##Number of remaining faculty
N.F.Remaining <-length(unique(Filtered.Data1$`Faculty Random ID`))
##Proportion of faculty remaining
Prop.F.Remaining <- N.F.Remaining/(N.F.Remaining + N.F.Removed)

##Show that there are no entries of Filtered.Data1 that have an NA value for First.Prof - that is we have successfully chosen only those students who did not withdraw from their first course
N.No.First.Prof <- length(Filtered.Data1$First.Prof[is.na(Filtered.Data1$First.Prof)])
```

|    After filtering for students that (1) took only one course in their first term which (2) they received a grade for and (3) proceeded to receive a grade in a different term,  we are left with `r N.S.Remaining` unique students from the original `r N.S.Removed + N.S.Remaining`, distributed over `r N.F.Remaining` unique faculty members. Only one faculty member is removed entirely by these conditions, though many more will have insufficient data to be significantly differentiated from the mean professor. The mean faculty member contributes to `r N.S.Remaining/N.F.Remaining` measurable outcomes. The new variable "First.Prof" is created for all students that passed filtration. As the name suggests First.Prof simply takes value equal to a student's first observed faculty, which is a unique value due to filtration.

```{r Initialize Graded.Term, cache = TRUE, include = FALSE}
##Here we convert Total.Term into numeric, and create the Graded.Term variable which describes which terms had effect on cumulative GPA

##Here we filter out all entries that resulted in Withdraws. Withdraws do not affect cumulative GPA, and they make the calculation of slope more diffcult. We will analyze withdraw rate separately later.
Temp_Data2 <- Filtered.Data1 %>%
                filter(Withdraw == FALSE)

##Function that determines which indecies within Temp_Data2 a given student possesses
student.index2 <- function(cntr){
  student.id <- unique(Temp_Data2$`Student Random ID`)[cntr]
  student.indecies <- which(Temp_Data2$`Student Random ID` == student.id)
  return(student.indecies)
}
    
##Function that numbers a student's graded terms in OCC's math and chemistry departments using positive integers. 
graded.terms <- function(index){
  total.terms <- Temp_Data2$Total.Term[index]
  graded.terms <- rep(0, length(total.terms))
  last.total.term <- total.terms[1]
  graded.terms[1] <- 1
  
  if(length(graded.terms) >=2){
  
    for(i in 2:length(graded.terms)){
      if(last.total.term == total.terms[i]){
        graded.terms[i] <- graded.terms[i-1]
      } else{
        last.total.term <- total.terms[i]
        graded.terms[i] <- graded.terms[i-1] + 1
      }
  }
    
  }
  return(graded.terms)
}


##Use functions to assign graded terms.
n.s <- length(unique(Temp_Data2$`Student Random ID`))

Filtered.Data1 <- Temp_Data2 %>%
                    mutate(Graded.Term = 0)
for(i in 1:n.s){
  Filtered.Data1$Graded.Term[student.index2(i)] <- graded.terms(student.index2(i))
}
```

\subsection{Linear Mixed-Effects Modeling}

|     As of 4/2021, the package lme4 was an up-to-date mixed modeling package available through CRAN. (https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf) The lme4 function lmer() allows the user to generate a linear model with a combination of fixed and random effects. A linear mixed-effects model represents a dependent variable - in our case the cumulative GPA of a student - as a linear function of some inputs. The benefit of using such a model is that instead of just using a fixed effect as predictor, such as the student's term number, we can manually specify certain variables to lme4 that either effect the intercept or the slope of our linear model. One issue with lmer() is that  Each student enters the dataset with various levels of previous exposure to mathematics and chemistry, and encounter different challenges during their first term; there are myriad factors which influence a student's first observation of GPA. Mixed-effects models allow an analyst to adjust for these factors.  We will build a linear mixed-effects model using lmer() and from scratch, and compare the results.

\subsubsection{Confounding Variables}
|    One influence on future cumulative GPA achievement is the course in which students begin their tenure at OCC's math/chemistry departments. For instance - there may be a difference in both the final cumulative GPA and the slope for cumulative GPA in the mean student that begins their community college math courses in Preparation for Algebra (MAT - 1050) and a student that began in Calculus II (Mat - 1740). Figure 5 is a series of boxplots that show the distribution of final cumulative GPA for students that begin in each allowed first course. It is clear from this plot that the best linear model for a student that starts in a particular course would be specific to their peers. If we were to lump the students that started in CHE-1510 with the students that started in MAT-1050, the chemistry students would tend to finish with cumulative GPAs above the mean just as the math students would be erroneously assigned negative scores. 

```{r Strata Justification, fig.cap = "Boxplots of Final Cumulative GPA for all Students that  Passed Filtration Conditions, Colored by First Course"}
##Create overall visualization of final cumulative GPA by first course
Filtered.Data1 %>%
  ungroup() %>%
    group_by(`Student Random ID`) %>%
      slice(n()) %>%
        dplyr::select(Cum.GPA, `Student Random ID`) %>%
          rename(Last.Term.Cum.GPA = Cum.GPA) %>%
            right_join(Filtered.Data1) %>%
              group_by(`Student Random ID`) %>%
                slice(n()) %>%
                  ungroup() %>%
                    ggplot() +
                    geom_boxplot(aes(x = Last.Term.Cum.GPA, color = `First.Course`)) +
                    theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
                    guides(color = guide_legend(ncol = 2)) +
                    labs(x = "Final Cumulative GPA", y = "Course Code", color = "First Course")
```

```{r Make Max.Terms Mean Final Cumulative GPA Data}
##Plot effect of max.terms onto final cumulative GPA for terms that had at least 30 students.
Term.Base <- Gen_Data8 %>%
              filter(max.terms == Student.Term, !is.na(Cum.GPA)) %>%
                ungroup()

Term.Means <- Term.Base %>%
                group_by(max.terms) %>%
                  summarise(Term.Mean.Cum.GPA = mean(Cum.GPA, na.rm = TRUE), Num.Students = n()) %>%
                    filter(Num.Students >= 30) %>%
                      mutate(Conf.Int.Low = 0, Conf.Int.High = 0)
                      
Term.confint1 <- function(term, conf.side){
  
  Term.GPAs <- Term.Base %>%
                  filter(max.terms == term) %>%
                    pull(Cum.GPA)
  n.sims <- 10^4
  boot.means <- rep(0, n.sims)
  for(i in 1:n.sims){
    boot.sample <- sample(Term.GPAs, size = length(Term.GPAs), replace = TRUE)
    boot.means[i] <- mean(boot.sample)
  }
  if(conf.side == "Low"){
    return(quantile(boot.means, 0.025))
  }
  if(conf.side == "High"){
    return(quantile(boot.means, 0.975))
  }
}

for(i in 1:length(Term.Means$max.terms)){
  Term.Means$Conf.Int.Low[i] <- Term.confint1(i, "Low")
  Term.Means$Conf.Int.High[i] <- Term.confint1(i, "High")
}

p <- Term.Means %>%
      filter(Num.Students >= 30) %>%
        ggplot(aes(x = max.terms, y = Term.Mean.Cum.GPA)) +
        geom_point(aes(size = Num.Students)) +
        geom_smooth() +
        scale_x_continuous(limits = c(1, 10), breaks = seq(from = 1, to = 10, by = 1)) +
        labs(x = "Number of Terms in Math and Chemistry Departments", y = "Mean Final Cumulative GPA (0-4 Scale)", title = , size = "Students in Category")

for(i in 1:length(Term.Means$max.terms)){
  Conf.Low <- Term.Means$Conf.Int.Low[i]
  Conf.High <- Term.Means$Conf.Int.High[i]
  p <- p + geom_segment(x = i, xend = i, y = Conf.Low, yend = Conf.High, color = "red")
}
```

|     Students take courses in the math and chemistry departments for different number of terms. Oakland Community College only grants Associate's degrees in math and chemistry - the highest math course is Linear Algebra, and the highest chemistry course is Organic Chemistry II. While it may be a false equivalency to compare students that enroll in community college to those that enroll in standard four year schools, the highest courses offered at OCC should be completed at the latest by the end of a four-year student's second year assuming the student is majoring in that field. By this rough heuristic I estimate that a student beginning in Calculus 1 should complete OCC's math regime in about four terms at one course per term. A student might not take a math course every term, and community college students that are seeking mathematics degrees may require additional training before calculus. 

```{r Plot Max.Terms Mean Cumulative GPA, fig.cap = "Mean Final Cumulative GPA Vs. Number of Terms in the Math and Chemistry Departments, with 95% Confidence Interval"}
p
```

|     As can be seen in figure six, the mean final cumulative GPA for those students that take between three and seven courses are essentially identical. Students that complete their stay in OCC's math and chemistry departments within this five course spread have the empirically highest mean cumulative GPA's by the time they finish. Past each extreme of this range mean final cumulative GPA begins to fall. There are two orientations, to each of which I will assign a speculative reason for the lower cumulative observed GPA. Please note the following hypotheses are merely my guesses at the mechanisms, and there are likely additional factors that I have not thought of that affect mean final cumulative GPA by max term.

1. $\textbf{Less than three terms:}$ A student that takes less than three terms in the math and chemistry departments may be testing out community college itself, or perhaps trialing the courses offered in the respective departments. Such students may not be as committed to their learning as those pursuing an associate's degree in math or chemistry. By choosing the students that only take one two terms of math and chemistry, we have retroactively largely filtered out those students that ARE pursuing such a degree, thus lowering the mean final cumulative GPA. An alternate explanation is that if a student does poorly in their first one or two courses in a department, they become less likely to continue study in that field. There would then be a higher concentration of lower grades in the first few terms, as we observe. 

2. $\textbf{Greater than seven terms:}$ Students that are hellbent on obtaining a degree in math or chemistry will take courses in those departments until they either finish the degree or change their conviction. If a student fails multiple courses within the department, they will necessarily need to take additional terms to retake the classes. They will then have both a higher number of terms taken, and a lower cumulative GPA. 

|     The mean final cumulative GPA for all terms lie within the set (`r min(Term.Means$Term.Mean.Cum.GPA)`, `r max(Term.Means$Term.Mean.Cum.GPA)`) GPA points. The minimum occurs in those students that only take one term in the math and chemistry departments, and the maximum occurs at five terms. The range is only about $0.3$ GPA points, so the effect that the total terms taken is rather small, approximately the difference between a C average and a C+ average.

|     As the number of terms increases, the number of students that were enrolled in the math and chemistry department for that number of terms decreases exponentially and moreover monotonically. I was forced to filter for terms that had at least thirty students finish within them, thus losing a small amount of data (on the order of $50$ students). The confidence intervals for the later terms become increasingly wide, to the point where the negative trend observed in terms $8$ through $10$ is barely significant at the $\alpha = 0.05$ level. Another interesting statistic is the mean change in cumulative GPA per term, which we will look at presently.

```{r Initialize Max.Term Slopes}
## The following amounts to the first derivative of the previous graph. 

Max.Terms.Base <- Gen_Data8 %>%
                    filter(max.terms != 1) %>%
                      ungroup() %>%
                        filter(!is.na(Cum.GPA)) %>%
                          group_by(`Student Random ID`) %>%
                            filter(n() > 1) %>%
                              slice(n()) %>%
                                mutate(Final.Cum.GPA = Cum.GPA) %>%
                                  dplyr::select(`Student Random ID`, Final.Cum.GPA) %>%
                                    right_join(Gen_Data8, by = "Student Random ID") %>%
                                      filter(!is.na(Cum.GPA), !is.na(Final.Cum.GPA)) %>%
                                        mutate(Graded.Terms = n()) %>%
                                          ungroup() %>%
                                            group_by(`Student Random ID`) %>%
                                              slice(1) %>%
                                                mutate(GPA.Slope = (Final.Cum.GPA - Cum.GPA)/Graded.Terms) %>%
                                                  ungroup()

Max.Term.Mean.GPA.Slopes <-  Max.Terms.Base %>%
                              group_by(max.terms) %>%
                                summarise(Mean.GPA.Slope = mean(GPA.Slope), Num.Students = n()) %>%
                                  filter(Num.Students > 30) %>%
                                    mutate(Conf.Int.Low = NA, Conf.Int.High = NA)

##This function makes a 95% confidence interval for MAx.Term.Mean.GPA.Slopes, the minimum of which will be stored as Cont.Int.Low, and the maximum of which will be stored as Conf.Int.High
Term.confint2 <- function(term, conf.side){
  Term.Slopes <- Max.Terms.Base %>%
                  filter(max.terms == term) %>%
                    pull(GPA.Slope)
  
  n.sims <- 10^4
  boot.means <- rep(0, n.sims)
  for(i in 1:n.sims){
    boot.sample <- sample(Term.Slopes, size = length(Term.Slopes), replace = TRUE)
    boot.means[i] <- mean(boot.sample)
  }
  if(conf.side == "Low"){
    return(quantile(boot.means, 0.025))
  }
  if(conf.side == "High"){
    return(quantile(boot.means, 0.975))
  }
}

##Create 95% confidence bounds for each valid term
for(i in seq_along(Max.Term.Mean.GPA.Slopes$max.terms)+1){
  Max.Term.Mean.GPA.Slopes$Conf.Int.Low[i-1] <- Term.confint2(i, "Low")
  Max.Term.Mean.GPA.Slopes$Conf.Int.High[i-1] <- Term.confint2(i, "High")
}
##Make graphic base
p <- Max.Term.Mean.GPA.Slopes %>%
      ggplot(aes(x = max.terms, y = Mean.GPA.Slope)) +
      geom_point(aes(size = Num.Students)) + 
      geom_smooth() +
      scale_x_continuous(limits = c(2,10), breaks = seq(from = 2,to = 10, by = 1)) +
      labs(x = "Total Number of Terms in Math and Chemistry Departments", y = "Mean Change in Cumulative GPA per Term", size = "Number of Students")

##Plot 95% confidence bounds for each term
for(i in 1:length(Max.Term.Mean.GPA.Slopes$max.terms)){
  Conf.Low <- Max.Term.Mean.GPA.Slopes$Conf.Int.Low[i]
  Conf.High <- Max.Term.Mean.GPA.Slopes$Conf.Int.High[i]
  p <- p + geom_segment(x = i+1, xend = i+1, y = Conf.Low, yend = Conf.High, color = "red")
}
```

```{r Plot Max.Term Slopes, fig.cap = "Mean change in cumulative GPA per term Vs. maximum number of terms, with 95% confidence intervals."}
##Add a horizontal line at zero to denote the null hypothesis
p +
  geom_hline(yintercept = 0, color = "green")
```

|     Figure seven compares the mean change in a student's GPA against the number of terms that they took courses in the math and chemistry departments. This statistic is measured as the difference between final cumulative GPA and the first recorded observation of GPA, divided by the number of graded terms.  As in figure six, ten is the highest term number for which there is sufficient data for inference. I did not have any guesses regarding whether certain terms would yield positive or negative effects, so we have used a two-sided confidence interval. These will be compared about the expected effect of zero. If we let the variable measured on the vertical axis be $s$ for slope, the hypotheses we will test for significance for each term are:
\begin{align*}
H_0: s = 0\\
H_A: s \neq 0
\end{align*}

|     For all terms except 10, at the $\alpha = 0.05$ level we reject the null hypothesis in favor of the alternative. Because we were using a two sided confidence interval, we have shown that for terms one through nine that $s < 0$. The $95\%$ confidence interval for term 10 intersects with the green line demarking the null hypothesis, so for term 10 we fail to reject $H_0$. We are running ten tests at the $\alpha = 0.05$ level, and so we would expect roughly one in twenty experiments to have a population mean outside of the confidence interval. For most of these CI's, the boundary is rather tight and so even in the case of such errors, the population mean is unlikely to usurp the validity of our results. Note that these changes in slope are relatively small. The average student that took five terms in OCC's math and chemistry department experienced a $-0.04$ decrease in cumulative GPA per term, for a total of $-0.24$ GPA points. The end effect is rather large, so it is worth adjusting for in the upcoming linear mixed-effects models.

|     Using these plots we have shown that students with the highest mean final cumulative GPA tend to take between three and seven terms in the math and chemistry departments, and that those students also have a downwards trend in their cumulative GPA from their entry into the departments to the last course they take.

\subsubsection{Deciding on a Model}

|     To provide a better prediction on final cumulative GPA we create a stratified model. Each course is assumed to have its own mean final cumulative GPA slope against which to measure student outcomes. A student's expected final cumulative GPA is determined by adding the GPA assigned by the first professor (a random effect) to the product of the number of graded terms and the first course's mean cumulative GPA slope (a fixed effect). Where $C_E$ is expected cumulative GPA, G_0 is the first observation of GPA, T is the current graded term number, S is the first course's change in mean cumulative GPA per term, and k is an adjustment on slope determined by the current term, the linear mixed-effects model is defined as follows:
\begin{equation}
C_E = G_0 + T(S+k)
\label{Linear Mixed-Effects Model}
\end{equation}
|     Each student is given their own affine predictor. By evaluating a student's model at their final graded term we determine the final cumulative GPA their average peer would have received given $G_0$. The derivation of $\Delta C$ follows as in equation ($\ref{DeltaC Definition}$). Note that there was only a single student to start in MAT-1140 and pass filtration: student number 460449. It will be impossible to create a valid model for that student because their $\Delta C$ will always get mapped to precisely zero. We will create models in two contexts: using the lmer() function and manually. We will use whichever ends up being the better of the two models.

|     The lmer() method from the R package lme4 provides a straightforward method of creating hierarchical models. First a user must specify how the variables are related, then define the data from which to obtain said variables. In lmer() syntax, a fixed effect is denoted by the name of the variable with no additional formatting. For example, the model "Cum.GPA ~ Graded.Term" would simply create a standard fixed-effects linear regression model with dependent variable cumulative GPA and independent variable graded term number.  To define a random intercept with no correlated variables, we use the (1 | X) notation: "Cum.GPA ~ Graded.Term + (1 | `Student Random ID`)" creates a model where all students have the same change in cumulative GPA per term, but may start with different GPAs. Allowing a correlated variable to act as both a random slope modifier and as a random intercept modifier requires the (1 + Y | X) clause. lmer() has the notable benefit of allowing grouping to be defined within the model to prevent unnecessary duplication of effort. Unfortunately, when I used this feature the models failed to converge. It is only a little more work to manually stratify the students by first course before running mixed effects models on each. After adjusting for first course, each of the 19 linear mixed-effects models have the same formula: "Cum.GPA ~ Graded.Term + (1 + Graded.Term | `Student Random ID`)". This allows each student's random deviation from the mean to affect both their starting point and their cumulative GPA slope. $\Delta C$s are calculated in accordance with equation ($\ref{DeltaC Definition}$) using the lmer() derived mean slopes per stratum.

|     As we will see later, lme4 has some issues with my data when supplied - likely from user error. I did quite a lot of reading on linear mixed-effects models, and recreated the concept from scratch using dplyr functions. First I stratify by each combination of total terms in the math and chemistry department and first course taken that had at least eight students. Now that the confounding variables and outliers are accounted for, we calculate the mean change in cumulative GPA per term for each stratum. These are the slopes of the linear mixed-effects models. Finally, we run the slope from the first observation of cumulative GPA, then calculate $\Delta C$ in accordance with equation ($\ref{DeltaC Definition}$).

```{r Make Manual DeltaC Models and DeltaCs, include = FALSE}

Filtered.Data2 <-  Filtered.Data1 %>%
                      ungroup() %>%
                        group_by(`Student Random ID`) %>%
                          slice(1) %>%
                            mutate(First.Course.GPA = Cum.GPA) %>%
                              dplyr::select(`Student Random ID`, First.Course.GPA) %>%
                                right_join(Filtered.Data1, by = "Student Random ID")

##  Store student's last term cumulative GPA to a variable in Filtered.Data for error analysis
Filtered.Data3 <- Filtered.Data2  %>%
                    ungroup()%>%
                      group_by(`Student Random ID`) %>%
                        slice(n()) %>%
                          mutate(Last.Term.Cum.GPA = Cum.GPA) %>%
                            dplyr::select(`Student Random ID`, Last.Term.Cum.GPA) %>%
                              right_join(Filtered.Data2, by = "Student Random ID")

##Initialize Course Slopes into Filtered.Data. This uses the maximum terms taken as the denominator, which approximates that student's Cum.GPA slope does not change during their stay in OCC's math and chemistry departments.
Filtered.Data4 <- Filtered.Data3 %>%
                    group_by(`Student Random ID`) %>%
                      mutate(Cum.GPA.Slope = (Last.Term.Cum.GPA - First.Course.GPA)/max(Graded.Term))

Course.Mean.Cum.GPA.Slopes <- Filtered.Data4 %>%
                                slice(1) %>%
                                  ungroup() %>%
                                    group_by(First.Course, max.terms) %>%
                                      summarise(Course.Mean.Cum.GPA.Slope = mean(Cum.GPA.Slope), Num.Students = n()) %>%
                                        filter(Num.Students >= 8)

Manual.DeltaC.Data <- Filtered.Data4 %>%
                        semi_join(Course.Mean.Cum.GPA.Slopes, by = c("First.Course", "max.terms")) %>%
                          left_join(Course.Mean.Cum.GPA.Slopes, by = c("First.Course","max.terms")) %>%
                            ungroup() %>%
                              group_by(`Student Random ID`) %>%
                                mutate(Expected.Last.Term.Cum.GPA = Course.Mean.Cum.GPA.Slope*max(Graded.Term) + First.Course.GPA) %>%
                                  mutate(DeltaC = Last.Term.Cum.GPA - Expected.Last.Term.Cum.GPA) %>%
                                    dplyr::select(`Student Random ID`, First.Course, First.Prof, DeltaC, everything()) %>%
                                      group_by(`Student Random ID`) %>%
                                        slice(1)

Manual.Experiment.One.Mean.DeltaCs <- Manual.DeltaC.Data %>%
                                        ungroup() %>%
                                          group_by(First.Prof) %>%
                                            summarise(Mean.DeltaC = mean(DeltaC), Sd.DeltaC2 = sd(DeltaC), Num.Students = n()) %>%
                                              filter(Num.Students >= 8) %>%
                                                mutate(standard.error = Sd.DeltaC2/sqrt(Num.Students)) %>%
                                                  mutate(observed.tval = abs(Mean.DeltaC - mean(Mean.DeltaC))/standard.error, requisite.tval = qt(0.95, Num.Students - 1)) %>%
                                                    mutate(Significant = if_else(observed.tval >= requisite.tval, TRUE, FALSE))   

Manual.Experiment.Two.Mean.DeltaCs <- Manual.DeltaC.Data %>%
                                        ungroup() %>%
                                          group_by(First.Prof, First.Course) %>%
                                            summarise(Mean.DeltaC = mean(DeltaC), Sd.DeltaC2 = sd(DeltaC), Num.Students = n()) %>%
                                              filter(Num.Students >= 8) %>%
                                                mutate(standard.error = Sd.DeltaC2/sqrt(Num.Students)) %>%
                                                  group_by(First.Course) %>%
                                                    mutate(observed.tval = abs(Mean.DeltaC - mean(Mean.DeltaC))/standard.error, requisite.tval = qt(0.95, Num.Students - 1)) %>%
                                                      ungroup() %>%
                                                        mutate(Significant = if_else(observed.tval >= requisite.tval, TRUE, FALSE))  
```

```{r, fig.cap = "Barcharts for mean slope of final cumulative GPA per term enrolled by first course code for each student that passed filtration. Mean course slopes calculated manually."}

Course.Mean.Cum.GPA.Slopes2 <-Course.Mean.Cum.GPA.Slopes %>%
                                ungroup() %>%
                                  group_by(First.Course) %>%
                                    mutate(First.Course = factor(First.Course)) %>%
                                      summarise(Overall.Course.Mean.Cum.GPA.Slope = sum(Course.Mean.Cum.GPA.Slope*Num.Students)/sum(Num.Students), Num.Students = sum(Num.Students))

Course.Mean.Cum.GPA.Slopes2 %>%
  ggplot(aes(x = fct_reorder(First.Course, -Overall.Course.Mean.Cum.GPA.Slope), y = Overall.Course.Mean.Cum.GPA.Slope, fill = First.Course)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  guides(fill = guide_legend(ncol = 2)) +
  labs(x = "Course Code", y = "Mean Cumulative GPA Slope (GPA Points per term)", fill = "Course Code")

```

```{r Make LME4 Models}
## Find those courses that affect a students future cumulative GPA slope as a function of term. ERROR SOURCE: LMER CREATES T VALUES UNDER THE NULL HYPOTHESIS THAT MU EQUALS ZERO - SMALL ERROR.

courses <- Filtered.Data1 %>%
            arrange(`Course Code`) %>%
              pull(`Course Code`) %>%
                unique()


## These linear mixed-effects models predict a student's Cumulative GPA in the math and chemistry departments during their stay at OCC depending on the course they took first.
make.course.model <- function(course){
  
  One.Course.Data <- Filtered.Data1 %>%
                      filter(First.Course == course)
  
    if(length(unique(One.Course.Data$`Student Random ID`)) >= 8){
      Cum.course.model <- lmer(Cum.GPA ~ Graded.Term + (1 + Graded.Term|`Student Random ID`), data = One.Course.Data)
      return(summary(Cum.course.model)$coefficients)
  
    } else{return(str_c("Insufficient students started at OCC in ", course, " to create an accurate model (minimum 8, observed ", length(One.Course.Data$`Student Random ID`), ")",   sep = ""))}
  
}

course.summaries <- sapply(courses, make.course.model)


##This function pulls those courses with a significant positive impact on future cumulative gpa
sig.summaries.positive <- function(list){
  
  Sig.Courses.Positive <- rep("Remove", length(list))
  
  for(i in 1:length(list)){
    this.course.code <- courses[i]
    this.course.sample.size <- Filtered.Data1 %>%
                                  filter(`Course Code` == this.course.code) %>%
                                    pull(`Student Random ID`) %>%
                                      unique() %>%
                                        length()
    
    if(!is.null(dim(course.summaries[[i]]))){
    
      intercept.tval <- list[[i]][1,3]
      Graded.Term.tval <- list[[i]][2,3]
      
      if(intercept.tval >= qt(0.95, this.course.sample.size - 1) & Graded.Term.tval >= qt(0.95, this.course.sample.size - 1)){ 
        Sig.Courses.Positive[i] <- names(course.summaries[i])
    }
    }
  }
  return(Sig.Courses.Positive[Sig.Courses.Positive != "Remove"])
}

## And this is a function that extracts those courses that have a significant negative effect on future GPA
sig.summaries.negative <- function(list){
  Sig.Courses.Negative <- rep("Remove", length(list))
  for(i in 1:length(list)){
    this.course.code <- courses[i]
    this.course.sample.size <- Filtered.Data1 %>%
                                pull(`Student Random ID`) %>%
                                  unique() %>%
                                    length()
    
    if(!is.null(dim(list[[i]]))){
    intercept.tval <- course.summaries[[i]][1,3]
    Graded.Term.tval <- course.summaries[[i]][2,3]
      if(intercept.tval >= qt(0.95, this.course.sample.size - 1) & Graded.Term.tval <= qt(0.05, this.course.sample.size - 1)){
        Sig.Courses.Negative[i] <- names(course.summaries[i])
    }
    }
  }
  return(Sig.Courses.Negative[Sig.Courses.Negative != "Remove"])
}


##These vectors contain all the courses that had significant effects on future cumulative GPA.
significant.courses.positive <- sig.summaries.positive(course.summaries)
significant.courses.negative <- sig.summaries.negative(course.summaries)

##These are the courses that did not have enough students begin in them to use the approximate Z test statistic for estimation of future cumulative GPA
not.enough.data <- rep("remove", length(course.summaries))
for(i in 1:length(course.summaries)){
  if(is.null(dim(course.summaries[[i]]))){
    not.enough.data[i] <- names(course.summaries[i])
  }
}
not.enough.data <- not.enough.data[not.enough.data != "remove"]

## Finally, these are the courses that did have enough data to use the approximate Z test statistic but did not significantly influence future cumulative GPA. Students that began in these courses will be compared to the global mean.
`%notin%` <- Negate(`%in%`)
not.significant <- courses[courses %notin% significant.courses.positive & courses %notin% significant.courses.negative & courses %notin% not.enough.data]

## Tibble used to calculate mean number of courses taught by faculty for following paragraph.
Prof.Courses <-  Filtered.Data1 %>%
                    ungroup() %>%
                      group_by(`Faculty Random ID`, `Course Code`) %>%
                        count() %>%
                          dplyr::select(-n) %>%
                            ungroup() %>%
                              group_by(`Faculty Random ID`) %>%
                                summarise(Num.Courses.Taught = n())
```

 

```{r LMER Mean Cumulative GPA Slopes, fig.cap = "Barcharts for mean slope of final cumulative GPA per term enrolled by first course code for each student that passed filtration. Mean course slopes calculated by lme4."}
LMER.Course.Slopes <- tibble(Course.Code = names(course.summaries), slopes = as.numeric(NA))
for(i in seq_along(LMER.Course.Slopes$Course.Code)){
  if(!is.null(dim(course.summaries[[i]]))){
  LMER.Course.Slopes$slopes[i]<- course.summaries[[i]][2,1]
  }
}
LMER.Course.Slopes %>%
  mutate(Course.Code = factor(Course.Code)) %>%
    ggplot(aes(x = fct_reorder(Course.Code, -slopes), y = slopes, fill = Course.Code)) +
    geom_bar(stat = "identity") + 
    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
    guides(fill = guide_legend(ncol = 2)) +
    labs(x = "Course Code", y = "Mean Cumulative GPA Slope (GPA Points per term)", fill = "Course Code")
```



```{r Slope Differences between LMER and Manual models}
Slope.Diffs <- LMER.Course.Slopes %>%
                filter(!is.na(slopes)) %>%
                  right_join(Course.Mean.Cum.GPA.Slopes2, by = c("Course.Code" = "First.Course")) %>%
                    rename(LMER.slope = slopes, Manual.slope = Overall.Course.Mean.Cum.GPA.Slope) %>%
                      mutate(Slope.Diff = abs(Manual.slope - LMER.slope), Course.Code = factor(Course.Code)) %>%
                        dplyr::select(Course.Code, Slope.Diff, Num.Students) %>%
                          arrange(desc(Slope.Diff))
kable(Slope.Diffs, caption = "Absolute differences in mean course slopes between the manual and lmer models", padding = 3)
```


|     Figures $8$ and $9$ show the mean cumulative GPA slopes for each first course that had at least eight students, calculated manually and by lmer() respectively. Table $8$ shows the absolute value of the difference in the mean slopes between figures $6$ and $7$. CHE-2650, MAT-1140, and MAT-1525 were removed from both models because less than eight students began in those courses. (*CITE BOOTSTRAP GUY*) If the two models were calculated identically, each entry of the Slope.Diff column in table $8$ would be zero. One model will end up being better than the other, though we cannot determine which based only on the mean course slopes. Let's see how they do at evaluating the $\Delta C$ statistic, keeping in mind that the mean $\Delta C$ for each course code *should* be zero barring some minor deviations based on the discreteness of cumulative GPA and non-linearity of cumulative GPA. The better method will be the one whose individual course mean $\Delta C$'s deviate the least from zero.

```{r Create LME4 DeltaCs, include = FALSE}
## Create course - specific models of Cum.GPA progression through student's stay at OCC in Filtered.Data using Cum.GPA as a predictor. First, obtain a confidence interval for the effect of Graded.Term on the mean student for superposition onto those courses that had a measurable non-significant deviation from the mean effect on Graded.Term.

##Overall model for students 
Overall.Cum.lmer.model2 <- lmer(Cum.GPA ~ Graded.Term + (1|`Student Random ID`), data = Filtered.Data1)

##First need to create tibble containing the variables (1) Course Code, (2) LMER slope for all courses that had a significant effect. If the course had no significant effect the slope will be the mean slope defined in the previous chunk.
n.courses <- length(unique(Filtered.Data4$`Course Code`))
course.slopes <- tibble(First.Course = unique(Filtered.Data4$`Course Code`), 
                        Course.LMER.slope = rep(0, n.courses))

## Grab lmer slopes for significant courses
set.sig.Course.LMER.Slopes <- function(list){
  
  temp.course.slopes <- course.slopes
  
  for(i in list){
    course.slope <- course.summaries[[i]][2,1]
    temp.course.slopes$Course.LMER.slope[temp.course.slopes$First.Course == i] <- course.slope
  }
  return(temp.course.slopes)
}

course.slopes <- set.sig.Course.LMER.Slopes(significant.courses.positive)
course.slopes <- set.sig.Course.LMER.Slopes(significant.courses.negative)

##Superimpose mean onto those courses that had at least 30 observations, but did not have significant deviation from the mean 

set.mean.Course.LMER.Slopes <- function(list){
  
  temp.course.slopes <- course.slopes
  
  for(i in list){
    course.slope <- summary(Overall.Cum.lmer.model2)$coefficients[2,1]
    temp.course.slopes$Course.LMER.slope[temp.course.slopes$First.Course == i] <- course.slope
  }
  return(temp.course.slopes)
}

course.slopes <- set.mean.Course.LMER.Slopes(not.significant) %>%
                    filter(Course.LMER.slope != 0)



##Join significant courses slopes back onto Filtered.Data  
LMER.Slopes.Data <- Filtered.Data2 %>%
                      semi_join(course.slopes, by = "First.Course") %>%
                        left_join(course.slopes, by = "First.Course")

## Determine difference between expected and observed final cumulative GPA in DeltaC, and rejoin into Filtered.Data
LMER.DeltaC.Data <- LMER.Slopes.Data %>% 
                      ungroup() %>%
                        group_by(`Student Random ID`) %>%
                          slice(n()) %>%
                            ungroup() %>%
                              mutate(LMER.Predict = First.Course.GPA + Course.LMER.slope*Graded.Term) %>%
                                mutate(DeltaC = Cum.GPA - LMER.Predict) %>%
                                  dplyr::select(`Student Random ID`, DeltaC) %>%
                                    right_join(LMER.Slopes.Data, by = c("Student Random ID"))
```


```{r Initialize Course Heights, include = FALSE}
##Course Heights for plots
Course.Heights <- tibble(`Course Code` = filter(LMER.Course.Slopes, !is.na(slopes))$Course.Code) %>%
                    filter(`Course Code` != "CHE-0950") %>%
                      arrange(`Course Code`) %>%
                        cbind(tibble(Height = seq(-1, 1, length.out = length(filter(LMER.Course.Slopes, !is.na(slopes))$Course.Code)-1)))
```

```{r Initialize Comparison Data, include = FALSE}
Manual.Comparison.Data <- Manual.DeltaC.Data %>%
                            dplyr::select(`Student Random ID`, DeltaC, First.Course, First.Prof) %>%
                              rename(Manual.DeltaC = DeltaC)

LMER.Comparison.Data <- LMER.DeltaC.Data %>%
                          group_by(`Student Random ID`) %>%
                            slice(1) %>%
                              ungroup() %>%
                                dplyr::select(`Student Random ID`, DeltaC, First.Course, First.Prof) %>%
                                  rename(LMER.DeltaC = DeltaC)
  
Comparison.Data <- full_join(Manual.Comparison.Data, LMER.Comparison.Data, by = c("Student Random ID", "First.Course", "First.Prof")) %>%
                    mutate(Delta.DeltaC = Manual.DeltaC - LMER.DeltaC) %>%
                      dplyr::select(`Student Random ID`, Delta.DeltaC, Manual.DeltaC, LMER.DeltaC, First.Prof, First.Course)

##Course Mean DeltaC Values for all students that passed filtration assumptions using LMER models
Course.DeltaC.Means <- Manual.Comparison.Data %>%
                        ungroup() %>%
                          group_by(First.Course) %>%
                            summarise(Course.Mean.DeltaC = mean(Manual.DeltaC), Course.Sd.DeltaC = sd(Manual.DeltaC))
```


```{r LMER Course Mean DeltaCs, fig.cap = "Barchart of course mean Delta C's as generated by lmer models"}
##Bar Chart of Course Mean Delta C's for LMER Models
LMER.DeltaC.Data %>%
  filter(First.Course != "CHE-0950") %>%
    mutate(First.Course = as.factor(First.Course)) %>%
      ungroup() %>%
        group_by(First.Course) %>%
          summarise(Course.Mean.DeltaC = mean(DeltaC)) %>%
             ggplot(aes(x = fct_reorder(First.Course, -Course.Mean.DeltaC), y = Course.Mean.DeltaC)) +
              theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
              geom_bar(aes(fill = First.Course), stat = "identity") +
              guides(fill = guide_legend(ncol = 2)) +
              labs(x = "Course Code", y = "Course Mean \u0394C", fill = "Course Code")
```

```{r Manual Course Mean DeltaCs, fig.cap = "Barchart of course mean Delta C's as generated by manual models"}
##Bar Chart of Course Mean Delta C's for LMER Models
Manual.DeltaC.Data %>%
  mutate(First.Course = as.factor(First.Course)) %>%
    ungroup() %>%
      group_by(First.Course) %>%
        summarise(Course.Mean.DeltaC = mean(DeltaC)) %>%
           ggplot(aes(x = fct_reorder(First.Course, -Course.Mean.DeltaC), y = Course.Mean.DeltaC)) +
            theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
            geom_bar(aes(fill = First.Course), stat = "identity") +
            guides(fill = guide_legend(ncol = 2)) +
            labs(x = "Course Code", y = "Course Mean \u0394C", fill = "Course Code")
```

|     Figures $10$ and $11$ show the mean $\Delta C$ for each course as predicted by the lmer() and manual linear mixed-effects models.   The distribution of the lmer() generated $\Delta C$'s clearly have wide variance for each course, signifying that the wrong statistic is being calculated. Specifically, the range of mean $\Delta C$'s over all courses is approximately $[-0.4, 0.3]$. It seems that simply supplying lmer with a random intercept called Student Random ID that has a slope and intercept interaction with the current term number is insufficient in the generation of an unbiased $\Delta C$. Removing the max.terms stratification results in the manual models creating similarly biased $\Delta C$'s, so perhaps lmer() is not properly recognizing how maximum terms affects slope and instead calculated the change in slope on a term-by-term basis. My bet is the maximum terms slope dependency was automatically captured as part of the student-to-student random deviation, but applied as a random intercept rather than a random slope. Ultimately, lmer did not work. In contrast, the mean $\Delta C$ for each course generated by the manual models compactly surround zero. The range of course mean $\Delta C$'s in the manual models was approximately $[-0.015, 0.01]$. The manual models were better by an order of magnitude, so we will use them exclusively going forwards. 

```{r Manual Experiment One Mean DeltaCs are Normal, fig.cap = "Normal probability plot of all manual professor mean DeltaC's from experiment one."}
##Normal probability plot for All mean DeltaC using Manual experiment one
Manual.Experiment.One.Mean.DeltaCs %>%
  pull(Mean.DeltaC) %>%
   qqnorm(main = NULL)
```

```{r Manual Experiment Two Mean DeltaCs are Normal, fig.cap = "Normal probability plot of all manual professor mean Delta C's from experiment two."}
##Normal probability plot for ALL mean DeltaC using Manual experiment two
Manual.Experiment.Two.Mean.DeltaCs %>%
  pull(Mean.DeltaC) %>%
   qqnorm(main = NULL)
```

|     Figures $12$ and $13$ depict normal probability plots using every student's $\Delta C$ calculated in experiments one and two respectively. A perfectly normal distribution would be a straight diagonal line, indicating a one-to-one correspondence between the theoretical normal quantiles and the observed sample quantiles. There is a small amount of curvature on the tails of the experimental distribution, but this is expected from sampling error and the fact that $\Delta C$ is a discrete variable bounded on the set $[-4,4]$. Because the population distribution of $\Delta C$ is normal for both experiments we are sanctioned for usage of the one sample t-test. Moreover, because each stratum has at least eight students, we can also use the bootstrap principal for significance testing. These procedures are carried out in accordance with equations ($\ref{T Hypotheses}$) and ($\ref{Bootstrap Hypotheses}$). For visualization of how the bootstrap confidence interval is employed for significance testing, I have included figures $14$ and $15$.

```{r DeltaC Bootstrap Functions, include = FALSE}
##Bootstrap for each faculty member that taught students that passed filtration assumptions. This chunk creates the necessary functions.
Boot.Data <- Manual.DeltaC.Data %>%
              ungroup() %>%
                group_by(`Student Random ID`) %>%
                  slice(n()) %>%
                    dplyr::select(`Student Random ID`, DeltaC, First.Prof, First.Course)

##Initialize list of faculty that passed filtration conditions
Filtered.Profs <- unique(Boot.Data$First.Prof)

##Record mean DeltaC over all students that passed filtration conditions. Also create boot.data 
global.mean <- mean(Boot.Data$DeltaC)

##Create bootstrapper function which creates a bootstrap distribution for a given faculty member either overall or by course code
make.cum.bootstrap.means <- function(prof.num, course.code = NA) {
  if(is.na(course.code)){
    og.sample <- Boot.Data$DeltaC[Boot.Data$First.Prof == prof.num]
  } else{
    og.sample <- Boot.Data$DeltaC[Boot.Data$First.Prof == prof.num & Boot.Data$First.Course == course.code]
  }
  samp.size <- length(og.sample)
  boot.means <- rep(0,10^4)
    for(i in seq_along(boot.means)){
      boot.sample <- sample(og.sample, replace = TRUE, size = samp.size)
      boot.means[i] <- mean(boot.sample)
    }
  boot.means <- tibble(DeltaC.Boot.Means = boot.means)
  return(boot.means)
}

##Create observed mean grabber for both overall and course-specific bootstraps
get.observed.mean <- function(course.code = NA){
  if(is.na(course.code)){
  observed.DeltaCs <- Boot.Data %>%
                          pull(DeltaC)
  } else{
    observed.DeltaCs <- Boot.Data %>%
                        filter(First.Course == course.code) %>%
                          pull(DeltaC)
  }
  return(mean(observed.DeltaCs))
}
```

```{r Evaluate bootstrap significance, echo = FALSE}
##Make significance dataframes for general bootstraps. The column Significant 2 notes that this significance test may create different results than the one sample t-test. We will use \alpha = 0.05 as we did for the t-test. This  chunk runs slowly due to the fact that it needs to run 10^4 * 10^3 = ten million simulations for accuracy. Increasing runtime will decrease error slightly, but there are exponentially diminishing returns. We need to restrict our bootstraps to at least 8 distinct students so that there is sufficiently rich data. CITATION https://stats.stackexchange.com/questions/33300/determining-sample-size-necessary-for-bootstrap-method-proposed-method

##This is for a faculty's overall mean DeltaC's
Sig.Profs <- Manual.DeltaC.Data %>% 
                ungroup() %>%
                  group_by(`Student Random ID`) %>%
                    slice(1) %>%
                      ungroup() %>%
                        group_by(First.Prof) %>%
                          summarise(Mean.DeltaC = mean(DeltaC), Num.Students = n(), SD.DeltaC = sd(DeltaC)) %>%
                            filter(Num.Students >= 8)

Sig.Profs <- Sig.Profs %>%
                cbind(Significant2 = rep(NA, length(Sig.Profs$First.Prof)))

##This is for the course-specific bootstraps.
Sig.Courses <- Manual.DeltaC.Data %>% 
                  ungroup() %>%
                    group_by(`Student Random ID`) %>%
                      slice(1) %>%
                        ungroup() %>%
                          group_by(First.Prof, First.Course) %>%
                            summarise(Mean.DeltaC = mean(DeltaC), Num.Students = n(), SD.DeltaC = sd(DeltaC)) %>%
                              filter(Num.Students >= 8)

Sig.Courses <- Sig.Courses %>%
                cbind(Significant3 = rep(NA, length(Sig.Courses$First.Prof)))

##Create significance evaluation function to be applied against all faculty in Sig.Profs / Sig.Courses. The output will be a significance observation for one professor.
evaluate.significance <- function(prof.num, course.code = NA){
  current.significance <- NA
  if(is.na(course.code)){
  current.bootstrap.distribution <- make.cum.bootstrap.means(prof.num) %>% 
                                      pull(DeltaC.Boot.Means)
  } else{
    current.bootstrap.distribution <- make.cum.bootstrap.means(prof.num, course.code) %>% 
                                        pull(DeltaC.Boot.Means)
  }

  lower.quantile <- quantile(current.bootstrap.distribution, 0.025)
  upper.quantile <- quantile(current.bootstrap.distribution, 0.975)
  benchmark <- get.observed.mean(course.code)
  
    if(upper.quantile <= benchmark){
      current.significance <- TRUE
    } else if(lower.quantile >= benchmark){
      current.significance <- TRUE
    } else{
      current.significance <- FALSE
    }
  return(current.significance)
}

##Neither sapply nor apply wanted to work so I just manually apply the functions here. This fills out the Significant2 column in Sig.Profs, and the Significant3 column in Sig.Courses.
for(i in seq_along(Sig.Profs$First.Prof)){
  Sig.Profs$Significant2[i] <- evaluate.significance(Sig.Profs$First.Prof[i])  
}

for(i in seq_along(Sig.Courses$First.Prof)){
  Sig.Courses$Significant3[i] <- evaluate.significance(Sig.Courses$First.Prof[i], Sig.Courses$First.Course[i])
}

##Make two test cases for visualization - one where the faculty has a significant mean delta and one where they do not.
sig.test.prof <- Filtered.Profs[1]
nonsig.test.prof <- Filtered.Profs[2]
```

```{r Significant Bootstrap Example Plot, fig.cap = "Example histogram of a bootstrap confidence interval that indicates a rejection of the null hypothesis. Simulated using DeltaC among students first placed with faculty number 643249"}
##Create visualization of a faculty member with a mean DeltaC significantly different from 0. This is bootstrap of all taught courses
sig.test.data <- make.cum.bootstrap.means(sig.test.prof)
sig.test.data %>%
  ggplot() +
  geom_histogram(aes(x = DeltaC.Boot.Means)) +
  geom_vline(xintercept = c(quantile(sig.test.data$DeltaC.Boot.Means, 0.025), quantile(sig.test.data$DeltaC.Boot.Means, 0.975), global.mean), color = c("red", "red", "blue")) +
  labs(x = "Bootstrap Sample Mean \u0394C", y = "Count")
```

```{r Non-Significant Bootstrap Example Plot, fig.cap = "Example histogram of a bootstrap confidence interval that fails to reject the null hypothesis."}
##Create visualization of a faculty member with a mean DeltaC NOT significantly different from 0.
nonsig.test.data <- make.cum.bootstrap.means(nonsig.test.prof)
nonsig.test.data %>%
  ggplot() +
  geom_histogram(aes(x = DeltaC.Boot.Means)) +
  geom_vline(xintercept = c(quantile(nonsig.test.data$DeltaC.Boot.Means, 0.025), quantile(nonsig.test.data$DeltaC.Boot.Means, 0.975), global.mean), color = c("red", "red", "blue")) +
  labs(x = "Bootstrap Sample Mean \u0394C", y = "Count")
```



```{r Make Experiment One and Experiment Two Statistics, include = FALSE}
##Analyse the differences between the bootstraps and t-test results. 

##Experiment one
##Merge bootstrap significance to significance found by t-test for comparison.
Double.Profs <- Manual.Experiment.One.Mean.DeltaCs %>%
                  dplyr::select(First.Prof, Significant, Mean.DeltaC, Num.Students) %>%
                    full_join(Sig.Profs, by = c("First.Prof", "Num.Students", "Mean.DeltaC")) %>%
                      mutate(Sig.Agree = if_else(Significant == Significant2, TRUE, FALSE)) %>%
                        rename(T.Test.Sig = Significant, Bootstrap.Sig = Significant2)


##Determine total proportion of agreements on significance between the t-test and the bootstrap. 
total.agree1 <- Double.Profs %>% 
                filter(Sig.Agree == TRUE) %>%
                  pull(Sig.Agree) %>%
                    sum()

total.cases1 <- length(Double.Profs$Sig.Agree)

prop.agree1 <- total.agree1/total.cases1


##Determine the mean and standard deviation of DeltaC for the faculty that obtained the same significance from the t-test and the bootstrap (POSITIVE DeltaC)
positive.mean1 <- Double.Profs %>%
                  filter(Sig.Agree == TRUE, Bootstrap.Sig == TRUE,  Mean.DeltaC > global.mean) %>%
                    pull(Mean.DeltaC) %>%
                      mean()

positive.sd1 <- Double.Profs %>%
                  filter(Sig.Agree == TRUE, Bootstrap.Sig == TRUE,  Mean.DeltaC > global.mean) %>%
                    pull(Mean.DeltaC) %>%
                      sd()

##Determine the mean of DeltaC for the faculty that obtained the same significance from the t-test and the bootstrap (NEGATIVE DeltaC)
negative.mean1 <- Double.Profs %>%
                  filter(Sig.Agree == TRUE,  Bootstrap.Sig == TRUE, Mean.DeltaC < global.mean) %>%
                    pull(Mean.DeltaC) %>%
                      mean()

negative.sd1 <- Double.Profs %>%
                  filter(Sig.Agree == TRUE,  Bootstrap.Sig == TRUE, Mean.DeltaC < global.mean) %>%
                    pull(Mean.DeltaC) %>%
                      sd()

##Experiment Two
##Compare Bootstrap to T-Test
Double.Courses <- Sig.Courses %>%
                    dplyr::select(First.Prof, First.Course, Mean.DeltaC, Significant3, Num.Students) %>%
                      full_join(dplyr::select(Manual.Experiment.Two.Mean.DeltaCs, First.Prof, First.Course, Mean.DeltaC, Significant, Num.Students)) %>%
                        rename(T.Sig = Significant, Bootstrap.Sig = Significant3) %>%
                          mutate(T.Test.Only.Reject = if_else(T.Sig == FALSE & Bootstrap.Sig == TRUE, TRUE, FALSE), Bootstrap.Only.Reject = if_else(T.Sig == TRUE & Bootstrap.Sig == FALSE, TRUE, FALSE), Sig.Agree = if_else(T.Sig == FALSE & Bootstrap.Sig == FALSE | T.Sig == TRUE & Bootstrap.Sig == TRUE, TRUE, FALSE)) %>%
                            left_join(Course.DeltaC.Means)

##Determine total proportion of agreements on significance between the t-test and the bootstrap. 
total.agree2 <- Double.Courses %>% 
                  filter(Sig.Agree == TRUE) %>%
                    pull(Sig.Agree) %>%
                      sum()

total.cases2 <- length(Double.Courses$Sig.Agree)

prop.agree2 <- total.agree2/total.cases2


##Determine the mean and standard deviation of DeltaC for the faculty that obtained the same significance from the t-test and the bootstrap (POSITIVE DeltaC)
positive.mean2 <- Double.Courses %>%
                  filter(Sig.Agree == TRUE,  Bootstrap.Sig == TRUE, Mean.DeltaC > Course.Mean.DeltaC) %>%
                    pull(Mean.DeltaC) %>%
                      mean()

positive.sd2 <- Double.Courses %>%
                  filter(Sig.Agree == TRUE,  Bootstrap.Sig == TRUE, Mean.DeltaC > Course.Mean.DeltaC) %>%
                    pull(Mean.DeltaC) %>%
                      sd()

##Determine the mean of DeltaC for the faculty that obtained the same significance from the t-test and the bootstrap (NEGATIVE DeltaC)
negative.mean2 <- Double.Courses %>%
                  filter(Sig.Agree == TRUE,  Bootstrap.Sig == TRUE, Mean.DeltaC < Course.Mean.DeltaC) %>%
                    pull(Mean.DeltaC) %>%
                      mean()

negative.sd2 <- Double.Courses %>%
                  filter(Sig.Agree == TRUE,  Bootstrap.Sig == TRUE, Mean.DeltaC < Course.Mean.DeltaC) %>%
                    pull(Mean.DeltaC) %>%
                      sd()

overall.mean.DeltaC <- Manual.DeltaC.Data %>%
                        pull(DeltaC) %>%
                          mean()

overall.sd.DeltaC <- Manual.DeltaC.Data %>%
                      pull(DeltaC) %>%
                        mean()

##statistics on the total number of students per faculty for experiment one
mean.n1 <-  Double.Profs %>% 
              ungroup() %>% 
                pull(Num.Students) %>% 
                 mean()

sd.n1 <- Double.Profs %>% 
          ungroup() %>% 
            pull(Num.Students) %>% 
              sd()

##statistics on the total number of students per faculty for experiment two
mean.n2 <-  Double.Courses %>% 
              ungroup() %>% 
                pull(Num.Students) %>% 
                 mean()

sd.n2 <- Double.Courses %>% 
          ungroup() %>% 
            pull(Num.Students) %>% 
              sd()
```

|     The overall population mean for $\Delta C$ in the manual models is `r overall.mean.DeltaC` with a standard deviation `r `, which means that either: 1) there is at least one confounding variable that affects a student's cumulative GPA unaccounted for, or that 2) the term dependency of a student's cumulative GPA is not quite linear. I am personally more inclined to believe the latter because GPA is bounded on the interval $[0,4]$ and thus cannot perfectly follow any affine path over an arbitrary number of semesters (unless the slope were exactly zero). That said, for the mean student taking the mean number of terms the manual models are pretty good. Now that we know the overall mean value for $\Delta C$, we can use the one sample t-test and bootstrap confidence intervals with $\alpha = 0.05$ to determine whether a given faculty member had a mean positive or negative on their students final cumulative GPA's. I want to be clear: the $\Delta C$ statistic I've created is not immune to exceptionally harsh/lenient grading by the first professor, because it is measured assuming that the first observed GPA value was an accurate intercept for the linear mixed effects model. We assume that course grading is standardized in OCC's math and chemistry departments. 

\subsection{Delta C Experiment One Results}

Using the confidence intervals outlined in equations ($\ref{T Confidence Interval}$) and ($\ref{Bootstrap Confidence Interval}$), significance calculations are carried out on the manual model.

```{r Experiment One Sufficient Sample Size}
##Number of Faculty represented in below histogram compared to total
Experiment.One.Sufficient.Sample.Size <- Filtered.Data4 %>%
                                          ungroup() %>%
                                            group_by(`Student Random ID`) %>%
                                              slice(1) %>%
                                                ungroup() %>%
                                                  group_by(First.Prof) %>%
                                                    summarise(Num.Students = n()) %>%
                                                      mutate(Sufficient.Sample.Size = if_else(Num.Students >= 8, TRUE, FALSE)) %>%
                                                        ungroup() %>%
                                                          group_by(Sufficient.Sample.Size) %>%
                                                            count()
kable(Experiment.One.Sufficient.Sample.Size, caption = "Number of faculty members that had at least eight students start with them in experiment one. We can run significance tests on the 233 with at least eight.", padding = 3)
```


```{r DeltaC.Sig.Diff1}
##Table of differences between t-test significance and overall bootstrap significance - experiment one.
DeltaC.Sig.Diff1 <- Double.Profs %>%
                      mutate(T.Test.Only.Reject = if_else(T.Test.Sig == FALSE & Bootstrap.Sig == TRUE, TRUE, FALSE), Bootstrap.Only.Reject = if_else(Bootstrap.Sig == FALSE & T.Test.Sig == TRUE, TRUE, FALSE)) %>%
                        group_by(T.Test.Only.Reject, Bootstrap.Only.Reject) %>%
                          count()

kable(DeltaC.Sig.Diff1, caption = "Discrepencies found in significance tests for experiment one. Out of 233 faculty, the one sample t-test and the bootstrap confidence interval agreed regarding whether a faculty's mean DeltaC was significantly different than the overall mean 224 times, and disagreed 9 times.", padding = 3)
```

|      Table 9 shows that out of the $260$ faculty that survived filtration, $233$ had at least eight students start in one of their courses. In experiment one we can run significance tests on $233$ faculty. Table 10 summarizes the differences in experiment one between the overall one sample t-test and bootstrap significance measures for all faculty that had at least eight students begin in their courses and pass filtration. In the vast majority of cases, the bootstrap test agrees with the t-test. The significance implied by the t-test and the bootstrap agreed for `r total.agree1` out of `r total.cases1` faculty members, which is approximately  `r prop.agree1 *100`$\%$ of all professors.  $9$ discrepancies occurred where the t-test implied that the observation of mean $\Delta C$ for a faculty member was significantly different from the overall  mean, while the bootstrap implied that it was not. There were no observed cases where the t-test rejected significance while the bootstrap failed to reject. We would expect more bootstrap rejections than t-test rejections if the sample data were not sufficiently rich due to the sample sizes being too small. The requirement that a faculty have a minimum of eight students start in their courses helps with the richness issue, but the high variance in $\Delta C$ between students is something to be mindful of when considering the results.

|     Here are some relevant statistics regarding experiment one. The average mean $\Delta C$ for those faculty who were doubly-significant and had positive effect was `r positive.mean1` with standard deviation `r positive.sd1`. As for the faculty doubly-significant negative mean $\Delta C$ values, the average mean $\Delta C$ was `r negative.mean1` with a standard deviation of `r negative.sd1`. The sample size (total number of students that passed filtration and started with a professor) had mean `r mean.n1` and standard deviation `r sd.n1`. The distribution of the sample size is therefore clearly not Gaussian. The experimental error rate for experiment one - where such an error is measured by the bootstrap and t-test disagreeing on the statistical significance of a faculty - was $\frac{9}{233}*100\% = 3.86\%$. This is a bit high in terms of data loss as we lose ratings on nine faculty members, though it's certainly better safe than sorry. The overall accuracy of the experiment is increased because we eliminated nine potentially invalid results. 

|     In an effort to be as conservative as possible with inference, we will now create plots using only those faculty members endorsed by both confidence intervals. Additionally, I will include a plot of sample size and a plot showing the relationship between the number of students that started with a faculty and the mean $\Delta C$. 

```{r Make Double.Profs.Data}
##Recreate graphs for doubly-significant faculty: Experiment One

##Determine which faculty members had doubly significant effect onto their students.
Doubly.Sig.Profs <- Double.Profs %>%
                      filter(Sig.Agree == TRUE, Bootstrap.Sig == TRUE) %>%
                        dplyr::select(First.Prof)

##Rejoin doubly significant faculty. It is sufficient to just filter the above three lines without the select command, but I wanted to call Manual.Experiment.One.Mean.DeltaCs for continuity.
Double.Profs.Data <- Manual.Experiment.One.Mean.DeltaCs %>%
                      ungroup() %>%
                        semi_join(Doubly.Sig.Profs, by = "First.Prof") %>%
                          dplyr::select(First.Prof, Mean.DeltaC, Num.Students)
```


```{r Full Experiment One Histogram, fig.cap = "Histogram of all experiment one mean Delta Cs, colored by effect polarity from the mean."}
##Histogram of doubly-significant mean professor effects onto final cumulative GPA for students that obeyed filtration assumptions.
Manual.Experiment.One.Mean.DeltaCs %>%
  mutate(Hist.Colors = factor(if_else(Mean.DeltaC < mean(pull(Double.Profs.Data, Mean.DeltaC)), "green", "red"))) %>%
      ggplot() +
      geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 22) +
      labs(x = "Doubly-Significant Mean \u0394C (GPA Points)", y = "Count", title = "") +
      theme(legend.position = "none")
```

```{r Doubly Significant Experiment One Histogram, fig.cap = "Histogram of only those experiment one mean Delta Cs that were significantly different from the global mean, colored by effect polarity from the global mean."}
##Histogram of doubly-significant mean professor effects onto final cumulative GPA for students that obeyed filtration assumptions.
Double.Profs.Data %>%
  mutate(Hist.Colors = factor(if_else(Mean.DeltaC < mean(pull(Double.Profs.Data, Mean.DeltaC)), "green", "red"))) %>%
      ggplot() +
      geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 22) +
      labs(x = "Doubly-Significant Mean \u0394C (GPA Points)", y = "Count") +
      theme(legend.position = "none")
```


|     The overall distribution of mean $\Delta C$ is shown in figure $16$ to be approximately normal. After removing all the faculty that had effects indistinguishable from zero, we are left with the bimodal distribution in figure $17$. I want to be clear: even though the faculty removed did not have an effect on their students *significantly different from the mean effect*, that does not imply that they did not have zero effect. The final mean cumulative GPA slopes for the students of these removed professors are well-represented by figure $8$. A word of warning - just because the students placed first with a specific faculty happened to significantly underperform or overperform against the manual model does not necessitate a true effect. It is important to consider the result in context - did a faculty have a mean $\Delta C$ of $0.5$ among twelve students or among twelve hundred? Both cases are represented equivalently in figures $16$ and $17$, though one is much more certain and impactful than the other. Figure $18$ is intended to provide such context.



```{r Experiment One DeltaC Context Plot, fig.cap = "Scatterplot of each doubly significant faculty's mean Delta C against the number of students that passed filtration and started with them. Colored by effect polarity from the mean. Boxplot provided for summary of sample size distribution."}
##Experimental Plot: Mean.DeltaC Vs. Number of Students. Do good/bad teachers teach more students?
##Determine the total number of students taught by each faculty
Overall.Num.Students <- Gen_Data8 %>%
                          ungroup() %>%
                            group_by(`Faculty Random ID`) %>%
                              count() %>%
                                rename(Overall.Students = n)

##Join total number of students taught to data frame containing only those faculty that had significant Mean DeltaC
DeltaC.Context.Plot1 <- Double.Profs %>%
                          filter(Sig.Agree == TRUE, T.Test.Sig == TRUE ) %>%
                            mutate(Point.Colors = if_else(Mean.DeltaC < mean(Double.Profs$Mean.DeltaC), "green", "red")) %>%
                              dplyr::select(First.Prof, Mean.DeltaC, Num.Students, Point.Colors) %>%
                                left_join(Overall.Num.Students, by = c("First.Prof" = "Faculty Random ID")) %>%
                                  ggplot(aes(x = Num.Students)) +
                                  geom_point(aes(color = Point.Colors, y = Mean.DeltaC)) +
                                  geom_boxplot(color = "red", fill = "green", width = 0.1) +
                                  theme(legend.position = "none") +
                                  labs(x = "Number of Students that Passed Filtration and Started with Faculty", y = "Doubly-Significant Mean \u0394C (GPA Points)")

##Print the context plot for DeltaC Experiment One 
DeltaC.Context.Plot1
```

|     As the number of students trends towards zero, figure $18$ indicates that mean $\Delta C$ becomes more volatile. This volatility seems to evaporate past the $0.5$ quantile of sample size, stabilizing to a rough local average of $\pm 0.25$ for each polarity.  Additionally, the mean $\Delta C$ tends towards zero as the number of students grows. I can think of two potential causes for this trend. In the order of likelihood:

1. The statistic called mean $\Delta C$ is highly susceptible random variance. The approximate CLT given by equation ($\ref{Small N CLT}$) indicates that as the number of samples grows, the standard error decreases as we close in on the true population mean. As the number of students per doubly significant faculty grows, the range of mean $\Delta C$'s decreases, indicating that the range was not so big to begin with. This uses the assumption that faculty that have taught many students have the same distribution of population mean $\Delta C$'s as faculty that have not taught as many students.

2. There is a fundamental difference between faculty that have taught many students and those that have not. In particular, the wide variance at low sample size could be seen as "good" low-student-count faculty having radical effective ideas and "bad" low-student-count faculty not knowing how to teach. This option is far less likely when considering the high-student-count behavior of the distribution. Any professor that teaches several courses is bound to learn what is effective and how best to disperse knowledge in a community college setting. This would indicate a universal increase in mean $\Delta C$. While we observe an average increase in the mean $\Delta C$ for faculty with doubly-significant negative effects, we observe a *decrease* in the average mean $\Delta C$ for faculty with doubly-significant positive effects.
Figure $18$ shows that while a faculty's mean $\Delta C$ may a good indicator of a real effect for higher sample sizes, it must be taken with a healthy dose of trepidation for low sample sizes as large $\Delta C$'s are likely illusory.

\subsection{Delta C Experiment Two Results}
```{r DeltaC Experiment Two Sufficient Sample Size}
##Number of Faculty-Courses represented in below histogram
Experiment.Two.Sufficient.Sample.Size <- Filtered.Data4 %>%
                                          ungroup() %>%
                                            group_by(`Student Random ID`) %>%
                                              slice(1) %>%
                                                ungroup() %>%
                                                  group_by(First.Course, First.Prof) %>%
                                                    summarise(Num.Students = n()) %>%
                                                      mutate(Sufficient.Sample.Size = if_else(Num.Students >= 8, TRUE, FALSE)) %>%
                                                        ungroup() %>%
                                                          group_by(Sufficient.Sample.Size) %>%
                                                            count()

kable(Experiment.Two.Sufficient.Sample.Size, caption = "Number of faculty-course combinations that had at least eight students start with them in experiment two. We can run significance tests on the 607 with at least eight.", padding = 3)
```

```{r DeltaC.Sig.Diff2}
##Table of differences between t-test and bootstrap for course-specific experiment two.
DeltaC.Sig.Diff2 <- Double.Courses %>%
                      group_by(T.Test.Only.Reject, Bootstrap.Only.Reject) %>%
                        count()

kable(DeltaC.Sig.Diff2, caption = "Discrepencies found in significance tests for experiment two. Out of 605 faculty-course combinations, the one sample t-test and the bootstrap confidence interval agreed regarding whether a faculty's mean DeltaC was significantly different than the course mean 560 times, and disagreed 45 times.", padding = 3)
```

|     In experiment two, we increase precision to the point of individual courses. Whereas in experiment one $\Delta C$ was averaged across all the students that passed filtration and started with a specific teacher (as long as at least eight students did), in experiment two the average was restricted across students that started with a faculty $\textit{in a course}$ (as long as at least eight students did). The sample size minimum caused many more omissions in experiment two than in experiment one. Out of $989$ faculty-course combinations, only $607$ had at least eight students start in them. Two of these faculty-course combinations were from the omitted courses, so we are left with $605$ significance tests to run.

|     Table $12$ shows a larger significance correction than in experiment one. The experimental error rate for experiment two was $\frac{37+8}{37+8+560}*100\% = 7.44\%$. This higher rejection rate is predictable because the sample size must necessarily go down when we choose not to average over all a faculty's courses - as long as the faculty taught at least two courses in which students started their path through OCC's math and chemistry departments. The mean sample size in experiment two was only `r  mean.n2` with standard deviation `r sd.n2`. Because the ratio of mean to standard deviation decreased from experiments one to two, we predict the distribution of experiment two's sample sizes to be even more skewed than that of experiment one.

|     The average mean $\Delta C$ for those faculty-course combinations that were doubly-significant and had positive effect was `r positive.mean2` with standard deviation `r positive.sd2`. For the faculty-course combinations with doubly-significant negative mean $\Delta C$ values, the average mean $\Delta C$ was `r negative.mean2` with standard deviation `r negative.sd2`. The sample size had mean `r mean.n2` and standard deviation `r sd.n2`. The distribution of sample size is therefore even more skewed than in experiment one.

```{r Full Experiment Two Histogram, fig.cap = "Histogram of all experiment two mean Delta Cs, colored by effect polarity from the mean."}
##Recreate graphs for doubly-significant faculty: Experiment Two

##Histogram of Distribution of ALL Mean Delta C's
Double.Courses %>%
  mutate(Hist.Colors = factor(if_else(Mean.DeltaC < Course.Mean.DeltaC, "green", "red"))) %>%
    ggplot() +
    geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 21) +
    labs(x = "Course's Doubly-Significant Mean \u0394C (GPA Points)", y = "Count") +
    theme(legend.position = "none")
```

```{r Doubly Significant Experiment Two Histogram, fig.cap = "Histogram of only those experiment two mean Delta Cs that were significantly different from the course mean, colored by effect polarity from the course mean."}
##Histogram of Distribution of Doubly-Significant Mean Delta C's
Double.Courses %>%
  filter(Sig.Agree == TRUE, T.Sig == TRUE) %>%
    mutate(Hist.Colors = factor(if_else(Mean.DeltaC < Course.Mean.DeltaC, "green", "red"))) %>%
      ggplot() +
      geom_histogram(aes(x = Mean.DeltaC, fill= Hist.Colors), bins = 21) +
      labs(x = "Course's Doubly-Significant Mean \u0394C (GPA Points)", y = "Count") +
      theme(legend.position = "none")
```

|     The distribution of all experiment two mean $\Delta C$'s in figure $19$ is not noticeably different than the distribution for experiment one observed in figure $16$.  Figures $20$ and $17$ are likewise similar. Experiment two includes information regarding the course to which an observation of mean $\Delta C$ belongs. We will generate the same type of context plot as we did for experiment one, but first let's separate the distribution of mean $\Delta C$ into each possible first course.

```{r All Experiment Two Mean DeltaCs by course, fig.cap = "Scatterplot of all mean Delta C's for experiment two"}
##Scatterplot of all course specific mean DeltaC's
Double.Courses %>%
  left_join(Course.Heights, by = c("First.Course" = "Course Code")) %>%
    ggplot(aes(x = Mean.DeltaC, y = Height)) +
    geom_point(aes(color = First.Course, size = Num.Students), position = position_jitter(height = 0.015)) +
    scale_x_continuous(limits = c(-1, 1), breaks = seq(from = -1, to = 1, by = .5)) +
    theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
    guides(color = guide_legend(ncol = 3), size = guide_legend(ncol = 2)) +
    labs(x = "Course's Doubly-Significant Mean \u0394C (GPA Points)", y = "Course", color = "Course Code", size = "Number of Students")
```

```{r Doubly-Significant Experiment Two Mean DeltaCs by course, fig.cap = "Scatterplot of doubly-significant mean Delta C's for experiment two"}
##Scatterplot of doubly-significant course specific mean DeltaC's
Double.Courses %>%
  filter(T.Sig == TRUE, T.Sig == TRUE) %>%
    left_join(Course.Heights, by = c("First.Course" = "Course Code")) %>%
      ggplot(aes(x = Mean.DeltaC, y = Height)) +
      geom_point(aes(color = First.Course, size = Num.Students), position = position_jitter(height = 0.01)) +
      scale_x_continuous(limits = c(-1, 1), breaks = seq(from = -1, to = 1, by = .5)) +
      theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
      guides(color = guide_legend(ncol = 3), size = guide_legend(ncol = 2)) +
      labs(x = "Course's Doubly-Significant Mean \u0394C (GPA Points)", y = "Course", color = "Course Code", size = "Number of Students")
```

|      Figures $21$ shows the distribution of mean $\Delta C$ for all faculty-course combinations, while figure $22$ only depicts doubly significant faculty-course combinations. There are few faculty that had at least eight people start in their high level chemistry courses, indicated by the few dots in the CHE-2610 stratum of figure $21$ and the altogether absence of CHE-2620 in either figure. Courses disappearing from the legends between figures $21$ and $22$ indicate that there were no faculty with doubly-significant mean $\Delta C$'s in that course. Specifically, while CHE-2610 and MAT-1070 had faculty-course combinations wherein at least eight students started none of these combinations were doubly significant. In experiment one the mean $\Delta C$ statistic was covariant with sample size. To get some context regarding the relationship between mean $\Delta C$ and sample size for experiment two, let's look at figure $23$.

```{r Experiment Two Context Plot, fig.cap = "Scatterplot of each doubly significant faculty-course combination's mean Delta C against the number of students that passed filtration and started in it. Colored by effect polarity from the mean. Boxplot provided for summary of sample size distribution."}
##Experimental Plot: Mean.DeltaC Vs. Number of Students. Do good/bad teachers teach more students?
##Determine the total number of students taught by each faculty
Course.Num.Students <- Gen_Data8 %>%
                          ungroup() %>%
                            group_by(`Faculty Random ID`, `Course Code`) %>%
                              count() %>%
                                rename(Overall.Students = n)

##Join total number of students taught to data frame containing only those faculty that had significant Mean DeltaC
Double.Courses %>%
  filter(Sig.Agree == TRUE, T.Sig == TRUE ) %>%
    mutate(Point.Colors = if_else(Mean.DeltaC < mean(Double.Profs$Mean.DeltaC), "green", "red")) %>%
      dplyr::select(First.Prof, First.Course, Mean.DeltaC, Num.Students, Point.Colors) %>%
        left_join(Course.Num.Students, by = c("First.Prof" = "Faculty Random ID", "First.Course" = "Course Code")) %>%
          ggplot(aes(x = Num.Students)) +
          geom_point(aes(color = Point.Colors, y = Mean.DeltaC)) +
          geom_boxplot(color = "red", fill = "green", width = .2, lwd = 1) +
          theme(legend.position = "none") +
          labs(x = "Total Number of Students Taught in Course", y = "Course's Doubly-Significant Mean \u0394C (GPA Points)")
```

|    As in experiment one, the deviation of mean $\Delta C$ for faculty-course combinations in experiment two is covariant with sample size.  From a macroscopic perspective, it seems like the data take values under an exponential decay or a very tall sideways Gaussian. The volatility evens out around the $50$ student mark down to an average mean $\Delta C$ of $\pm 0.25$ as in experiment one. There are more data with lower sample size because we have increased the number of strata without increasing the number of viable students. Given more data, it is likely that these radical values would normalize towards zero.



```{r Cumulative GPA Discreteness}
## This table enumerates the possible values of cumulative GPA for a given student's term assuming that all courses have the same weight. This does not include NA's. This is an estimate, and is not mathematically rigorous. It becomes difficult to determine when means using different numbers will yield the same result - the set {1, 2, 3, 4} gives the same mean as {1, 1, 4, 4} and {2, 2, 3, 3}. As such we use the unique() command to filter out these multiplicities. The high simulation size gives a decent confidence bound for small student terms.
Term.Grades <- Gen_Data %>%
                filter(!is.na(`GPA Assigned`)) %>%
                  arrange(`GPA Assigned`) %>%
                    pull(`GPA Assigned`) %>%
                      unique()

##This function determines the number of unique cumulative GPA's possible for a given term number
Possible.Cum.GPAs <- function(term){
                        n.sims <- 10^6
                        random.cum.gpas <- rep(-1,n.sims)
                        
                        for(i in 1:n.sims){
                              random.cum.gpas[i] <- mean(sample(Term.Grades, size = term, replace = TRUE))
                        }
                        return(unique(random.cum.gpas))
}

##This function determines the overall proportion of students that took n terms at OCC's math and chemistry departments
Term.Prop.Students <- function(term){
                        n.students <- Gen_Data8 %>%
                                        ungroup() %>%
                                          group_by(`Student Random ID`) %>%
                                            slice(1) %>%
                                              ungroup() %>%
                                                filter(max.terms == term) %>%
                                                  pull(max.terms) %>%
                                                    length()
                        prop.students <- n.students/(length(unique(Gen_Data8$`Student Random ID`)))
                        return(prop.students)
}

##Print out the approximate possible number of GPA values per term for first eight terms. 
Cum.GPA.Data <- tibble(Student.Term = seq(from = 1, to = 6, by = 1), Prop.Students = -1, Possible.Cum.GPAs = -1, )
for(i in 1:6){
  Cum.GPA.Data$Possible.Cum.GPAs[i] <- length(Possible.Cum.GPAs(i)) + 1
  Cum.GPA.Data$Prop.Students[i] <- Term.Prop.Students(i)
}

kable(Cum.GPA.Data, caption = "Low estimates of the number of possible values for cumulative GPA for students that take one course per term. Estimates for terms one through six including proportion of students that were enrolled in OCC's math and chemistry departments for that number of terms.", padding = 3)
```

\subsection{DeltaC Error Discussion}

The bootstrap and one sample t-test have error in different ways. The one sample t-test assumes that:

1. Data come from multiple measurements of a continuously distributed random variable,

2. Any subset of data (an event) is a simple random sample from the overall distribution,

3. The random variable which is being tested has a Gaussian distribution,

4. The sample size is sufficiently large (generally n >= 3), and

5. The random variable has a finite constant standard deviation.

##Citation https://www.investopedia.com/ask/answers/073115/what-assumptions-are-made-when-conducting-ttest.asp
For both experiments one and two, our data does not satisfy condition one. Cumulative GPA is a fundamentally discrete beast. For a student with a cumulative GPA $c$, the permitted values for cumulative GPA after one more term depend on c, the allowed course credits, and the allowed GPA's obtainable in a course. For example, for a student that took a single course in their first term (which is the filtration assumption) there are only twelve possible values for cumulative GPA: `r Term.Grades`, and the `r NA` which corresponds to a withdraw. Table thirteen simulates a low estimate for the total possible number of distinct cumulative GPAs as a function of term enrolled in OCC's math and chemistry department. The simulation I used does not take into account course credits because most courses have the same 4-credit valuation. The second term already gives $33$ possible values for cumulative GPA. In my mind $33 values$ is not quite approximately continuous considering they are spread across the range $[0,4]$ with $0.12$ GPA points between each discrete possibility. The $86$ options that corresponds to three terms is much better, but only $49.7%$ of the students used in significance testing were enrolled for at least three terms. Additionally, there is no guarantee that students enrolled for three terms recieved grades for three terms, nor is there any guarantee that all possible cumulative GPAs are equally likely. In short, the discreteness of cumulative GPA is a major source of error for both experiments one and two.

|     The bootstrap principle does not require that its data come from a continuous variable to create a confidence interval, nor must the data come from an approximately normal population. It does however need sufficiently many distinct values in order to accurately depict the sampling distribution. For example, if all eight of a faculty's students obtained $\Delta C = 0$ by some divine providence, the bootstrap would be unable to simulate any possible mean $\Delta C$ other than zero, resulting in a mean $\Delta C$ of zero with a *standard error of zero*. While this never occurred experimentally, I did find a case of a faculty-course combination wherein students all received negative $\Delta C$. For reference, this was faculty   203178 teaching MAT-1540. The bootstrap sampling distribution cannot include the course mean (near zero) if there are no positive $\Delta C$s to obtain in a bootstrap sample, and so the bootstrap was guaranteed to mark faculty 203178's effect as significantly different from the mean in MAT-1540. As said before, the eight student minimum helps mitigate such errors. 

|     The probability that any student would have a negative $\Delta C$ is about $\frac{1}{2}$. The probability that all eight students of a faculty with the minimum acceptable sample size have negative $\Delta C$s is about $(\frac{1}{2})^8 = \frac{1}{256}$. Doubling this quantity gives the probability that all eight students have either positive or negative $\Delta C$s as $\frac{1}{128}$. In experiment two there are $605$ faculty-course combinations that get tested by the bootstrap confidence interval, so we would expect roughly $4.72$ instances of a faculty being randomly assigned eight students with matching $\Delta C$ polarity. The variance in random assignment of mean $\Delta C$ clearly must decrease with sample size. Considering figures $18$ and $23$ show convergence of mean $\Delta C$ towards zero as sample size increases, we have evidence that a faculty's mean $\Delta C$ may be discounted for low sample sizes. The art is determining what the minimum acceptable sample size is for a "real" mean $\Delta C$ effect.

|     Usage of a linear mixed-effects model may not be appropriate for the $\Delta C$ statistic. No affine can properly predict a student's cumulative GPA progression through an arbitrary number of terms. If a student failed their first course and that course had a mean negative cumulative GPA slope after adjusting for the number of terms the student was enrolled in the math and chemistry departments, the student's $\Delta C$ would be positive even if they failed every single course they were enrolled in. Similarly, a student that received a perfect $4.0$ cumulative GPA would have a negative $\Delta C$ if the mean course slope were positive. These scenarios imply that stratification by first GPA received may be warranted. The first GPA received is highly covariant with first course and so there may be the issue of double counting. GPA received in the first course may be a suitable replacement for the first course stratification, but that would be a different project.

|     Faculty beyond the first encountered also have an effect on student outcomes. The $\Delta C$ statistic operates on the assumption that future faculty effects average out over sufficiently many possible future instructors. This assumption is flawed for faculty that had few students start in their course due to high variance. Higher-level courses have fewer instructors because fewer students take the courses. For example, the effect of future faculty may not be random for students whose first course in the dataset was CHE-2610, because there were only seven unique faculty that teach CHE-2620 between 2010 and 2017. Moreover the maximum number of faculty teaching CHE-2620 in a given term was three; there are not a continuous distribution of future professor effects for CHE-2620. If the three faculty assigned different mean GPAs in their courses, a bias would be created on the mean $\Delta C$ statistic for CHE-2610 instructors. 

```{r Max.Terms Frequency for Organic Chemistry 2 and College Algebra}
Orgo2.Terms1 <- Gen_Data7 %>%
                  ungroup() %>%
                    group_by(`Student Random ID`) %>%
                      slice(1) %>%
                        mutate(First.Course = `Course Code`) %>%
                          dplyr::select(First.Course, `Student Random ID`) %>%
                            right_join(Gen_Data7) %>%
                              filter(First.Course == "CHE-2620") %>%
                                count() %>%
                                  ungroup() %>%
                                    group_by(n) %>%
                                      count() %>%
                                        rename(max.terms = n, Num.Students = nn) %>%
                                          ungroup()

Orgo2.Terms2 <- Orgo2.Terms1 %>%
                  mutate(rel.freq = Num.Students/sum(Orgo2.Terms1$Num.Students))
    
kable(Orgo2.Terms2, padding = 3, caption = "Frequency and relative frequency table of number of terms enrolled in OCC's math and chemistry departments for students starting in CHE-2620.")  

Col.Alg.Terms1 <- Gen_Data7 %>%
                    ungroup() %>%
                      group_by(`Student Random ID`) %>%
                        slice(1) %>%
                          mutate(First.Course = `Course Code`) %>%
                            dplyr::select(First.Course, `Student Random ID`) %>%
                              right_join(Gen_Data7) %>%
                                filter(First.Course == "MAT-1540") %>%
                                  count() %>%
                                    ungroup() %>%
                                      group_by(n) %>%
                                        count() %>%
                                          rename(max.terms = n, Num.Students = nn) %>%
                                            ungroup()

Col.Alg.Terms2 <- Col.Alg.Terms1 %>%
                    mutate(rel.freq = Num.Students/sum(Col.Alg.Terms1$Num.Students))

kable(Col.Alg.Terms2, padding = 3, caption = "Frequency and relative frequency table of number of terms enrolled in OCC's math and chemistry departments for students starting in MAT-1540.")
```

|     Faculty which predominately teach higher level courses are less likely to be assigned a mean $\Delta C$ because the mean $\Delta C$ statistic requires at least eight students to pass the imposed filtration conditions and start with the professor. Table fourteen gives the frequency and relative frequency of total terms enrolled in the math and chemistry departments for those students that started in CHE-2620. Table fifteen does the same with students that began in MAT-1540. CHE-2620 is the highest level chemistry course offered, whereas MAT-1540 is OCC's equivalent of high school algebra II. Not only does CHE-2620 contain many fewer cases overall, a larger proportion of students that started in CHE-2620 took only one term as compared to MAT-1540. It must be reiterated that because OCC is a community college, students are likely to transfer to a school offering a four year degree after completing necessary prerequisites. There is no way to calculate the effect CHE-2620 faculty had on students that transferred out of OCC not because they had no effect, but because the grades received are not recorded in Professor Eckstrom's data. The mean $\Delta C$ statistic is then given more credence within low level courses like MAT-1540 in which students are both: 1) likely to start in, and 2) likely to take more courses after. This error is amplified in experiment two, contributing to the higher rejection rate shown in table 11 than that of table 9.




\subsection{DeltaW}
       
```{r Make DeltaW Bootstrapper Functions}
##Check whether each student withdrew from their course in Gen_Data
Gen_Data10 <- Gen_Data8 %>%
                mutate(Withdraw = if_else(is.na(`GPA Assigned`), TRUE, FALSE))

##Determine overall withdraw rate for usage in experiment one
Overall.Withdraw.Rate <- Gen_Data10 %>%
                          filter(is.na(`GPA Assigned`)) %>%
                            mutate(Withdraw = TRUE) %>%
                              pull(Withdraw) %>%
                                sum()/length(Gen_Data8$`GPA Assigned`)

##Determine each Course's Withdraw Rate for usage in get.observed.prop() for experiment two
Course.Withdraw.Rates <- Gen_Data10 %>%
                          ungroup() %>%
                            group_by(`Course Code`) %>%
                              mutate(Course.Students = n()) %>%     
                                filter(Course.Students >= 100) %>%
                                  filter(is.na(`GPA Assigned`)) %>%
                                    mutate(Course.Withdraws = n()) %>%
                                      mutate(Course.Withdraw.Rate = Course.Withdraws/Course.Students) %>%
                                        slice(1) %>%
                                          dplyr::select(`Course Code`, Course.Withdraw.Rate)

##Make Bootstrapper functions to evaluate significance of each faculties' observed withdraw proportion

##This grabs the overall course withdraw rate from Course.Withdraw.Rates
get.observed.prop <- function(course.code = NA){
  if(is.na(course.code)){
    observed.prop <- Overall.Withdraw.Rate
  } else{
      observed.prop <- Course.Withdraw.Rates %>%
                        filter(`Course Code` == course.code) %>%
                          pull(Course.Withdraw.Rate)
  }
  return(observed.prop)
}
##This makes a bootstrap distribution using those students that took a course with a faculty member.
make.withdraw.bootstrap.props <- function(prof.num, course.code = NA){
  if(is.na(course.code)){
    og.sample <- Gen_Data10$Withdraw[Gen_Data10$`Faculty Random ID` == prof.num]
  } else{
    og.sample <- Gen_Data10$Withdraw[Gen_Data10$`Faculty Random ID` == prof.num & Gen_Data10$`Course Code` == course.code]
  }
  samp.size <- length(og.sample)
  boot.props <- rep(0,10^4)
  for(i in seq_along(boot.props)){
    boot.sample <- sample(og.sample, replace = TRUE, size = samp.size)
    boot.props[i] <- mean(boot.sample)
  }
  boot.props <- tibble(Bootstrap.Withdraw.Prop = boot.props)
  return(boot.props)
}
##This determines whether the faculty's withdraw rate is significantly different from the course rate.
evaluate.withdraw.significance <- function(prof.num, course.code = NA){
  current.significance <- NA
  if(is.na(course.code)){
  current.bootstrap.distribution <- make.withdraw.bootstrap.props(prof.num) %>% 
                                      pull(Bootstrap.Withdraw.Prop)
  } else{
    current.bootstrap.distribution <- make.withdraw.bootstrap.props(prof.num, course.code) %>% 
                                        pull(Bootstrap.Withdraw.Prop)
  }

  lower.quantile <- quantile(current.bootstrap.distribution, 0.025)
  upper.quantile <- quantile(current.bootstrap.distribution, 0.975)
  benchmark <- get.observed.prop(course.code)
  
    if(upper.quantile <= benchmark){
      current.significance <- TRUE
    } else if(lower.quantile >= benchmark){
      current.significance <- TRUE
    } else{
      current.significance <- FALSE
    }
  return(current.significance)

}
```



```{r Create and Test DeltaW Data Sets}
##Initialize Experiment One Data Set
Overall.DeltaW.Data <- Gen_Data10 %>%
                        semi_join(Course.Withdraw.Rates, by = "Course Code") %>%
                          ungroup() %>%
                            group_by(`Faculty Random ID`) %>%
                              summarise(Observed.Withdraw.Prop = sum(Withdraw)/n(), Num.Students = n()) %>%
                                mutate(Overall.DeltaW = Overall.Withdraw.Rate - Observed.Withdraw.Prop, Significant = as.logical(NA))

##Test Significance for Experiment One
for(k in seq_along(Overall.DeltaW.Data$`Faculty Random ID`)){
  current.prof <- Overall.DeltaW.Data$`Faculty Random ID`[k]
  Overall.DeltaW.Data$Significant[k] <- evaluate.withdraw.significance(current.prof)
}

##Initialize Experiment Two Data set
Course.DeltaW.Data <- Gen_Data8 %>%
                        ungroup() %>%
                          group_by(`Faculty Random ID`, `Course Code`) %>%
                            summarise(Observed.Withdraw.Rate = sum(is.na(`GPA Assigned`)/n()), Num.Students = n()) %>%
                              left_join(Course.Withdraw.Rates, by = "Course Code") %>%
                                mutate(Course.DeltaW = Course.Withdraw.Rate - Observed.Withdraw.Rate) %>%
                                  filter(Num.Students >= 8) %>%
                                    mutate(Significant = as.logical(NA)) %>%
                                      semi_join(Course.Withdraw.Rates, by = "Course Code")

##We now apply evaluate.significance across Course DeltaW Data to obtain significance results for each combination of course and professor with at least 8 students. This is experiment two.
n.evaluations <- length(Course.DeltaW.Data$`Faculty Random ID`) 
for(k in 1:n.evaluations){
  current.prof <- Course.DeltaW.Data$`Faculty Random ID`[k]
  current.course <- Course.DeltaW.Data$`Course Code`[k]
  Course.DeltaW.Data$Significant[k] <- evaluate.withdraw.significance(current.prof, current.course)
}
```

```{r}
Overall.Withdraw.Sd <- Overall.DeltaW.Data %>%
                        pull(Observed.Withdraw.Prop) %>%
                          sd()

Course.Withdraw.Sd <- Course.DeltaW.Data %>%
                        pull(Observed.Withdraw.Rate) %>%
                          sd()
```

|   The definition of $\Delta W$ given by equation ($\ref{DeltaW Definition}$) involves the expected withdraw rate $W_E$ and the observed withdraw rate $W_O$. The expected withdraw rate in experiment one is the probability `r Overall.Withdraw.Rate`, calculated as the total number of withdraws in the data set divided by the total number of observations. That is, it is the mean withdraw rate among all students between $2010$ and $2017$ in OCC's math and chemistry departments. The observed withdraw rate is calculated as the total number of students that withdraw from *any* of a particular professor's courses divided by the total number of students taught over all courses. In experiment two $W_E$ is set to the mean withdraw rate for students in the germane course, while $W_O$ is the withdraw rate in a particular course code taught by a faculty member. To reiterate, a positive value of $\Delta W$ corresponds to a professor having a lower withdraw rate than the mean, and a negative value corresponds to a higher withdraw rate than the mean. This orientation is selected for ease of comparison with the mean $\Delta C$ statistic.

```{r DeltaW Experiment One Probability Plot, fig.cap = "Normal probability plot of Delta W over all faculty for experiment one"}
##Normal probability plot of DeltaW over all faculty for experiment one
Overall.DeltaW.Data %>%
  pull(Overall.DeltaW) %>%
    qqnorm(main = NULL)
```

```{r DeltaW Experiment Two Probability Plot, fig.cap = "Normal probability plot of Delta W over all faculty for experiment two"}
##Normal probability plot of DeltaW over all faculty for experiment one
Course.DeltaW.Data %>%
  pull(Course.DeltaW) %>%
    qqnorm(main = NULL)
```

|    Figure $24$ is a normal probability plot of $\Delta W$'s distribution in experiment one, and figure $25$ shows the distribution in experiment two.  Both graphs demonstrate a slight left-skew for data approximately to the left of the $-1$ theoretical quantile. Observations of $\Delta W$ greater than the $-1$ theoretical quantile fit a normal distribution quite well. Regardless, usage of the one sample t-test generated confidence interval is ill-advised due to the requirement that data come from a normal distribution. Data will only be tested for significance using the bootstrap confidence interval given by equation ($\ref{Bootstrap Confidence Interval}$). We stipulate that valid observations of $\Delta W$ possess at least eight withdraws so that data are sufficiently rich. This is likely overkill, but simply requiring eight students is not sufficient. Considering the overall withdraw rate of `r Overall.Withdraw.Rate`, the probability of not observing a single withdraw within eight students (which I denote $P(0\in8)$) is given by:

$$P(0 \in 8) = P(0 \in 1)^8 = (1-0.238)^8 = 0.114$$

In other words, the p-value of a faculty that obtains no withdraws within eight students is $0.114$. Because we set $\alpha = 0.05$, we would automatically fail to reject the $H_0$ from equation ($\ref{Bootstrap Hypotheses}$). This is clearly unacceptable. Requiring fourty students is standard guidance for usage of the central limit theorem, but as figures $24$ and $25$ show that $\Delta W$'s theoretical restriction on the domain $[-1,1]$ induces non-normality onto its distribution. We've been using the number eight throughout this case study, so we will use a minimum eight withdraws as an aesthetically pleasing arbitrary benchmark for sufficient richness of data.



\subsubsection{DeltaW Experiment One Results}

```{r DeltaW Experiment 1 Sufficient Sample Size}
##Number of Faculty that had at least eight withdraws 
Suff.Samp.DeltaW.1 <- Gen_Data10 %>%
                        filter(Withdraw == TRUE) %>%
                          group_by(`Faculty Random ID`) %>%
                            summarise(Num.Students = n()) %>%
                              mutate(Sufficient.Sample.Size = if_else(Num.Students >= 8, TRUE, FALSE)) %>%
                                ungroup() %>%
                                  group_by(Sufficient.Sample.Size) %>%
                                    count()

kable(Suff.Samp.DeltaW.1, padding = 3, caption = "Number of faculty that had at least eight students withdraw from their all their courses. We can use bootstrap confidence intervals on the 245 professors with at least eight withdraws.")
```

|     Table fifteen shows that in experiment one, of the $263$ faculty that had at least one student withdraw, $245$ survived the requirement of eight students withdrawing from their courses. Note that there were four faculty in Dr. Eckstrom's dataset that did not have any withdraws and so cannot be theoretically evaluated for significance using our bootstrap confidence interval. I supply these along with p-values in table sixteen. Three of the four professors listed pass the $p = \frac{\alpha}{2} = 0.025$ benchmark for the rejection of $H_0$. While they will not be included in the following graphics among the faculty that had at least eight students withdraw, it is important to note that there is an error in assuming that none of the faculty that had fewer than eight students withdraw had a significant $\Delta W$.  We will now create the same graphics for the $\Delta W$ statistic as were created for mean $\Delta C$.

```{r}
Gen_Data10 %>%
  ungroup() %>%
    group_by(`Faculty Random ID`) %>%
      mutate(total.withdraws = sum(Withdraw)) %>%
        filter(total.withdraws == 0) %>%
          dplyr::select(`Faculty Random ID`) %>%
            count() %>%
              rename(Num.Students = n) %>%
                mutate(pval = (1-0.238)^(Num.Students))
```

```{r Create DeltaW Experiment One Statistics}
Overall.Mean.DeltaW <- Overall.DeltaW.Data %>%
                        pull(Overall.DeltaW) %>%
                          mean()

Overall.Sd.DeltaW <- Overall.DeltaW.Data %>%
                        pull(Overall.DeltaW) %>%
                          sd()

Overall.Positive.Mean.DeltaW <- Overall.DeltaW.Data %>%
                                  filter(Significant == TRUE) %>%
                                    filter(Overall.DeltaW > 0) %>%
                                      pull(Overall.DeltaW) %>%
                                        mean()

Overall.Positive.Sd.DeltaW <- Overall.DeltaW.Data %>%
                                  filter(Significant == TRUE) %>%
                                    filter(Overall.DeltaW > 0) %>%
                                      pull(Overall.DeltaW) %>%
                                        sd()

Overall.Negative.Mean.DeltaW <- Overall.DeltaW.Data %>%
                                  filter(Significant == TRUE) %>%
                                    filter(Overall.DeltaW < 0) %>%
                                      pull(Overall.DeltaW) %>%
                                        mean()

Overall.Negative.Sd.DeltaW <- Overall.DeltaW.Data %>%
                                  filter(Significant == TRUE) %>%
                                    filter(Overall.DeltaW < 0) %>%
                                      pull(Overall.DeltaW) %>%
                                        sd()

DeltaW.Experiment.One.N.mean <- Overall.DeltaW.Data %>%
                                  pull(Num.Students) %>%
                                    mean()

DeltaW.Experiment.One.N.sd <- Overall.DeltaW.Data %>%
                                  pull(Num.Students) %>%
                                    sd()
```

```{r DeltaW Experiment One Overall Histogram, fig.cap = "Histogram of all experiment one Delta W's, colored by effect polarity from the mean."}
##Experiment One Graphs
##Histogram of DeltaW over all faculty for experiment one
Overall.DeltaW.Data %>%
  mutate(Hist.Colors = if_else(Observed.Withdraw.Prop > Overall.Withdraw.Rate, "green", "red")) %>%
    ggplot(aes(x = Overall.DeltaW, fill = Hist.Colors)) +
    geom_histogram(binwidth = 0.03) + 
    theme(legend.position = "none") +
    labs(x = "\u0394W (Probability)", y = "Count")
```

```{r , fig.cap= "Histogram of only those experiment one Delta W's that were significantly different from the mean, colored by effect polarity from the mean."}
##Histogram of DeltaW over significant faculty for experiment one
Overall.DeltaW.Data %>%
  filter(Significant == TRUE) %>%
    mutate(Hist.Colors = if_else(Observed.Withdraw.Prop > Overall.Withdraw.Rate, "green", "red")) %>%
      ggplot(aes(x = Overall.DeltaW, fill = Hist.Colors)) +
      geom_histogram(binwidth = 0.03) + 
      theme(legend.position = "none") + 
      labs(x = "Significant \u0394W (Probability)", y = "Count")

```

|     As was predicted by figure $24$, the distribution of $\Delta W$s in experiment one has a slight left skew. Figure $26$ shows the overall distribution, and figure $27$ shows the distribution of only those $\Delta W$s that were significantly different from zero. The left skew will be more pronounced in experiment two when there are more observations to work with. Now we indulge in some summary statistics regarding $\Delta W$ experiment one.

|     In experiment one the $\Delta W$ statistic had an overall mean of `r Overall.Mean.DeltaW`, with standard deviation `r Overall.Sd.DeltaW`. Similar to the mean $\Delta C$ we expect the overall mean $\Delta W$ to be zero with some wiggle room, where divergence from zero is created by random variable discreteness and the removal of some observations. Faculty with significantly positive $\Delta W$ values had mean $\Delta W$ `r Overall.Positive.Mean.DeltaW` with standard deviation `r Overall.Positive.Sd.DeltaW`. Those faculty with significantly negative $\Delta W$s had mean `r Overall.Negative.Mean.DeltaW` and standard deviation `r Overall.Negative.Sd.DeltaW`. The significantly negative mean is farther from zero than the significantly positive mean, and the standard deviation for negative $\Delta W$'s was larger than the standard deviation of the positive ones. These statistics taken together provide a clear description of a left-skew. 

|     Why is the $\Delta W$ distribution left-skewed? Because it is a probability, the overall mean withdraw rate can theoretically take any value on the set $[0,1]$. The $\Delta W$ statistic then has a measure of length one somewhere on the set $[-1,1]$ depending on the observed mean $W_O$. In experiment one $W_O$ is `r Overall.Withdraw.Rate`, which means the empirical $\Delta W$ distribution will have domain $[-0.762 , 0.238]$. There is much more room for negative effects than positive effects, and so there will be disproportionately many negative $\Delta W$s. The quantity of outliers is a function of sample size, so we expect the emperical distribution of experiment two $\Delta W$s to be more skewed than that of experiment one. 

```{r Experiment One DeltaW Context Plot, fig.cap = "Scatterplot of each significant faculty's experiment one Delta W against the number of students that took their courses. Colored by effect polarity from the mean. Boxplot provided for summary of sample size distribution."}

##Join total number of students taught to data frame containing only those faculty that had significant Mean DeltaC
DeltaW.Context.Plot1 <- Overall.DeltaW.Data %>%
                          filter(Significant == TRUE) %>%
                            mutate(Point.Colors = if_else(Observed.Withdraw.Prop <  Overall.Withdraw.Rate, "green", "red")) %>%
                              dplyr::select(`Faculty Random ID`, Overall.DeltaW, Num.Students, Point.Colors) %>%
                                left_join(Overall.Num.Students, by = "Faculty Random ID") %>%
                                  ggplot(aes(x = Num.Students)) +
                                  geom_point(aes(color = Point.Colors, y = Overall.DeltaW)) +
                                  geom_boxplot(color = "red", fill = "green", width = 0.03) +
                                  theme(legend.position = "none") +
                                  labs(x = "Total Students Taught", y = "Significant \u0394W (Probability)")

##Print the context plot for DeltaW Experiment One 
DeltaW.Context.Plot1
```

|     Faculty taught a mean total of $521$ students with a standard deviation of $594$ students. As was the case in both experiments on mean $\Delta C$, the distribution of sample size is very right skewed. The deviation of $\Delta W$ from zero shown in figure $28$ follows a similar pattern as shown in figures $18$ and $23$. The volatility of $\Delta W$ as the number of students approaches the minimum eight is most likely an illusory effect caused by the growth of standard error as $N \to 0$.

\subsubsection{DeltaW Experiment Two Results}

```{r Course Withdraw Rates, fig.cap = "Bar chart of all course withdraw rates."}
##Histogram of course withdraw rates. There seems to be a slight left skew, but this is likely due to variance as there only being 21 remaining courses. Citation https://stackoverflow.com/questions/35090883/remove-all-of-x-axis-labels-in-ggplot for prettying up the x axis.
Course.Withdraw.Rates %>%
  mutate(`Course Code` = factor(`Course Code`)) %>%
    ggplot(aes(x = fct_reorder(`Course Code`, -Course.Withdraw.Rate), y = Course.Withdraw.Rate, fill = `Course Code`)) +
    geom_bar(stat = "identity") +
    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
    labs(x = "Course Code", y = "Withdraw Rate (Probability)")
```


```{r DeltaW Experiment Two Sufficient Sample Size}
##Number of Faculty-Course combinations represented in significant histogram compared to total
DeltaW.Experiment.Two.Sufficient.Sample.Size <- Gen_Data10 %>%
                                                  filter(Withdraw == TRUE) %>%
                                                    group_by(`Faculty Random ID`, `Course Code`) %>%
                                                      summarise(Num.Students = n()) %>%
                                                        mutate(Sufficient.Sample.Size = if_else(Num.Students >= 8, TRUE, FALSE)) %>%
                                                          ungroup() %>%
                                                            group_by(Sufficient.Sample.Size) %>%
                                                              count()

kable(DeltaW.Experiment.Two.Sufficient.Sample.Size, padding = 3, caption = "Number of faculty-course combinations from which at least eight students withdrew. We can use bootstrap confidence intervals on the 736 professors with at least eight withdraws.")
```

asdf

```{r DeltaW Experiment Two Total Histogram, fig.cap = "Histogram of all experiment two Delta W's, colored by effect polarity from the mean."}
##Histogram of DeltaW over all faculty for experiment two  
Course.DeltaW.Data %>%
  mutate(Hist.Colors = if_else(Observed.Withdraw.Rate > Course.Withdraw.Rate, "green", "red")) %>%
    ggplot(aes(x = Course.DeltaW, fill = Hist.Colors)) +
    geom_histogram(binwidth = 0.05) + 
    theme(legend.position = "none") %>%
    labs(x = "\u0394W", y = "Count")
```


```{r DeltaW Experiment Two Significant Histogram, fig.cap = "Histogram of only those experiment two Delta W's that were significantly different from the mean, colored by effect polarity from the mean."}
##Histogram of DeltaW over signifiacnt faculty for experiment two  
Course.DeltaW.Data %>%
  filter(Significant == TRUE) %>%
    mutate(Hist.Colors = if_else(Observed.Withdraw.Rate > Course.Withdraw.Rate, "green", "red")) %>%
      ggplot(aes(x = Course.DeltaW, fill = Hist.Colors)) +
      geom_histogram(binwidth = 0.05) + 
      theme(legend.position = "none") +
      labs(x = "\u0394W", y = "Count")
```

```{r}
##Scatterplot of ALL DeltaW by course
Course.DeltaW.Data %>%
  left_join(Course.Heights, by = "Course Code") %>%
    ggplot(aes(x = Course.DeltaW, color = `Course Code`, y = Height, size = Num.Students)) +
    geom_point(position = position_jitter(height = 0.009)) +
    theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
    labs(x = "\u0394W", color = "Course Code", y = "Course Code", size = "Number of Students", title = "Scatterplot of ALL \u0394W's by Course Code")

##Scatterplot of Significant DeltaW by course
Course.DeltaW.Data %>%
  filter(Significant == TRUE) %>%
  left_join(Course.Heights, by = "Course Code") %>%
    ggplot(aes(x = Course.DeltaW, color = `Course Code`, y = Height, size = Num.Students)) +
    geom_point(position = position_jitter(height = 0.009)) +
    theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
    labs(x = "\u0394W", color = "Course Code", y = "Course Code", size = "Number of Students", title = "Scatterplot of Significant \u0394W's by Course Code")

##Histogram of sample sizes - greater than or equal to eight - for experiment one.
Course.DeltaW.Data %>%
  ggplot(aes(x = Num.Students)) +
  geom_histogram(bins = 17) +
  geom_boxplot(color = "red", fill = "green", width = 20, lwd = 1.5) +
  scale_x_continuous(limits = c(0,150), breaks = seq(from = 0, to = 150, by = 25)) +
  labs(x = "Number of Students", y = "Count", title = "Histogram and Boxplot of Sample Sizes for Experiment Two.")

```

```{r DeltaW and DeltaC Comparison Plots}
##Experiment One Plots
##Mean DeltaC Vs DeltaW for all faculty with a mean DeltaC, colored by Significance levels
Experiment.One.Stat.Comparison <- Double.Profs %>%
                                    mutate(SigC = if_else(T.Test.Sig == TRUE & Bootstrap.Sig == TRUE, TRUE, FALSE)) %>%
                                      dplyr::select(First.Prof, Mean.DeltaC, SigC) %>%
                                          full_join(Overall.DeltaW.Data, by = c("First.Prof" = "Faculty Random ID")) %>%
                                            rename(SigW = Significant) %>%
                                              dplyr::select(First.Prof, Mean.DeltaC, Overall.DeltaW, SigC, SigW) %>%
                                                mutate(Significance = factor(if_else(SigC == TRUE & SigW == TRUE, "Both \u0394C and \u0394W", if_else(SigC == FALSE & SigW == TRUE, "Only \u0394W", if_else(SigC == TRUE & SigW == FALSE, "Only \u0394C", if_else(SigC == FALSE & SigW == FALSE, "Neither \u0394C nor \u0394W", as.character(NA))))))) %>%
                                                  filter(!is.na(Significance))

Experiment.One.Stat.Comparison %>%
  ggplot(aes(x = Mean.DeltaC, y = Overall.DeltaW)) +
  geom_point(aes(color = Significance)) + 
  geom_smooth() + 
  labs(x = "Mean \u0394C (GPA Points)", y = "\u0394W", color = "Significance", title = "Mean \u0394C Vs. \u0394W for All Faculty in Experiment One")

lm(Overall.DeltaW ~ Mean.DeltaC, data = Experiment.One.Stat.Comparison)

##Experiment Two Plots
##Mean DeltaC Vs. DeltaW for all faculty with a mean DeltaC in experiment two

Experiment.Two.Stat.Comparison <- Double.Courses %>%
                                    mutate(SigC = if_else(T.Sig == TRUE & Bootstrap.Sig == TRUE, TRUE, FALSE)) %>%
                                      dplyr::select(First.Prof, First.Course, Mean.DeltaC, SigC) %>%
                                        full_join(Course.DeltaW.Data, by = c("First.Prof" = "Faculty Random ID", "First.Course" = "Course Code")) %>%
                                          rename(SigW = Significant) %>%
                                            dplyr::select(First.Prof, First.Course, Mean.DeltaC, Course.DeltaW, SigC, SigW) %>%
                                              mutate(Significance = factor(if_else(SigC == TRUE & SigW == TRUE, "Both \u0394C and \u0394W", if_else(SigC == FALSE & SigW == TRUE, "Only \u0394W", if_else(SigC == TRUE & SigW == FALSE, "Only \u0394C", if_else(SigC == FALSE & SigW == FALSE, "Neither \u0394C nor \u0394W", as.character(NA)))))))

Experiment.Two.Stat.Comparison %>%
  ggplot(aes(x = Mean.DeltaC, y = Course.DeltaW)) +
  geom_point(aes(color = Significance)) + 
  geom_smooth() + 
  labs(x = "Mean \u0394C (GPA Points)", y = "\u0394W", color = "Significance", title = "Mean \u0394C Vs. \u0394W for all faculty in Experiment Two")

lm(Course.DeltaW ~ Mean.DeltaC, data = Experiment.Two.Stat.Comparison)
```
      
       
       
       
       
       
       
       
       
       
        
      ERROR ANALYSIS
1. Who teaches OCC courses?  https://oaklandcc.edu/faculty/default.aspx
  49 Math Faculty
  12 Chemistry Faculty
  Are some faculty being recyled for anonymity, or does OCC not report former facult?





|     Calculation of the $\Delta C$ statistic amounts to determining the difference between a students observed and expected final cumulative GPA based on a couple parameters. 

\subsection{The Issue With lme4}

        c.	Linear Mixed Effects Modeling
                i.	https://www.youtube.com/watch/VhMWPkTbXoY 
                ii.	https://m-clark.github.io/mixed-models-with-R/ 
                iii.	https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models 
                iv.	https://www.rensvandeschoot.com/tutorials/lme4/ 
                v.	Difference between lm and lmer()
                        1.	Standard Linear Regression formula
                                a.	Calculation of slope
                                b.	Calculation of mean intercept
                                c.	Residual Variance
                        2.	Fixed Effects Vs. Mixed Effects
                        3.	lme4 syntax
                vi.	Terminology
                
VII.	Case Study
       

V.	Coursera Work
        a.	Overview 
                i.	Videos
                        1.	Automated for DSTB to keep material up to date
                                a.	Transcript + Slideshow
                        2.	Non-automated for R-Programming
                ii.	Readings
                        1.	Some independent readings
                        2.	All videos have transcripts
                iii.	Book – R Programming for Data Science by Roger D. Peng [version 9/3/2020]
                        1.	https://bookdown.org/rdpeng/rprogdatascience/ 
                iv.	Quizzes
                v.	Programming Assignments
                        1.	Graded by quiz
                        2.	Graded by peers
        b.	The Data Scientist’s Toolbox – Show Certificate
                i.	Topics
                        1.	Week 1
                                a.	Introduction to Data Science
                                b.	Getting help on projects appropriately
                                        i.	Rules of Decorum
                                        ii.	Forums (I primarily used StackOverflow)
                                c.	The Data Science Process
                                        i.	Ask a question
                                        ii.	Obtain Data for use
                                        iii.	Analyse Data 
                                        iv.	Communicate Results
                        2.	Week 2
                                a.	Installing RStudio,
                                        i.	How to install packages
                                b.	RStudio tour
                                c.	Superfluous week considering usage of R and RStudio in probability and math stats
                        3.	Week 3
                                a.	Version Control
                                        i.	GitHub
                                                1.	GitBash – I didn’t end up using my personal copy of RStudio in favor of RStudio Cloud offered through the college. GitBash was therefore ultimately unnecessary
                                        ii.	Linking GitHub to RStudio
                                                1.	Commit
                                                2.	Stage
                                                3.	Push/Pull
                                                4.	Also covered in Data Science course
                                        iii.	Merge Conflicts
                        4.	Week 4
                                a.	R Markdown
                                        i.	How to create reports with knitr
                                        ii.	R code chunks
                                                1.	How to label
                                                2.	Options
                                                3.	Inline and standalone
                                        iii.	LaTeX
                                b.	Asking Data Science Questions
                                c.	Experimental Design
                                d.	Big Data
                                        i.	Three V’s
                                                1.	Volume
                                                2.	Variety
                                                3.	Velocity
                                        ii.	Benefits
                                                1.	Can ask questions that are more difficult to statistically infer answers (higher n)
                                                2.	Can ask new questions due to variety of data available (even if it is really nasty)
                                                        a.	Can identify hidden correlations – data mining
                                                3.	New data that measures the same effects can confirm or reject inferences previously held
                ii.	Course Project
                        1.	Verification that students had properly connected RStudio to GitHub and learned how to create a GitHub repository and commit changes to it.
                        2.	Push a markdown file to a repo
                        3.	Fork a repo
                        4.	Grade 2 peers
        c.	R Programming – Show Certificate
                i.	Week One
                        1.	Topics
                                a.	Background Material
                                        i.	For those students that hadn’t previously taken The Data Scientist’s Toolbox
                                        ii.	Links to instructor’s book
                                        iii.	Pre-course survey
                                        iv.	Directory management
                                                1.	getwd()
                                                2.	setwd()
                                                3.	list.files()
                                                4.	Restoring directory at end of program
                                b.	History of R
                                c.	Reiteration of how to get help
                                d.	Data Types
                                        i.	Class Invariants
                                                1.	Numeric
                                                2.	Character
                                                3.	Complex
                                                4.	Integer
                                                5.	Logical
                                                6.	Raw
                                        ii.	Factors – categorical variables
                                                1.	Used for sorting by levels
                                        iii.	Arrays
                                                1.	Matricies
                                                2.	Tibbles
                                        iv.	Vectors
                                                1.	Only have one class invariant
                                        v.	Lists
                                                1.	Can contain multiple class invariants
                                e.	Read in Data to R
                                        i.	read.table()
                                        ii.	read.csv()
                                        iii.	Not useful in context with readR – I used read_csv()
                                f.	Subsetting
                                        i.	Single Brackets - [a:b, c:d]
                                                1.	Returns object of same class as original
                                                2.	Can select more than one object using row / column indexing
                                                        a.	Numeric index
                                                        b.	Logical index
                                        ii.	Double Brackets - [[a]]
                                                1.	Extract element from list / data frame
                                                2.	
                                        iii.	$ operator
                                                1.	Return elements by name, if objects have names
                                        iv.	Removing NA’s
                                                1.	is.na()
                                                2.	is.nan()
                                g.	Vectorized Operations
                                        i.	Element wise operations
                        2.	Week One Swirl Assignments
                                a.	What is Swirl?
                                        i.	R Package that teaches R functionality and programming w/in R. 
                                        ii.	Review and Application of concepts taught in Videos/Readings from previous week
                                        iii.	Individual Modules = Programming Assignment
                                        iv.	Links back to Coursera for credit by registered email
                                b.	Week One Modules
                                        i.	Basic Building Blocks
                                                1.	Class Invariants
                                                2.	Operations
                                                3.	Saving values to objects
                                        ii.	Workspace and Files
                                                1.	Directory Management
                                        iii.	Sequences of Numbers
                                                1.	Working with lists
                                        iv.	Vectors
                                                1.	Working with vectors
                                        v.	Missing Values
                                                1.	Generate NA’s
                                                2.	Use is.na() and is.nan() to identify indices 
                                        vi.	Subsetting Vectors
                                                1.	[]
                                                2.	[[]]
                                                3.	$
                                        vii.	Matrices and Data Frames
                                                1.	Using names()
                                                2.	Multidimensional subsetting
                                                3.	Quiz – 20 Q’s
                        3.	No Programming Assignment
        ii.	Week Two – Programming With R
                1.	Topics
                        a.	Control Structures
                                i.	For loops – Run for a predetermined number of iterations
                                ii.	While loops – Checks a condition
                                iii.	Repeat loops – Infinite loop
                                iv.	In-Loop Commands
                                        1.	Next
                                        2.	Break
                                v.	If-else expressions
                        b.	Functions
                        c.	Scoping Rules
                        d.	Coding Standards
                                i.	Write code using a text editor
                                ii.	Proper indentation
                                        1.	Change spacing in RStudio
                                        2.	Cmd + I
                        e.	Dates, Times, and DateTimes
                                i.	Sys.time() gets datetime which has following classes:
                                ii.	POSIXct - Seconds since 12:00 AM  1/1/1970
                                iii.	POSIXlt - List of date / time information
                                        1.	Nine different pieces of information about time in question
                                iv.	Convert using as.posixct(), as.posixlt()
                2.	Week Two Swirl Assignments
                        a.	Logic
                                i.	Lots of practice problems using Booleans
                                ii.	Determine what boolean complicated logical expressions evaluate to
                        b.	Functions
                                i.	Create functions that take various levels of input
                                        1.	Required input
                                        2.	No input
                                        3.	Optional input
                                ii.	return()
                        c.	Dates and Times
                                i.	Play with conversion between DateTimes, POSIXct, and POSIXlt. 
                                ii.	Use subsetting to obtain information from POSIXlt
                3.	Programming Assignment One
                        a.	Directory Management – 332 CSV files in a file called specdata each containing three variables. One from each of 332 pollution monitoring stations around the US. Names of files are a number from 001 to 332 which indicate which monitoring station data are from. No codebook for location.
                                i.	date
                                ii.	sulfate – sulfate in air (micrograms per cubic meter) on that date
                                iii.	nitrate – nitrate in air (micrograms per cubic meter) on that date
                        b.	Goal: write three functions that analyse the datasets:
                                i.	pollutantmean(directory, pollutant, id = 1:332) – calculates the mean of a pollutant given the directory where the csv’s are found, the pollutant over which to average, and a subset of monitoring stations to collect data from.
                                ii.	complete(directory, id = 1:332) – Determines the total number of completely observed cases over a subset of monitoring stations.
                                iii.	corr(directory, threshold = 0) – Determines the correlation between  sulfate and nitrate pollution for all monitoring stations that have a number of complete observations at least equal to the threshold specified
                        c.	Graded by Quiz – Include functions in appendix
        iii.	Week 3 – Loop Functions and Debugging
                1.	Topics
                        a.	Apply functions
                                i.	lapply – given a list and a functions, applies the function to each element of the list. 
                                ii.	apply – apply a function across the margins of an array. For example, a rectangular MxN matrix would have margin options 1 or 2. 1 would specify the function to run over rows, while 2 would apply over columns.
                                iii.	sapply – smart wrapper for lapply, this is the bread and butter. Returns vector, list, or matrix if appropriate.
                                iv.	mapply – Multivariate apply. Not used in my project.
                                v.	tapply – Apply a function over subsets of a vector/list given indecies
                        b.	split()
                        c.	Debugging Tools
                                i.	traceback() – prints out the function call stack after an error occurs
                                ii.	debug() – allows line by line execution of a function
                                iii.	browser() – suspends execution of a function and puts R into debug mode
                                iv.	trace() – allows debugging code that does not affect functionality. Generally used in conjunction with browser()
                                v.	recover() – Whenever an error is encountered in the function,  open the browser.
                2.	Week 3 Swirl Assignments
                        a.	lapply and sapply
                                i.	Make lists and functions, then apply the functions over the lists
                        b.	vapply and tapply
                                i.	vapply was not covered in lecture, but it is no more complicated than any other apply. It applies a function specifically over a vector.
                                ii.	More of the same.
                3.	Programming Assignment Two – Lexical Scoping
                        a.	Introduction to cacheing and superassignment. 
                        b.	Goal: Write two functions:
                                i.	makeCacheMatrix: Creates a matrix that can cache its inverse (for an invertible matrix)
                                ii.	cacheSolve: Computes the inverse of the matrix generated by makeCacheMatrix. If this has already been calculated, call the cache instead.
                        c.	Include functions in Appendix
                        d.	Peer- Graded through GitHub, so must grade four peers.
                        e.	https://github.com/firstrider55/ProgrammingAssignment2 
        iv.	Week Four – Simulation and Profiling
                1.	Topics:
                        a.	
                        b.	str() – Alternative to summary(). Displays the contents and metadata of an input.
                        c.	ls()
                        d.	Generating random numbers – used all the time in probability and mathstats
                        e.	Simulating. Specifically used lm() and plot() to create a linear model for two variables. I use linear mixed-effects models in my study
                        f.	sample()
                        g.	Efficiency
                                i.	system.time()
                                ii.	Rprof()
                                        1.	Checks the function call stack at regular intervals and records which function is being executed
                                iii.	summaryRprof() takes Rprof() as an input, and determines how much time is being spent in each function. Two methods for data normalization:
                                        1.	“by.total” yields proportion of total run time
                                        2.	“by.self” does the same but first subtracts time elapsed by already executed functions – the last function (if executed only once) will have proportion 1.
                2.	Week Four Swirl Assignments
                        a.	Looking at Data
                                i.	str()
                                ii.	dim()
                                iii.	nrow() and ncol()
                                iv.	object.size() – returns amount of memory taken by object
                                v.	names()
                                vi.	head() and tail()
                                vii.	summary()
                                viii.	table()
                        b.	Simulation
                                i.	sample()
                                ii.	rnorm()
                                iii.	rbinom()
                        c.	Base Graphics
                                i.	plot()
                                ii.	hist()
                                iii.	boxplot()
                                iv.	Not useful because I use ggplot2 in my project
                3.	Programming Assignment Three
                        a.	Given data contains location information for 4706 hospitals, and patient outcomes statistics regarding:
                                i.	Heart Attack
                                ii.	Heart Failure
                                iii.	Pneumonia
                        b.	Goal: Write Three functions
                                i.	best(state, outcome) – determines the best hospital in a state ranked by 30 day mortality for the given outcome. 
                                        1.	If an invalid state is supplied, throw an error with message “invalid state”. If an invalid outcome is supplied, throw an error with message “invalid outcome”.
                                        2.	Ties are to be broken by alphabetical ordering of hospital name.
                                ii.	rankhospital(state, outcome, num = “best”) -  Rank hospitals in a state for the 30 day mortaility in the supplied outcome. Num grabs the rank wanted. “best” corresponds to rank 1, and “worst” corresponds to the last place hospital. If invalid num is supplied return NA. Same caveats as in best()
                                iii.	 rankall(outcome, num = “best”) – Essentially just applies rankhospital() onto all states. If there are fewer than num hospitals in a state, return NA for that state. No state caveats, otherwise same as rankhospital()
                        c.	Graded by Quiz
        v.	Chapter Quizzes
                1.	One quiz per week, so eight overall. Number of questions range from 5 to 20. Generally concerned with the material covered in the last week, but there were perhaps 3-4 instances of a bug where questions would be pulled from future weeks. Quizzes allowed to be retaken, which helped correct for this bug.
                
