---
title: "SIP_Report.Rmd"
author: "Ethan Tucker"
date: "4/7/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, message = FALSE)
options(digits = 3)
```

```{r Initialize, include = FALSE}
library(readxl)
library(tidyverse)
library(knitr)
# library(merTools)
library(lme4)
Gen_Data <- read_excel("Stats Research XLS.xlsx")
set.seed(1000)


Gen_Data <- Gen_Data %>%
                  filter(`Student Random ID` != 649419)
```

DATA SET ISSUES

1) GPA's not assigned consistently across all courses / all departments at OCC for the same letter grade. As such we need to build a grade converter that normalizes grades to GPA's. This requires some coin flipping.

2)

STUFF I HAVE

TABLES

1) Number of math and chemistry taken in each student term:
# ```{r} 
# N.Courses.By.Term 
# ```
        
2) Course Code / Course Credit / Course Name lookup table:
```{r}
# Course.Credits
```

NUMBERS

1) Difference between the total number of faculty - department combinations and the number of unique faculty. This is zero, so every faculty in the math and chemistry departments in the dataset only taught in one department.
```{r No muli-department faculty}
# ## This chunk shows that there are no faculty that teach in both the math and chemistry departments.
# counter <- Gen_Data7 %>%
#               group_by(`Faculty Random ID`, Department) %>%
#                 count() 
# ##This is the difference between the total number of faculty - department combinations and the number of unique faculty.
# length(counter$`Faculty Random ID`) - length(unique(Gen_Data7$`Faculty Random ID`))
```

2) Statistics regarding first filtration: Students must not withdraw from first course
```{r}
# ##Number of students removed by filtration
# length(unique(Gen_Data$`Student Random ID`)) - length(unique(Filtered.Data1$`Student Random ID`))
# ##Number of remaining students
# length(unique(Filtered.Data1$`Student Random ID`))
# ##Number of faculty removed by filtration
# length(unique(Gen_Data$`Faculty Random ID`)) - length(unique(Filtered.Data1$`Faculty Random ID`))
# ##Number of remaining faculty
# length(unique(Filtered.Data1$`Faculty Random ID`))
# 
# ##Show that there are no entries of Filtered.Data1 that have an NA value for First.Prof - that is we have successfully chosen only those students who did not withdraw from their first course
# length(Filtered.Data1$First.Prof[is.na(Filtered.Data1$First.Prof)])
```



GRAPHS

1) Not First Withdraw Justification
```{r Not First Withdraw Justification, warning=FALSE}
# ##Create plot to illustrate the inability to use all students for Q1. We must then do a filtration!
# Plot.Data1 <- Gen_Data8 %>%
#                 group_by(`Student Random ID`) %>%
#                   slice(1) %>%
#                     mutate(First.Term.Withdraw = if_else(is.na(Cum.GPA), "Yes", "No")) %>%
#                       dplyr::select(`Student Random ID`, First.Term.Withdraw) %>%
#                         right_join(Gen_Data8, by = c("Student Random ID"))
# 
# Plot.Data1 %>% 
#     filter(`Student Random ID` <= 10^3) %>%
#         ggplot(aes(x = Student.Term, y = Cum.GPA, group = factor(`Student Random ID`), color = First.Term.Withdraw)) +
#         geom_point(position = position_jitter(width = 0.2, height = 0.1)) + 
#         geom_smooth(method = "lm", se = FALSE) + 
#         labs(x = "Student's Term", y = "Cumulative GPA", title = "Cumulative GPA by Student's Term for 178 Randomly Selected Students", color = "Did student withdraw from first course?")
```

2) Course Code Stratification Justification
```{r}
# #Create overall visualization of final cumulative GPA by first course
# Filtered.Data1 %>%
#   ungroup() %>%
#     group_by(`Student Random ID`) %>%
#       slice(n()) %>%
#         filter(max.terms >= 2) %>%
#           dplyr::select(Cum.GPA, `Student Random ID`) %>%
#             rename(Last.Term.Cum.GPA = Cum.GPA) %>%
#               right_join(Filtered.Data1) %>%
#                   group_by(`Student Random ID`) %>%
#                     slice(n()) %>%
#                       ungroup() %>%
#                         ggplot() +
#                         geom_boxplot(aes(x = Last.Term.Cum.GPA, color = `First.Course`)) +
#                         theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
#                         labs(x = "Final Cumulative GPA", y = "Course Code", color = "First Course", title = "Boxplots of Final Cumulative GPA for all Students that \n Passed Filtration Conditions, Colored by First Course")
```


#R ALLOWS FOR DEFINITION OF BINARY OPERATORS

0. Table of Contents

Outline
I.	Introduction
        
  a.	Tutoring, interest in effects that teachers have on their students 
        
        
I started working as a math tutor for a local company called Thompson Tutoring back in the fall of 2019. At first it was just a way to pay rent and keep myself fed, but I came to enjoy it and care about the success of my students. I have about five students that have stuck with me since the beginning, each of whom I see once a week for an hour. Oftentimes I find myself wondering how much of an effect I really have on their mathematics education. As a tutor I am generally employed by a family in two settings:

1. A student that is struggling in their current math course. In this scenario my role is to be the remedial instructor, trying to fill in the gaps in my student's knowledge while simultaneously preparing them for the next test. 
2. A wealthy family looking to get their students ahead in mathematics.
        
  It is very difficult to measure the effect I have had on my students for a variety of reasons, primary among them small sample size. For my SIP I still wanted to combine my newfound addiction to data analysis with education, so Dr. Nordmoe sent me the link to a datset that Professor Andrew Eckstrom of Oakland Community College (hereafter OCC) had posted to a messageboard. The source data contain `r nrow(Gen_Data)` observations in `r ncol(Gen_Data)` variables - "Student Random ID", "Course Code", "Grade", "Faculty Random ID", and "Semester". Each row corresponds to one course taken by one student in one semester between the fall of $2010$ and the winter of $2017$ in the mathematics and chemistry departments of Oakland Community College. "Student Random ID" and "Faculty Random ID" are as expected randomly generated integers than track a given student or faculty member through the file. In total, the file tracks $66164$ students and $268$ faculty. My SIP (in half) is a statistical analysis on new variables I create from Dr. Eckstrom's dataset.

  To explain the goal of my project, we will need to briefly diverge into a personal tangent. I came into Kalamazoo College thinking I would follow the pre-med track and thus need to major in chemistry or biology. Four and a half years later, I am graduating with two majors, among them neither chemistry nor biology. I only took one course each in those department because in one course I found the professor unenthusiastic, and in the other borderline hostile. In the math and physics departments I found faculty that were both more personable and frankly better at teaching their subject. My majors in math and physics developed as I continued to take courses with these faculty. The central question I pose in my SIP is this: how does the first faculty member a student encounters in a department influence a student's outcome? 
        
  The data tracks students through time, so in conjunction with grading policies and course credits obtainable through OCC's website we can track each student's cumulative GPA through their stay in the math and chemistry departments. We will build linear mixed-effects models stratified to avoid some confounding variables to determine an expected final cumulative GPA for each student that passes a few conditions, then determine the difference between observed and expected final cumulative GPA. I call this statistic $\Delta C$, short for difference in cumulative GPA. Where $C_O$ is a student's observed final cumulative GPA and $C_E$ is the final cumulative GPA predicted by the linear mixed-effects model:
  
\begin{equation}
\Delta C = C_{O} - C_{E}
\end{equation}

We will then take the mean $\Delta C$ among all students that started with a certain faculty, which can be thought of as the average benefit to final cumulative GPA a student that started with the professor had with respect to the mean student. Finally, after running significance tests we will create faculty rankings based on their (1) the overall mean $\Delta C$, and (2) mean $\Delta C$ per course they taught. I promise there will be pretty pictures along the way.
        
        One of the conditions required to create the $\Delta C$ statistic is that students take multiple terms in the math and/or chemistry departments. This happens to be a pretty large requirement, especially for a community college such as OCC. I have pulled public data from DataUSA regarding OCC's enrollment statistics for all years available, which ended up being the years $2012$ through $2019$. The following tables and graph summarise this enrollment data.
        
```{r Load OCC Enrollment, include = FALSE}
##Load in OCC_Enrollment from 2012 to 2017. I am missing years 2010 - 2012. Data from https://datausa.io/profile/university/oakland-community-college
OCC_Enrollment <- read_csv("OCC_Enrollment.csv")
```

```{r Create Enrollment Tables and Graph, echo = FALSE}
##Create Enrollment table, obtained by manual data entry from 
OCC_Enrollment_Table <- OCC_Enrollment %>%
                          mutate(Total = `Full Time` + `Part Time`)

kable(OCC_Enrollment_Table, caption = "OCC Fall Enrollment by Year, 2012-2019", padding = 3)

##Create mean enrollment statistics
Mean.OCC_Enrollment <- OCC_Enrollment %>%
                        mutate(Total = `Full Time` + `Part Time`) %>%
                          summarise(`Mean Full Time` = mean(`Full Time`), `Mean Part Time` = mean(`Part Time`), `Mean Total` = mean(Total))

kable(OCC_Enrollment_Table, caption = "OCC Mean Enrollment Statistics", padding = 3)

##Legend creation method from https://community.rstudio.com/t/adding-manual-legend-to-ggplot2/41651
colors <- c("Full Time" = "blue", "Part Time" = "red", "Total" = "green")

OCC_Enrollment_Plot <- OCC_Enrollment %>%
                        mutate(Total = `Full Time` + `Part Time`) %>%
                            ggplot(aes(x = Year)) +
                            geom_line(aes(y = `Full Time`, color = "Full Time")) +
                            geom_line(aes(y = `Part Time`, color = "Part Time")) +
                            geom_line(aes(y = Total, color = "Total")) +
                            labs(x = "Year", y = "Enrollment (Number of Students)", color = "Legend", title = "OCC Enrollment Vs. Year") +
                            scale_y_continuous(limits = c(0,30000), breaks = seq(0,30000, 5000)) +
                            scale_color_manual(values = colors)
```
```{r OCC Enrollments Plot, fig.cap = "OCC Enrollments By Year for Full Time, Part Time, and Total Student Body"}
OCC_Enrollment_Plot
```
    
  Both full and part time enrollment monotonically declining over the reported years. Unfortunately, professor Eckstrom's dataset begins in the year 2010, so there are two relevant years we do not have data regarding enrollment. Regardless, between $2012$ and $2019$ on average only `r Mean.OCC_Enrollment[[1]]/Mean.OCC_Enrollment[[3]]*100`$\%$ of students were enrolled full time. According to the National Student Clearinghouse, community college dropout rate is a function of time enrolled - therefore part time students are less likely to take multiple courses in the same department due to a lower course load per term. (https://www.studentclearinghouse.org/nscblog/research-center-snapshot-report-showcases-yearly-success-and-progress-rates-for-fall-2012-freshman-class/) As we will see, requiring students to have two *graded* terms drastically reduces the sample size, but still allows for many faculty to be evaluated by the $\Delta C$ metric. 
  
  Another difficulty in light of a second filtration condition - students must not withdraw from the first course they take in the math or chemistry departments. It would be possible to start the models at the first graded term, but if a student withdraws from a course then takes it again, their grade may be artificially inflated and thus not serve as an honest intercept. As a result, the expectation on $\Delta C$ becomes negative rather than zero. Requiring students to receive a standard letter grade ensures that the linear mixed effects models have a consistent starting point, and that individual teachers are evaluated fairly. What if a certain faculty member had a high withdraw rate due to their course being hard, but prepared their students well for future courses and thus also had a high mean $\Delta C$? Simply put, is there a correlation between a faculty's mean $\Delta C$ and their withdraw rate? $\Delta C$ is blind to any effect outside the realm of cumulative GPA, so we must introduce a new statistic to capture variance in withdraw rates. Fortunately, the calculation of withdraw rate for individual faculty does not require filtration so the sample sizes will be much larger and thus the results much more accurate. As with the $\Delta C$ rankings, we will construct rankings using the difference between the overall withdraw rate and a faculty's observed rate for: (1) the overall case, and (2) by course taught. To be clear, where $W_O$ is a faculty's mean withdraw rate and $W_E$ is the germane expected rate, this statistic will be calculated as:

\begin{equation}
\Delta W = W_E - W_O
\end{equation}

  Note that we have reversed the direction of difference with respect to $\Delta C$. We have chosen orientations such that positive $\Delta$ values correspond to "good" effects for ease of human interpretation. Additionally, note that while an observation on $\Delta C$ corresponds to one student, an observation on $\Delta W$ is specific to a faculty member. To reiterate, we will average over the $\Delta C$ values among those students that started with a professor to obtain a value unique to that faculty.
        
        b.	The Importance of Data-Driven decision making
        
  Dr. Tobochnik has drilled into my mind that any conclusions we make about the world around us need to be supported by data. Theory is nice, but even if the math is pretty and complete there is no guarantee the assumptions we made to create the model were correct. I will therefore be conservative in speculation and only make conjecture regarding trends demonstrably present in the data. This is in accordance to the dogma of facts-based decision making. In the context of corporate decision making, David Fradin says "The reasons why facts drive better decisions include – the ability of the computer to find non-obvious answers, ability to crunch large datasets very fast, can take into account hundreds of market variables and ability to adopt a methodical repeatable process for analysis." (“Organizing and Managing Insanely Great Products” by David Fradin with RN Prasad) Reproducibility and objectivity are key to any good data analysis, and are necessary when seeking optimal answers to any scientific problems. (https://plato.stanford.edu/entries/scientific-reproducibility/) 
  
  Any empirically derived model will have error. Statistical methods such as hypothesis testing employ significance tests to determine whether a measured effect is "real". Such tests remove the human element by determining the likelihood a measured result could be generated by random chance under the assumption that the effect is "fake". (Laura M. Chihara and Tim C. Hesterberg, Mathematical Statistics with Resampling and R).
  
  Even if the data collected are accurate and complete, confounding variables may still be present in any models created based on choice of groups and variables. According to Glenn Hymel, "If an extraneous variable is not appropriately controlled, it may be unequally present in the comparison groups. As a result, the variable becomes a confounding variable." (Research Essentials for Massage in the Healthcare Setting, Glenn M. Hymel, in Clinical Massage in the Healthcare Setting, 2008) 
        
                i.	Importance of Data Science
  As of 4/6/2021, Glassdoor ranks Data Scientist as the second best job in America, with a median starting salary of $\$113,736$. (https://www.glassdoor.com/List/Best-Jobs-in-America-LST_KQ0,20.htm) Data Scientists use techniques from mathematics, statistics, and computer science to create and use tools for the purpose of collecting data and analysing potentially hidden trends stored within. (Grolemund and Wickham, 2017) The world around us is being deluged with a treasure trove of data, waiting to be analysed. In machine learning, programmers feed trial data to their AI, which then creates a model. This model is then used to predict the distribution of future data. According to 
                        
        c.	R For Data Science (Textbook)
        
  When I began this project, I had no idea what I was doing. For the first month or so I was using some retrospectively atrocious C++ inspired hard-coding methods, one remnant of which I have included commented out in the code that follows this paper. When I showed Dr. Nordmoe one copious script I had created to determine the total number of students/faculty in the data, he told me to install the tidyverse and call the unique() function. In approximately twenty seconds the process I had spent several hours on had been recreated. Moreover, the runtime was spectacularly improved. I was then told to read chapter five of \textit{R For Data Science} by Hadley Wickham and Garrett Grolemund. Chapter five is a guide to the R package dplyr, one of the many packages from the tidyverse I used in the construction of this project. As it happens, I didn't need to reinvent the wheel.  A lot of smart people have built a lot of nice functions to make R a phenomenal work environment for data analysis. \textit{R For Data Science} was immensely helpful to my project, and was my primary guide to working with tidyverse functions beyone Dr. Nordmoe's data science course. The full book can be found online for free at https://r4ds.had.co.nz/. I would like to relay the author's wishes for support of the Kakapo parrot, a critically endangered New Zealand native, at https://www.doc.govt.nz/kakapo-donate.
        
        d.	Data Science Course
        
  In order to help learn skills for this project, I enrolled in MAT-295 (Introduction to Data Science) at K. The course used the same free textbook that I read for this project, and so is an excellent offering for all students regardless of means. MAT-295 covered tidying and presenting data, text manipulation by means of regular expressions, manipulation of categorical variables, basic date/time conversion methods, creation of simple linear models, and a brief introduction to programming in R among other subjects. Course material does not include any mathematical theory such as probability or mathematical statistics, though the math department offers courses in those subjects in the Winter/Spring each year.  Introduction to Data Science does not require probability or math stats for enrollment, but statistics in particular is a powerful weapon in data analysis that we will be making heavy use of in this paper. MAT-295 was enormously helpful in giving me practice manipulating and presenting data using the tidyverse approach, which comprises at least half of the following project.
        
        
        e.	Statistical Background
        
 Probability theory is the backbone of any statistical method. In particular, the central limit theorem is an amazing piece of technology that guarantees when sufficiently many nicely behaved i.i.d. random variables are added, their sum will have a Gaussian distribution. (https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html) Where the i.i.d. random variables $X_i$ with population mean $\mu$ and standard deviation $\sigma$:

\begin{equation}
\lim_{k \to \infty} \sum_{i=1}^k X_i ~  N(k\mu, \frac{\sigma^2}{k})
\label{Large N CLT}
\end{equation}
        
(CITE PROBABILITY BOOK)        
        
 Clearly a perfectly Normal distribution would require the sum of infinitely many random variables. In reality it is quite tedious to attempt to collect an infinite number of values to add together, so it is common practice to set a lower bound of observations before data is considered approximately Normal. This bound is often considered to be between $30$ and $40$ depending on the author, but this value can decrease arbitrarily if the random variables themselves have Normal distribution. (https://www.investopedia.com/terms/c/central_limit_theorem.asp#:~:text=Sample%20sizes%20equal%20to%20or,characteristics%20of%20a%20population%20accurately., https://guides.fscj.edu/Statistics/centrallimit) Unfortunately, final cumulative GPA is inherently discrete, and withdraw rate is binomial. As such we must use an approximation to the central limit theorem. In my analysis, I use the more conservative minimum of 40 cases per stratum. Where the random variables $X_1$ through $X_n$ are i.i.d. each with sample mean $\bar{x}$ and standard error $s = \frac{\sigma_x}{\sqrt{n}}$,

\begin{equation}
X = \sum_{i=1}^n X_i \sim N(n\bar{x}, s^2)
\label{Small N CLT}
\end{equation}
(CITE MATHSTATS BOOK)

  Like the Gaussian distribution, Student's t distribution is heavily used in significance testing. For a sample of $N$ i.i.d. random variables with sample mean $\bar{x}$, a-priori population mean $\mu$, and sample standard deviation $\sigma_x$, the t statistic is defined to be:

\begin{equation}
t = \frac{\bar{x} - \mu}{\sigma_x / \sqrt{N}}
\label{T  Statistic}
\end{equation}
  
The t distribution is designed to enable the analysis of small sample size, in accordance to a few conditions. According to CITE STATISTICS TEXTBOOK, these are:
  
1. Data must be collected from random variables that come from a continuous scale. This assumption is relaxed as the number of possible values increases. As I will show later, both $\Delta C$ and $\Delta W$ have sufficient possible values per case to pass this condition.  
2. Data must be collected by way of a simple random sample. From what I can tell, Professor Eckstrom's file contains a census, and so is immune to this requirement.
3. Random variables must resemble the Normal random variable under combination. This assumption protects against pathological cases such as the Cauchy distribution that has undefined variance. The possible range for $\Delta C$ is given by the set $(-4,4)$ in units GPA points, and the possible range for $\Delta W$ is given by $(-1,1)$. The empirical mean $\Delta C$ distribution will ultimately have range $()$, as $\Delta W$ will have empirical range $()$ **[FILL THIS IN]**
4. The sample must have a reasonable sample size. The central t-statistic is undefined for $N = 1$, and gradually converges to the standard normal z-statistic as $N$ increases. The smaller the original sample size, the higher the requisite t-value for significance. The only requirement on sample size is that there are enough data points to calculate the sample standard deviation. For safety, in my analysis I will mandate a minimum sample size of 8 students for calculation of both $\Delta C$ and $\Delta W$.

  We will use the t distribution to perform significance tests at the $\alpha = 0.05$ level, where $\alpha$ is the probability of rejecting the null hypothesis when it is true. (CITE STATISTICS TEXTBOOK) $\alpha$ is also known as the type I error. (CITE STATISTICS TEXTBOOK) For any significance test, we first define the null and alternate hypotheses. Faculty effects on future cumulative GPA can be either positive, zero, or negative, as can their effect on withdraw rate. There is no information outside the data that lead us to believe an individual faculty member will have a positive or negative $\Delta W$ or mean $\Delta C$, so we will use a two sided hypothesis. Where $\theta$ is the true value of a statistic (i.e. the $\Delta W$ or mean $\Delta C$ which would be calculated after an infinite number of students for one faculty):

\begin{align}
H_0: \theta = 0 \\
H_A: \theta \neq 0
\label{Hypotheses}
\end{align}

  Just as it would be impossible to collect infinite variables to ensure perfect Normality, we cannot mathematically prove from a sample which provides an estimate $\hat{\theta}$ that the null hypothesis is false. However, we can create a range of values where $\theta$ is expected to lie based on the observation of $\hat{\theta}$. This concept is called a confidence interval. In my analysis, I will construct such confidence intervals using two techniques from mathematical statistics:

1. The one sample t-test. As the name suggests, the one sample t-test utilizes the t distribution. Where $\alpha$ is the probability of type I error and $t_{\alpha/2, N-1}$ is the t statistic's $\frac{\alpha}{2}$ quantile for N degrees of freedom, the derivation of the $100(1 - \alpha)\%$ confidence interval follows: ##CITATION http://www.ams.sunysb.edu/~zhu/ams571/Lecture11_571.pdf
\begin{align*}
P(-t_{\alpha/2, N-1} \leq t \leq t_{\alpha/2, N-1}) = 1 - \alpha \\
P(-t_{\alpha/2, N-1} \leq \frac{\bar{x} - \mu}{\sigma_x / \sqrt{N}} \leq \leq t_{\alpha/2, N-1}) = 1 - \alpha \\
P(\bar{x} - t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}} \leq \mu \leq \bar{x} + t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}) = 1 - \alpha\\
\end{align*}
\begin{equation}
100(1-\alpha)\% \quad CI = [\bar{x} - t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}, \bar{x} + t_{\alpha/2, N-1} \frac{\sigma_x}{\sqrt{N}}]
\label{T Confidence Interval}
\end{equation}

2. The bootstrap. The bootstrap principle is a powerful tool that can allow fo significance testing with fewer assumptions than the one sample t-test. The general process for the bootstrapping of a population parameter $\theta$ using a given sample containing an estimate $\hat{\theta}$ follows:

1) Create an empty vector $V_{boot}$ with length equal to a large number $n_{sim}$
2) Create a sample with replacement $S_{boot}$ from the original sample whose length is equal to the length of the original sample. In this example this length is N.
3) Store the observed mean of $\hat{\theta}$ from (2) into $V_{boot}$
4) Repeat (2) and (3) $n_{sim}-1$ times.

The resulting vector $V_{boot}$ contains the bootstrap sampling distribution, which is an estimate for the sampling distribution. Due to $\ref{Small N CLT}$ As such, we can use the bootstrap distribution to create a confidence interval for $\theta$. R's built-in quantile() function is used to create $95\%$ confidence intervals for $\hat{\theta}$ from this bootstrap sampling distribution. The $100(1-\alpha)\%$ bootstrap CI is given by the set $[\textbf{quantile}(V_{boot}, .025),\quad \textbf{quantile}(V_{boot}, .975)]$. 

1. COURSERA

  As previously stated, when I first began this project I had never taken a programming course. A large chunk of the project boiled down to creation and manipulation of new variables through the tidyverse method, but there was still many pieces of knowledge I lacked to make the analysis run. Coursera offers many excellent online courses on a wide variety of subjects, among them computer science and data science. To learn the programming skills I required to analyse Professor Eckstrom's data, I enrolled in and completed the first two modules of the *Data Science: Foundations Using R Specialization* offered by Johns Hopkins University on the Coursera platform: (1) The Data Scientist's Toolbox, and (2) R Programming. My certificates for these courses along with the grade I recieved are available in the appendix. I dedicate the following section as a summary of my learning from these courses. 
        
        f.	Coursera
        In this project my goal was to create a method of modeling the effects teachers have on their students. To do this, I needed to 
        g.	COVID Difficulties
II.	Base R
        a.	Atomic Data Types
CITATION FOR ENTIRE SECTION IS R Programming for Data Science by Roger D. Peng [version 9/3/2020]    

\subsection{The Atomic Data Types}
  In the R language, there are five atomic data types or class invariants. 
1. Numeric data, often also referred to as double-precision data, are real numbers. They can have up to fifteen significant digits, but generally are limited by observation to many fewer. In RMarkdown it is common practice to include the option "digits = k", which restricts the printing of doubles to k decimal points. Numeric data have the additional option of having the value NAN (for not a number), which most often corresponds to undefined in mathematical language. For instance, $\log(-1)$ yields the output `r log(-1)`. 
2. Integer data can only be real integers, and are useful in programming aspects such as indexing and preventing rounding errors. 
3. Character data, or strings, are the broadest class. Any set of alphanumeric+ characters can be represented in a *string* as a character object. Packages such as stringr are available to manipulate character variables with plentiful functions equipped for regular expressions.  
4. One of if not the most important data type for programming in R is the logical, or Boolean. A logical datum either has the value `r TRUE` or `r FALSE`. When two logicals $b_1$ and $b_2$ are compared over equality - i.e. $b_1 = b_2$ - R will output `r TRUE` if both data have the same value. We will momentarily investigate the behavior of logical data. As will be apparent when discussing subsetting, the ability to programmatically compare large quantities of logical data allows for fast and convenient indexing, sorting, and filtering.
5.The final class in the R language is complex, an element of which has the form $a + bi$, where $a$ and $b$ are numeric data. I personally never used complex data beyond the single Coursera quiz that asked for the identification of a complex datum among a list of options.

  One last important note - data can be missing in any class. A missing value is denoted with NA (for not available). The NA can therefore be represented in each class, and converted using the as.xxx() family of functions. R treats NA values as if they did have a value, but that the value is not known. For instance - the mean of the numeric vector c(.5, 2, NA) is `r mean(c(.5, 2, NA))` with class `r class(mean(c(.5, 2, NA)))`, because R cannot determine the third element's effect on the average but assumes it was a number due to how vectors are defined.
  

\subsection{Vectors}
  On that note, let's discuss one of the primary weapons of the R language - vectors. R can store any sequence of data from a single class as a vector and do operations on it. Such operations are performed element-wise unless else specified. The vector's class will be the same as that of its constituent data. There are several ways to create a vector including the concatenation operator c(), the sequence generator seq(), the replicate function rep(), the colon operaton :,  and extracting a column out of a dataframe either by using pull() or by subsetting.  Individual elements can have names, for instance:
  
```{r Demo Named Vector, echo = TRUE}
vector.names <- c("Jimmy", "Sammy", "Katrina")
my.vector <- c(17, 18, 17)
names(my.vector) <- vector.names

my.vector
```

  Vectors can sometimes be changed from one class to another through a process called coercion. The family of functions as.class() performs this conversion. Below is some sample output demonstrating the coercion capabilities of different classes using three demo vectors: num for numeric, chr for character, and log for logical:

```{r demo vectors, echo = TRUE}
##Define demo vectors. num is a numeric vector, chr is a character vector, and log is a logical vector.
num <- c(-1, 0, 1, 2)
chr <- c("a", "b", "0", "FALSE")
log <- c(TRUE, FALSE)

##Coercion of numeric vector to character
as.character(num)
##Coercion of numeric vector to logical
as.logical(num)
##Coercion of character vector to numeric
as.numeric(chr)
##Coercion of character vector to logical
as.logical(chr)
##Coercion of logical vector to numeric
as.numeric(log)
##Coercion of logical vector to character
as.character(log)
```
When coercing a numeric object into the character class, each element becomes represented as a string and loses its ability to partake in arithmetic operations. Conversion from double to logical is possible, but one must be careful - only zeroes get mapped to `r FALSE`, while all other numbers get mapped to `r TRUE` unlike in some other languages. Character vectors are the least coercible. While each unicode character is stored in memory as a very large integer, R doesn't want to return these for user safety. As a result, coercion of a vector from the character class to the numeric class will result in a missing value unless R thinks the string already represents a number. Strangely, `r TRUE` / `r FALSE` values stored in a character vector will be mapped to missing values in this coercion, even though they can be coerced into numerics. Calling as.logical() will similarly fail to notice $0$ as `r FALSE`.  If a vector's class is not clear, the class() method is an excellent tool, and can also be used programmatically to obtain the class(es) of an object. In my project I used the as.class() functions rather rarely, but they serve to elucidate the inner workings of the classes themselves. 



\subsection{Logic}
  Before moving on to subsetting vectors and lists, we first take a foray into R's evaluation of logical statements. The most straightforward way to do this is to look at some examples.

```{r Logical Practice 1, echo = TRUE}
##Practice with logical data
TRUE == TRUE
TRUE != TRUE
FALSE < TRUE
FALSE > TRUE
```
The double equals is a binary operator that outputs `r TRUE` i.f.f. both sides of the operator have the same evaluation. The less than and greater than operators work in a similarly intuitive manner. $!$ is the negation operator, which inverts the meaning of the logical operator it is paired with. Additionally a logical datum itself can be negated, such that !`r TRUE` evaluates to `r !TRUE`. Note that the expression `r FALSE` $<$ `r TRUE` evaluates to `r TRUE` and `r FALSE` $>$ `r TRUE` evaluates to `r FALSE` because R considers `r FALSE` to be the number $0$ and `r TRUE` to be the number $1$. This feels less than intuitive because as seen above any nonzero number coerces to `r TRUE` under the method as.logical(). Just as $0 < 1$ gets a `r TRUE`, when character data are compared over the binary operators, R considers the lexicographic ordering. This is worth exploring, but not relevant to my project. More pertinent is the evaluation of vector comparisons. Let's do another example.

```{r Logical Practice 2, echo = TRUE, warning == FALSE}
##Define some vectors to compare with different lengths.
##Vector a contains the sequence 1, 2.
a <- c(1, 2)
##Vector b contains the sequence 1, 2, 3, 4, 0.
b <- c(1:4,0)

##Compare a and b over equality
a == b
##Compare a and b over a formula
a + 2 == b
```

  R evaluates vector comparisons element-wise and outputs a vector with length equal to the longest input vector. If the compared vectors are not the same length, R will repeatedly replicates the shorter vector end-to-end until the lengths are equal. In the above example, R changed the expression $\{1,2\} == \{1,2,3,4,0\}$ into $\{1,2,1,2,1\} == \{1,2,3,4,0\}$. The shorter length of $2$ does not evenly divide the longer length $5$ so R throws a warning, but continued with evaluation regardless. The output of any logical comparison is a logical vector, the previous comparisons were simply a special case where the input vector lengths were each one. $a == b$ returns a five element output vector, because the longer of the two input vectors had five elements.
  
  The final hitch to logical evaluation in R are conditional expressions. R is equipped with three basic binary conditions: the and statement "&", the or statement "|", and the xor function xor(). A more complicated binary operator I use for the data analysis is the in operator "%in%", which checks whether the left object is within a range of values specified by the right object. These conditions work just as they do in set theory. Or has low priority in R's evaluation queue while and has high priority so that conditions evaluate in accordance to the generally desired order of operations. Just like in mathematics parenthetical statements are evaluated before their exterior for additional control over the evaluation queue. R allows for the user to define their own logical operators, a privilege I used one time to create a negated version of the "%in%" operator called "%notin%".

\subsection{Subsetting}

  CRAN outlines three operators for subsetting sequences in R: (https://cran.r-project.org/doc/manuals/R-lang.html#Indexing)
1. The single bracket operator $X[i]$, where $X$ is some array and $i$ is a set of indecies. Single bracket subsetting is generally used with vectors to extract the $i^{th}$ elements of X. When used on a list, the single bracket will return the $i^{th}$ elements as a list. The user can supply indecies either as an integer vector or a logical vector. To see how this works, examine the sample output below.

```{r}

```

2. 
3. 
.  Here elements are extracted from the demo vector used earlier.
  
  Lists are another common form of array that can be thought of as a hierarchical pyramid that can hold any type of object, including other lists. Just like vectors, list elements can have names. Lists are excellent organizational tools because they are not beholden to the stipulation that each element have the same class invariant; a two element list may for example contain one function and one dataframe. Because lists may contain a variety of information, a couple good ways to obtain a sense of the contents are the str() and  glimpse() commands. I am personally partial towards glimpse due to its good behavior with other dplyr verbs. 

```{r List Example, echo = TRUE}
##Create dummy functions for list
test.mean <- function(x){mean(x)}
test.median <- function(x){median(x)}
##Create list with multiple layers
my.list <- list(list(test.mean, list(test.median)), list(test.median), 100)
##Print the list
my.list
```
The double bracket operator is at its best when working with lists


        b.	Lists
        
 The special double bracket operator [[ ]] extracts 
        
        c.	Arrays
                i.	Named Lists
                ii.	Matrices
                iii.	Data Frames
        d.	Restricted phrases
        e.	Directory Management
                i.	getwd()
                ii.	setwd()
                iii.	conjunction with ReadR and loops
III.	R Programming
        a.	Vectors
                i.	Coercion
                ii.	rep()
        b.	Logic
                i.	Simple evaluation 
                ii.	Vectors evaluate logic element-wise
        c.	Subsetting
                i.	$
                ii.	[ ] – returns as same data type
                        1.	[k]
                        2.	[a, b]
                        3.	[ ,b]
                        4.	[a, ]
                iii.	[[k]] – returns as vector with class equal to the class of its components
        d.	Simulation
                i.	
        e.	Control Structures
                i.	Syntax
                ii.	If-else
                iii.	Loops
                        1.	For
                        2.	While
                        3.	In-loop commands
                                a.	Repeat
                                b.	Next
                                c.	Break
                iv.	
                v.	Common applications with vectors
        f.	Writing Functions
        g.	Apply Functions
                i.	lapply
                ii.	sapply – one dimensional data
                iii.	others not used
        h.	
        i.	The Scoping Rules – why R is great
IV.	The Tidyverse arsenal
        a.	The Packages
                i.	Dplyr
                1.	filter()
                2.	mutate()
                3.	group_by()
                4.	select
                5.	arrange
                6.	join commands
                        a.	left_join()
                        b.	right_join()
                        c.	semi_join()
                7.	summarise()
        b.	GGPlot2
                1.	Aesthetics
                        a.	X, Y
                        b.	Color, Fill
                        c.	
                2.	geom_point
                3.	geom_histogram
                4.	geom_bar
                5.	geom_boxplot
                6.	geom_smooth
                        a.	method = LOESS
                                i.	locally estimated scatterplot smoothing 
                                ii.	https://www.epa.gov/sites/production/files/2016-07/documents/loess-lowess.pdf
                        b.	method = LM
                                i.	linear model
                7.	geom_vline, geom_hline
                8.	geom_segment
                                iii.	ForCats
                1.	factor()
                2.	fct_reorder(x,y)
        c.	Tibble
                1.	tibble()
        d.	Piping
V.	Coursera Work
        a.	Overview 
                i.	Videos
                        1.	Automated for DSTB to keep material up to date
                                a.	Transcript + Slideshow
                        2.	Non-automated for R-Programming
                ii.	Readings
                        1.	Some independent readings
                        2.	All videos have transcripts
                iii.	Book – R Programming for Data Science by Roger D. Peng [version 9/3/2020]
                        1.	https://bookdown.org/rdpeng/rprogdatascience/ 
                iv.	Quizzes
                v.	Programming Assignments
                        1.	Graded by quiz
                        2.	Graded by peers
        b.	The Data Scientist’s Toolbox – Show Certificate
                i.	Topics
                        1.	Week 1
                                a.	Introduction to Data Science
                                b.	Getting help on projects appropriately
                                        i.	Rules of Decorum
                                        ii.	Forums (I primarily used StackOverflow)
                                c.	The Data Science Process
                                        i.	Ask a question
                                        ii.	Obtain Data for use
                                        iii.	Analyse Data 
                                        iv.	Communicate Results
                        2.	Week 2
                                a.	Installing RStudio,
                                        i.	How to install packages
                                b.	RStudio tour
                                c.	Superfluous week considering usage of R and RStudio in probability and math stats
                        3.	Week 3
                                a.	Version Control
                                        i.	GitHub
                                                1.	GitBash – I didn’t end up using my personal copy of RStudio in favor of RStudio Cloud offered through the college. GitBash was therefore ultimately unnecessary
                                        ii.	Linking GitHub to RStudio
                                                1.	Commit
                                                2.	Stage
                                                3.	Push/Pull
                                                4.	Also covered in Data Science course
                                        iii.	Merge Conflicts
                        4.	Week 4
                                a.	R Markdown
                                        i.	How to create reports with knitr
                                        ii.	R code chunks
                                                1.	How to label
                                                2.	Options
                                                3.	Inline and standalone
                                        iii.	LaTeX
                                b.	Asking Data Science Questions
                                c.	Experimental Design
                                d.	Big Data
                                        i.	Three V’s
                                                1.	Volume
                                                2.	Variety
                                                3.	Velocity
                                        ii.	Benefits
                                                1.	Can ask questions that are more difficult to statistically infer answers (higher n)
                                                2.	Can ask new questions due to variety of data available (even if it is really nasty)
                                                        a.	Can identify hidden correlations – data mining
                                                3.	New data that measures the same effects can confirm or reject inferences previously held
                ii.	Course Project
                        1.	Verification that students had properly connected RStudio to GitHub and learned how to create a GitHub repository and commit changes to it.
                        2.	Push a markdown file to a repo
                        3.	Fork a repo
                        4.	Grade 2 peers
        c.	R Programming – Show Certificate
                i.	Week One
                        1.	Topics
                                a.	Background Material
                                        i.	For those students that hadn’t previously taken The Data Scientist’s Toolbox
                                        ii.	Links to instructor’s book
                                        iii.	Pre-course survey
                                        iv.	Directory management
                                                1.	getwd()
                                                2.	setwd()
                                                3.	list.files()
                                                4.	Restoring directory at end of program
                                b.	History of R
                                c.	Reiteration of how to get help
                                d.	Data Types
                                        i.	Class Invariants
                                                1.	Numeric
                                                2.	Character
                                                3.	Complex
                                                4.	Integer
                                                5.	Logical
                                                6.	Raw
                                        ii.	Factors – categorical variables
                                                1.	Used for sorting by levels
                                        iii.	Arrays
                                                1.	Matricies
                                                2.	Tibbles
                                        iv.	Vectors
                                                1.	Only have one class invariant
                                        v.	Lists
                                                1.	Can contain multiple class invariants
                                e.	Read in Data to R
                                        i.	read.table()
                                        ii.	read.csv()
                                        iii.	Not useful in context with readR – I used read_csv()
                                f.	Subsetting
                                        i.	Single Brackets - [a:b, c:d]
                                                1.	Returns object of same class as original
                                                2.	Can select more than one object using row / column indexing
                                                        a.	Numeric index
                                                        b.	Logical index
                                        ii.	Double Brackets - [[a]]
                                                1.	Extract element from list / data frame
                                                2.	
                                        iii.	$ operator
                                                1.	Return elements by name, if objects have names
                                        iv.	Removing NA’s
                                                1.	is.na()
                                                2.	is.nan()
                                g.	Vectorized Operations
                                        i.	Element wise operations
                        2.	Week One Swirl Assignments
                                a.	What is Swirl?
                                        i.	R Package that teaches R functionality and programming w/in R. 
                                        ii.	Review and Application of concepts taught in Videos/Readings from previous week
                                        iii.	Individual Modules = Programming Assignment
                                        iv.	Links back to Coursera for credit by registered email
                                b.	Week One Modules
                                        i.	Basic Building Blocks
                                                1.	Class Invariants
                                                2.	Operations
                                                3.	Saving values to objects
                                        ii.	Workspace and Files
                                                1.	Directory Management
                                        iii.	Sequences of Numbers
                                                1.	Working with lists
                                        iv.	Vectors
                                                1.	Working with vectors
                                        v.	Missing Values
                                                1.	Generate NA’s
                                                2.	Use is.na() and is.nan() to identify indices 
                                        vi.	Subsetting Vectors
                                                1.	[]
                                                2.	[[]]
                                                3.	$
                                        vii.	Matrices and Data Frames
                                                1.	Using names()
                                                2.	Multidimensional subsetting
                                                3.	Quiz – 20 Q’s
                        3.	No Programming Assignment
        ii.	Week Two – Programming With R
                1.	Topics
                        a.	Control Structures
                                i.	For loops – Run for a predetermined number of iterations
                                ii.	While loops – Checks a condition
                                iii.	Repeat loops – Infinite loop
                                iv.	In-Loop Commands
                                        1.	Next
                                        2.	Break
                                v.	If-else expressions
                        b.	Functions
                        c.	Scoping Rules
                        d.	Coding Standards
                                i.	Write code using a text editor
                                ii.	Proper indentation
                                        1.	Change spacing in RStudio
                                        2.	Cmd + I
                        e.	Dates, Times, and DateTimes
                                i.	Sys.time() gets datetime which has following classes:
                                ii.	POSIXct - Seconds since 12:00 AM  1/1/1970
                                iii.	POSIXlt - List of date / time information
                                        1.	Nine different pieces of information about time in question
                                iv.	Convert using as.posixct(), as.posixlt()
                2.	Week Two Swirl Assignments
                        a.	Logic
                                i.	Lots of practice problems using Booleans
                                ii.	Determine what boolean complicated logical expressions evaluate to
                        b.	Functions
                                i.	Create functions that take various levels of input
                                        1.	Required input
                                        2.	No input
                                        3.	Optional input
                                ii.	return()
                        c.	Dates and Times
                                i.	Play with conversion between DateTimes, POSIXct, and POSIXlt. 
                                ii.	Use subsetting to obtain information from POSIXlt
                3.	Programming Assignment One
                        a.	Directory Management – 332 CSV files in a file called specdata each containing three variables. One from each of 332 pollution monitoring stations around the US. Names of files are a number from 001 to 332 which indicate which monitoring station data are from. No codebook for location.
                                i.	date
                                ii.	sulfate – sulfate in air (micrograms per cubic meter) on that date
                                iii.	nitrate – nitrate in air (micrograms per cubic meter) on that date
                        b.	Goal: write three functions that analyse the datasets:
                                i.	pollutantmean(directory, pollutant, id = 1:332) – calculates the mean of a pollutant given the directory where the csv’s are found, the pollutant over which to average, and a subset of monitoring stations to collect data from.
                                ii.	complete(directory, id = 1:332) – Determines the total number of completely observed cases over a subset of monitoring stations.
                                iii.	corr(directory, threshold = 0) – Determines the correlation between  sulfate and nitrate pollution for all monitoring stations that have a number of complete observations at least equal to the threshold specified
                        c.	Graded by Quiz – Include functions in appendix
        iii.	Week 3 – Loop Functions and Debugging
                1.	Topics
                        a.	Apply functions
                                i.	lapply – given a list and a functions, applies the function to each element of the list. 
                                ii.	apply – apply a function across the margins of an array. For example, a rectangular MxN matrix would have margin options 1 or 2. 1 would specify the function to run over rows, while 2 would apply over columns.
                                iii.	sapply – smart wrapper for lapply, this is the bread and butter. Returns vector, list, or matrix if appropriate.
                                iv.	mapply – Multivariate apply. Not used in my project.
                                v.	tapply – Apply a function over subsets of a vector/list given indecies
                        b.	split()
                        c.	Debugging Tools
                                i.	traceback() – prints out the function call stack after an error occurs
                                ii.	debug() – allows line by line execution of a function
                                iii.	browser() – suspends execution of a function and puts R into debug mode
                                iv.	trace() – allows debugging code that does not affect functionality. Generally used in conjunction with browser()
                                v.	recover() – Whenever an error is encountered in the function,  open the browser.
                2.	Week 3 Swirl Assignments
                        a.	lapply and sapply
                                i.	Make lists and functions, then apply the functions over the lists
                        b.	vapply and tapply
                                i.	vapply was not covered in lecture, but it is no more complicated than any other apply. It applies a function specifically over a vector.
                                ii.	More of the same.
                3.	Programming Assignment Two – Lexical Scoping
                        a.	Introduction to cacheing and superassignment. 
                        b.	Goal: Write two functions:
                                i.	makeCacheMatrix: Creates a matrix that can cache its inverse (for an invertible matrix)
                                ii.	cacheSolve: Computes the inverse of the matrix generated by makeCacheMatrix. If this has already been calculated, call the cache instead.
                        c.	Include functions in Appendix
                        d.	Peer- Graded through GitHub, so must grade four peers.
                        e.	https://github.com/firstrider55/ProgrammingAssignment2 
        iv.	Week Four – Simulation and Profiling
                1.	Topics:
                        a.	
                        b.	str() – Alternative to summary(). Displays the contents and metadata of an input.
                        c.	ls()
                        d.	Generating random numbers – used all the time in probability and mathstats
                        e.	Simulating. Specifically used lm() and plot() to create a linear model for two variables. I use linear mixed-effects models in my study
                        f.	sample()
                        g.	Efficiency
                                i.	system.time()
                                ii.	Rprof()
                                        1.	Checks the function call stack at regular intervals and records which function is being executed
                                iii.	summaryRprof() takes Rprof() as an input, and determines how much time is being spent in each function. Two methods for data normalization:
                                        1.	“by.total” yields proportion of total run time
                                        2.	“by.self” does the same but first subtracts time elapsed by already executed functions – the last function (if executed only once) will have proportion 1.
                2.	Week Four Swirl Assignments
                        a.	Looking at Data
                                i.	str()
                                ii.	dim()
                                iii.	nrow() and ncol()
                                iv.	object.size() – returns amount of memory taken by object
                                v.	names()
                                vi.	head() and tail()
                                vii.	summary()
                                viii.	table()
                        b.	Simulation
                                i.	sample()
                                ii.	rnorm()
                                iii.	rbinom()
                        c.	Base Graphics
                                i.	plot()
                                ii.	hist()
                                iii.	boxplot()
                                iv.	Not useful because I use ggplot2 in my project
                3.	Programming Assignment Three
                        a.	Given data contains location information for 4706 hospitals, and patient outcomes statistics regarding:
                                i.	Heart Attack
                                ii.	Heart Failure
                                iii.	Pneumonia
                        b.	Goal: Write Three functions
                                i.	best(state, outcome) – determines the best hospital in a state ranked by 30 day mortality for the given outcome. 
                                        1.	If an invalid state is supplied, throw an error with message “invalid state”. If an invalid outcome is supplied, throw an error with message “invalid outcome”.
                                        2.	Ties are to be broken by alphabetical ordering of hospital name.
                                ii.	rankhospital(state, outcome, num = “best”) -  Rank hospitals in a state for the 30 day mortaility in the supplied outcome. Num grabs the rank wanted. “best” corresponds to rank 1, and “worst” corresponds to the last place hospital. If invalid num is supplied return NA. Same caveats as in best()
                                iii.	 rankall(outcome, num = “best”) – Essentially just applies rankhospital() onto all states. If there are fewer than num hospitals in a state, return NA for that state. No state caveats, otherwise same as rankhospital()
                        c.	Graded by Quiz
        v.	Chapter Quizzes
                1.	One quiz per week, so eight overall. Number of questions range from 5 to 20. Generally concerned with the material covered in the last week, but there were perhaps 3-4 instances of a bug where questions would be pulled from future weeks. Quizzes allowed to be retaken, which helped correct for this bug.
VI.	Statistical Background
        a.	One Sample T-Test
                i.	Minimum sample size
                ii.	Assumptions
                iii.	Statistic Derivation
                iv.	Confidence Interval
                v.	Hypothesis Testing
                        a. Experiment One
                        b. Experiment Two
        b.	The Bootstrap Principle
                i.	Minimum Sample Size
                ii.	Assumptions
                iii.	What is a sampling distribution?
                iv.	Confidence Interval
                v.	Hypothesis Testing
                        a. Experiment One
                        b. Experiment Two
        c.	Linear Mixed Effects Modeling
                i.	https://www.youtube.com/watch/VhMWPkTbXoY 
                ii.	https://m-clark.github.io/mixed-models-with-R/ 
                iii.	https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models 
                iv.	https://www.rensvandeschoot.com/tutorials/lme4/ 
                v.	Difference between lm and lmer()
                        1.	Standard Linear Regression formula
                                a.	Calculation of slope
                                b.	Calculation of mean intercept
                                c.	Residual Variance
                        2.	Fixed Effects Vs. Mixed Effects
                        3.	lme4 syntax
                vi.	Terminology
        d.	Normal approximation
                i.	Central Limit Theorem
                ii.	Minimum sample size ~ 30-40
                iii.	
VII.	Case Study
        a.	Introduction to Data Set
        b.	Question – how does the first professor a student has in a department influence their future scholastic performance? Future Cumulative GPA is optimal tracking method, but requires filtration conditions.
                i.	Filtration Conditions:
                        1.	Students must take exactly one course in their first term
                        2.	Students must not withdraw from their first course
                        3.	Students must receive at least two grades to create a cumulative GPA progression, which necessarily filters out all students that only take a single course in the math or chemistry departments
                                i.	We must therefore separately analyze withdraw rates 
        c.	Plan of Attack for DeltaC Rankings
                i.	Determine variables that affect final cumulative GPA
                        1.	Student Variance
                        2.	Initial Starting Course
                        3.	Number of Terms in OCC’s Math and Chemistry departments
                        4.	Faculty effects
                                a.	Initial faculty
                                b.	Following faculty effects should “mean out” over sufficient samples. That is, the variance imposed by difference in future faculty should have a Normal distribution centered about the mean of initial faculty
                                c.	It is possible to assign each faculty “points” for only the change in cumulative GPA that comes after, but the weights are unintuitive due to future courses having less effect on change in cumulative GPA as opposed to the extant value (beyond the second course).
                ii.	Create stratified models for cumulative GPA slope accounting for said confounding variables
                        1.	Adjust for starting course
                                a.	Justify with graphs
                        2.	Adjust for max.terms
                                a.	Justify with graphs
                        3.	Require sufficient observations in a stratum for approximate Normality of data, I choose N >= 40. That is, each valid combination of max.terms and starting course will have at least 40 students for t-test validity.
                iii.	Apply models onto individual students, starting mean slope at observation of initial cumulative GPA. Expected final cumulative GPA is then calculated as Initial GPA + slope * max.terms. 
                        1.	This is a mixed model – Fixed effect is Student.Term, random effect is initial gpa of a student. 
                                a.	Expected final cum GPA is calculated as f(max(Student.Term))
                        2.	Talk about differences between lmer and manual – compare results. 
                                a.	lmer automatically captures residual error when given Student Random ID as a random effect, but does I was unable to make the model converge when adjusting for max.terms as acting on slope. My bet is this was automatically captured as part of the student-to-student random deviation. Unfortunately, lmer cannot do data analysis for me so I still need to compare its results with a manual tabulation to check validity.
                                b.	The issue with doing a manual model is that as a human I cannot imagine all the confounding variables that influence final cumulative GPA. We are measuring first professor effect, so we don’t want to stratify based on that. The big two that I could think of were the number of terms that a student enrolled in OCC’s math and chemistry departments. A third I considered was the overall term students were enrolled in – if a student was enrolled during a term where the average teaching quality for a course was markedly lower/higher than other terms, their final cumulative GPA may decrease/increase. For most courses there are not enough faculty that teach them to adjust for this deviation in a meaningful way – if there are three faculty, two “good” and one “poor”, then when we go to measure the mean effect 
                iv.	Create statistic called DeltaC = Observed Final Cumulative GPA – Expected Final Cumulative GPA.
                v.	Determine mean DeltaC of those students that passed filtration and started with a specific professor
                        1.	Experiment One: Professors are assumed to have one overall level of impact on student’s final cumulative GPA. Sum all DeltaC values of all students that passed filtration and started with that faculty, then divide by the number of students.   This may be unfair if they are made to teach a course they are unfamiliar with instructing, so we also do Experiment Two.
                        2.	Experiment Two: A Professor’s Mean DeltaC is calculated over each course code they taught using only those students that started in the appropriate course code. Do example with a professor – print out all DeltaC’s for a prof with at least 3.
                vi.	Test Significance of results using t-test and bootstrap for double confidence. We could validly get away with just the t-test because we ensured that data came from strata with at least 40 observations (thus approximately Normal), but as this is my SIP I want to use all the firepower I can. Additionally, some assumptions of the one sample t-test are not rigorously fulfilled by DeltaC, so usage of the bootstrap can correct for the errors imposed by the t-test.
                vii.	
        d.	Basic New Variables (through Gen_Data 9)
        e.	Plan of Attack for DeltaW
        
      ERROR ANALYSIS
1. Who teaches OCC courses?  https://oaklandcc.edu/faculty/default.aspx
  49 Math Faculty
  12 Chemistry Faculty
  Are some faculty being recyled for anonymity, or does OCC not report former facult?


